# **Module 01** - Natural Language Processing with Classification and Vector Spaces
## **Week 3: Vector Space Models**

---
### **Vector Space Models**
---

- Tu·∫ßn n√†y, s·∫Ω h·ªçc v·ªÅ **kh√¥ng gian vect∆° (vector spaces)** v√† lo·∫°i th√¥ng tin m√† ch√∫ng c√≥ th·ªÉ m√£ h√≥a.
- Gi·ªõi thi·ªáu √Ω t∆∞·ªüng chung ƒë·∫±ng sau **m√¥ h√¨nh kh√¥ng gian vector (vector space models)**, l·ª£i th·∫ø v√† ·ª©ng d·ª•ng c·ªßa ch√∫ng trong **NLP** (X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n).

#### Kh·∫£ nƒÉng c·ªßa M√¥ h√¨nh Kh√¥ng gian Vect∆°

> V√≠ d·ª•

![01_Example_Application_Vector_Spaces_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/01_Example_Application_Vector_Spaces_Model.png)

1.  **N·∫Øm b·∫Øt √Ω nghƒ©a (Semantic Meaning):**
    - C√°c m√¥ h√¨nh kh√¥ng gian vect∆° gi√∫p x√°c ƒë·ªãnh c√°c c√¢u c√≥ √Ω nghƒ©a gi·ªëng nhau, ngay c·∫£ khi ch√∫ng *kh√¥ng* chia s·∫ª c√πng m·ªôt t·ª´ (v√≠ d·ª•: hai c√¢u h·ªèi kh√°c nhau nh∆∞ng c√≥ c√πng √Ω nghƒ©a).
    - Ch√∫ng c≈©ng gi√∫p ph√¢n bi·ªát c√°c c√¢u c√≥ t·ª´ gi·ªëng h·ªát nhau nh∆∞ng √Ω nghƒ©a kh√°c nhau (v√≠ d·ª•: "B·∫°n ƒëang ƒëi ƒë√¢u?" so v·ªõi "B·∫°n ƒë·∫øn t·ª´ ƒë√¢u?").
    - ·ª®ng d·ª•ng: X√°c ƒë·ªãnh s·ª± t∆∞∆°ng ƒë·ªìng cho c√¢u tr·∫£ l·ªùi, di·ªÖn gi·∫£i v√† t√≥m t·∫Øt c√¢u h·ªèi.

2.  **N·∫Øm b·∫Øt s·ª± ph·ª• thu·ªôc gi·ªØa c√°c t·ª´ (Dependencies):**
    - Ch√∫ng c√≥ th·ªÉ n·∫Øm b·∫Øt c√°c m·ªëi quan h·ªá v√† s·ª± ph·ª• thu·ªôc gi·ªØa c√°c t·ª´.
    - V√≠ d·ª• 1: C√°c t·ª´ "ng≈© c·ªëc" (cereal) v√† "b√°t" (bowl) c√≥ li√™n quan.
    - V√≠ d·ª• 2: Trong c√¢u "b·∫°n mua m·ªôt c√°i g√¨ ƒë√≥ v√† ng∆∞·ªùi kh√°c b√°n n√≥", n·ª≠a sau c·ªßa c√¢u ph·ª• thu·ªôc v√†o n·ª≠a ƒë·∫ßu.

#### ·ª®ng d·ª•ng

- C√°c m√¥ h√¨nh kh√¥ng gian vect∆° ƒë∆∞·ª£c s·ª≠ d·ª•ng trong:
    + **Tr√≠ch xu·∫•t th√¥ng tin (Information extraction)** (tr·∫£ l·ªùi c√¢u h·ªèi ai, c√°i g√¨, ·ªü ƒë√¢u, nh∆∞ th·∫ø n√†o).
    + **D·ªãch m√°y (Machine translation)**.
    + L·∫≠p tr√¨nh **chatbots** v√† nhi·ªÅu ·ª©ng d·ª•ng kh√°c.

#### Kh√°i ni·ªám c·ªët l√µi

- Tr√≠ch d·∫´n c·ªßa **John Firth**, m·ªôt nh√† ng√¥n ng·ªØ h·ªçc Anh: "**B·∫°n s·∫Ω bi·∫øt m·ªôt t·ª´ b·ªüi c√¥ng ty m√† n√≥ gi·ªØ**" (You shall know a word by the company it keeps).
- ƒê√¢y l√† m·ªôt trong nh·ªØng kh√°i ni·ªám c∆° b·∫£n nh·∫•t trong NLP.
- M√¥ h√¨nh kh√¥ng gian vect∆° th·ª±c hi·ªán ƒëi·ªÅu n√†y b·∫±ng c√°ch **x√°c ƒë·ªãnh ng·ªØ c·∫£nh (context)** xung quanh m·ªói t·ª´, t·ª´ ƒë√≥ n·∫Øm b·∫Øt ƒë∆∞·ª£c **√Ω nghƒ©a t∆∞∆°ng ƒë·ªëi (relative meaning)**.
- **K·∫øt lu·∫≠n (Eureka):** M√¥ h√¨nh kh√¥ng gian vect∆° cho ph√©p b·∫°n bi·ªÉu di·ªÖn c√°c t·ª´ v√† t√†i li·ªáu d∆∞·ªõi d·∫°ng **vect∆°**, n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a t∆∞∆°ng ƒë·ªëi.

---
### **Word by Word and Word by Doc**
---

- H∆∞·ªõng d·∫´n c√°ch x√¢y d·ª±ng **vect∆°** (vectors) d·ª±a tr√™n **ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán** (co-occurrence matrices).
* T√πy thu·ªôc v√†o nhi·ªám v·ª•, b·∫°n c√≥ th·ªÉ c√≥ m·ªôt s·ªë **thi·∫øt k·∫ø (designs)** kh·∫£ thi ƒë·ªÉ m√£ h√≥a m·ªôt t·ª´ ho·∫∑c t√†i li·ªáu th√†nh vect∆°.

#### Hai thi·∫øt k·∫ø M√¥ h√¨nh Kh√¥ng gian Vect∆°
> Word by Word Design

![02_Word_by_Word](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/02_Word_by_Word.png)

1.  **Thi·∫øt k·∫ø T·ª´ng t·ª´ (Word-by-word)**
    - B·∫°n t·∫°o m·ªôt ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán v√† tr√≠ch xu·∫•t vect∆° (b·∫£n tr√¨nh b√†y) cho c√°c t·ª´ trong kho (corpus) c·ªßa b·∫°n.
    - **S·ª± ƒë·ªìng xu·∫•t hi·ªán (Co-occurrence)** c·ªßa hai t·ª´ l√† s·ªë l·∫ßn ch√∫ng xu·∫•t hi·ªán c√πng nhau trong m·ªôt **kho·∫£ng c√°ch t·ª´ nh·∫•t ƒë·ªãnh k** (a certain word distance k).
    - **V√≠ d·ª•:** V·ªõi $k=2$, n·∫øu "d·ªØ li·ªáu" (data) v√† "ƒë∆°n gi·∫£n" (simple) c√πng xu·∫•t hi·ªán 2 l·∫ßn (m·ªôt l·∫ßn c√°ch 1 t·ª´, m·ªôt l·∫ßn c√°ch 2 t·ª´), gi√° tr·ªã trong ma tr·∫≠n l√† 2.
    - **H√†ng (row)** c·ªßa ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán (v√≠ d·ª•: h√†ng cho t·ª´ "d·ªØ li·ªáu") tr·ªü th√†nh **bi·ªÉu di·ªÖn vect∆°** c·ªßa t·ª´ ƒë√≥ (v√≠ d·ª•: [2, 1, 1, 0]).

> Word by Document Design

![03_Word_by_Document](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/03_Word_by_Document.png)

2.  **Thi·∫øt k·∫ø T·ª´ theo t√†i li·ªáu (Word-by-document)**
    - Qu√° tr√¨nh n√†y kh√° gi·ªëng nhau, nh∆∞ng b·∫°n ƒë·∫øm s·ªë l·∫ßn c√°c t·ª´ xu·∫•t hi·ªán trong c√°c **t√†i li·ªáu (documents)** thu·ªôc c√°c **danh m·ª•c (categories)** c·ª• th·ªÉ.
    - **V√≠ d·ª•:** M·ªôt kho t√†i li·ªáu c√≥ 3 danh m·ª•c: gi·∫£i tr√≠, kinh t·∫ø, v√† h·ªçc m√°y.
    - T·ª´ "d·ªØ li·ªáu" (data) xu·∫•t hi·ªán: 500 l·∫ßn (gi·∫£i tr√≠), 6.620 l·∫ßn (kinh t·∫ø), 9.320 l·∫ßn (h·ªçc m√°y).
    - T·ª´ "phim" (movie) xu·∫•t hi·ªán: 7.000 l·∫ßn (gi·∫£i tr√≠), 4.000 l·∫ßn (kinh t·∫ø), 1.000 l·∫ßn (h·ªçc m√°y).

#### X√¢y d·ª±ng Kh√¥ng gian Vect∆° v√† S·ª± t∆∞∆°ng ƒë·ªìng
> Kh√¥ng gian Vect∆° v√† S·ª± t∆∞∆°ng ƒë·ªìng

![04_Vector_Space_and_Similarity](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/04_Vector_Space_and_Similarity.png)

- T·ª´ ma tr·∫≠n "T·ª´ theo t√†i li·ªáu", b·∫°n c√≥ th·ªÉ l·∫•y bi·ªÉu di·ªÖn cho *t·ª´* (t·ª´ c√°c h√†ng) ho·∫∑c cho *lo·∫°i t√†i li·ªáu* (t·ª´ c√°c c·ªôt).
- **V√≠ d·ª• (l·∫•y theo c·ªôt):** Kh√¥ng gian vect∆° s·∫Ω c√≥ hai chi·ªÅu (t∆∞∆°ng ·ª©ng v·ªõi t·ª´ "d·ªØ li·ªáu" v√† "phim").
    + Vect∆° "gi·∫£i tr√≠" = [500, 7.000]
    + Vect∆° "kinh t·∫ø" = [6.620, 4.000]
    + Vect∆° "h·ªçc m√°y" = [9.320, 1.000]
- Trong kh√¥ng gian n√†y, c√≥ th·ªÉ th·∫•y r·∫±ng t√†i li·ªáu "kinh t·∫ø" v√† "h·ªçc m√°y" **gi·ªëng nhau (similar)** h∆°n nhi·ªÅu so v·ªõi "gi·∫£i tr√≠".
- S·∫Øp t·ªõi, b·∫°n s·∫Ω h·ªçc c√°ch so s√°nh c√°c bi·ªÉu di·ªÖn vect∆° n√†y b·∫±ng **s·ª± t∆∞∆°ng ƒë·ªìng cosin (cosine similarity)** v√† **kho·∫£ng c√°ch Euclide (Euclidean distance)**.

#### K·∫øt lu·∫≠n

- B·∫°n ƒë√£ th·∫•y c√°ch l·∫•y kh√¥ng gian vect∆° b·∫±ng hai thi·∫øt k·∫ø: **t·ª´ng t·ª´** v√† **t·ª´ng vƒÉn b·∫£n (word-by-document)**.
- B·∫°n ƒë√£ h·ªçc c√°ch x√°c ƒë·ªãnh m·ªëi quan h·ªá (nh∆∞ **s·ª± t∆∞∆°ng ƒë·ªìng - similarity**) gi·ªØa c√°c lo·∫°i t√†i li·ªáu trong kh√¥ng gian vect∆°.

---
### **Euclidean Distance**
---

- Gi·ªõi thi·ªáu v·ªÅ **kho·∫£ng c√°ch Euclide** (Euclidean distance), m·ªôt **s·ªë li·ªáu t∆∞∆°ng ƒë·ªìng** (similarity metric) d√πng ƒë·ªÉ x√°c ƒë·ªãnh hai ƒëi·ªÉm (ho·∫∑c vect∆°) c√°ch nhau bao xa.

#### **Tr∆∞·ªùng h·ª£p 2 chi·ªÅu (2D):**

> T√≠nh kho·∫£ng c√°ch Euclidian gi·ªØa 2 vector

![05_Euclidian_Distance](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/05_Euclidian_Distance.png)

- S·ª≠ d·ª•ng v√≠ d·ª• v·ªÅ hai vect∆° corpora ("gi·∫£i tr√≠" v√† "h·ªçc m√°y") v·ªõi hai chi·ªÅu l√† s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ "d·ªØ li·ªáu" v√† "phim".
- Kho·∫£ng c√°ch Euclide l√† **chi·ªÅu d√†i c·ªßa ƒëo·∫°n ƒë∆∞·ªùng th·∫≥ng** (length of the line segment) n·ªëi hai vect∆° ƒë√≥ trong kh√¥ng gian.
- C√¥ng th·ª©c ƒë∆∞·ª£c s·ª≠ d·ª•ng l√† m·ªôt v√≠ d·ª• c·ªßa **ƒë·ªãnh l√Ω Pythagore** (Pythagorean theorem): 

- $d(B,A)=\sqrt{(\text{kho·∫£ng c√°ch ngang})^2 + (\text{kho·∫£ng c√°ch d·ªçc})^2}$.
- Trong v√≠ d·ª•, k·∫øt qu·∫£ x·∫•p x·ªâ 10.667.

#### **Tr∆∞·ªùng h·ª£p K√≠ch th∆∞·ªõc cao h∆°n (n-Dimension):**

> T√≠nh t·ªïng qu√°t h√≥a vi·ªác t√¨m kho·∫£ng c√°ch gi·ªØa hai ƒëi·ªÉm (A, B) sang kho·∫£ng c√°ch gi·ªØa m·ªôt vect∆° n chi·ªÅu nh∆∞ sau:

![06_Euclidian_Distance_Generalization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/06_Euclidian_Distance_Generalization.png)

> V√≠ d·ª• t√≠nh kho·∫£ng c√°ch gi·ªØa 2 vector (n = 3).

![07_Example_Euclidian_Distance_Generalization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/07_Example_Euclidian_Distance_Generalization.png)

- Quy tr√¨nh n√†y l√† s·ª± **kh√°i qu√°t h√≥a** (generalization) c·ªßa tr∆∞·ªùng h·ª£p 2D.
- ƒê·ªÉ t√¨m kho·∫£ng c√°ch gi·ªØa hai vect∆° (v√≠ d·ª•: 'ice-cream' v√† 'boba'):
    1.  L·∫•y **s·ª± kh√°c bi·ªát** (difference) gi·ªØa m·ªói k√≠ch th∆∞·ªõc.
    2.  **B√¨nh ph∆∞∆°ng** (Square) nh·ªØng kh√°c bi·ªát ƒë√≥.
    3.  **T·ªïng h·ª£p** (Sum) ch√∫ng l·∫°i.
    4.  L·∫•y **cƒÉn b·∫≠c hai** (square root) c·ªßa k·∫øt qu·∫£.
- C√¥ng th·ª©c n√†y (t·ª´ ƒë·∫°i s·ªë) ƒë∆∞·ª£c g·ªçi l√† **ƒë·ªãnh m·ª©c c·ªßa s·ª± kh√°c bi·ªát** (norm of the difference) gi·ªØa c√°c vect∆°.

#### **Tri·ªÉn khai trong Python:**

- B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng m√¥-ƒëun `linalg` (linear algebra) t·ª´ **NumPy**.
- H√†m `norm` (`np.linalg.norm(v - w)`) c√≥ th·ªÉ t√≠nh to√°n ƒë·ªãnh m·ª©c c·ªßa ch√™nh l·ªách (t·ª©c l√† kho·∫£ng c√°ch Euclide) cho kh√¥ng gian n chi·ªÅu.

#### **ƒêi·ªÉm r√∫t ra ch√≠nh:**
- Kho·∫£ng c√°ch Euclide v·ªÅ c∆° b·∫£n l√† chi·ªÅu d√†i c·ªßa ƒë∆∞·ªùng th·∫≥ng n·ªëi hai vect∆°.
- B·∫±ng c√°ch s·ª≠ d·ª•ng s·ªë li·ªáu n√†y, b·∫°n c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c hai t√†i li·ªáu ho·∫∑c t·ª´ **gi·ªëng nhau** (similar) nh∆∞ th·∫ø n√†o (kho·∫£ng c√°ch c√†ng nh·ªè, c√†ng gi·ªëng nhau).

---
### **Cosine Similarity: Intuition**
---

- Ph·∫ßn n√†y gi·ªõi thi·ªáu v·ªÅ **s·ª± t∆∞∆°ng ƒë·ªìng cosin** (cosine similarity), m·ªôt lo·∫°i **ch·ª©c nƒÉng t∆∞∆°ng ƒë·ªìng** (similarity function) kh√°c.
- V·ªÅ c∆° b·∫£n, n√≥ s·ª≠ d·ª•ng **cosin c·ªßa g√≥c** (cosine of the angle) gi·ªØa hai vect∆° ƒë·ªÉ cho bi·∫øt ch√∫ng c√≥ g·∫ßn nhau hay kh√¥ng.
- N√≥ s·∫Ω ch·ªâ ra v·∫•n ƒë·ªÅ c·ªßa vi·ªác s·ª≠ d·ª•ng **kho·∫£ng c√°ch Euclide** (Euclidean distance) khi so s√°nh c√°c t√†i li·ªáu (corpora) v√† c√°ch s·ª± t∆∞∆°ng ƒë·ªìng cosin kh·∫Øc ph·ª•c ƒëi·ªÅu n√†y.

> V√≠ d·ª• Consine Similarity: Intuition

![08_Example_Consine_Similarity_Intuition](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/08_Example_Consine_Similarity_Intuition.png)

#### **V√≠ d·ª• v·ªÅ v·∫•n ƒë·ªÅ c·ªßa Kho·∫£ng c√°ch Euclide:**

- Gi·∫£ s·ª≠ c√≥ m·ªôt kh√¥ng gian vect∆° v·ªõi c√°c t·ª´ "b·ªánh" (disease) v√† "tr·ª©ng" (eggs).
- C√≥ ba kho: **th·ª±c ph·∫©m** (food), **n√¥ng nghi·ªáp** (agriculture), v√† **l·ªãch s·ª≠** (history).
- Kho "th·ª±c ph·∫©m" c√≥ s·ªë l∆∞·ª£ng t·ª´ nh·ªè, trong khi "n√¥ng nghi·ªáp" v√† "l·ªãch s·ª≠" c√≥ s·ªë l∆∞·ª£ng t·ª´ t∆∞∆°ng t·ª± (l·ªõn h∆°n).
- Kho·∫£ng c√°ch Euclide $d_2$ (gi·ªØa n√¥ng nghi·ªáp v√† l·ªãch s·ª≠) **nh·ªè h∆°n** kho·∫£ng c√°ch $d_1$ (gi·ªØa th·ª±c ph·∫©m v√† n√¥ng nghi·ªáp).
- ƒêi·ªÅu n√†y (sai l·∫ßm) cho th·∫•y kho "n√¥ng nghi·ªáp" v√† "l·ªãch s·ª≠" gi·ªëng nhau h∆°n.

#### **Gi·∫£i ph√°p (S·ª± t∆∞∆°ng ƒë·ªìng Cosin):**

- M·ªôt ph∆∞∆°ng ph√°p kh√°c l√† t√≠nh cosin c·ªßa g√≥c trong (inner angle).
- N·∫øu g√≥c nh·ªè, cosin g·∫ßn b·∫±ng **m·ªôt (1)**. N·∫øu g√≥c g·∫ßn 90 ƒë·ªô, cosin g·∫ßn **kh√¥ng (0)**.
- Trong v√≠ d·ª•, **g√≥c Alpha** (gi·ªØa th·ª±c ph·∫©m v√† n√¥ng nghi·ªáp) **nh·ªè h∆°n** **g√≥c Beta** (gi·ªØa n√¥ng nghi·ªáp v√† l·ªãch s·ª≠).
- Do ƒë√≥, cosin c·ªßa c√°c g√≥c l√† m·ªôt **ƒë·∫°i di·ªán t·ªët h∆°n** (better representation) v·ªÅ s·ª± t∆∞∆°ng ƒë·ªìng so v·ªõi kho·∫£ng c√°ch Euclide.

#### **∆Øu ƒëi·ªÉm ch√≠nh:** 
- ∆Øu ƒëi·ªÉm c·ªßa s·ªë li·ªáu n√†y so v·ªõi kho·∫£ng c√°ch Euclide l√† n√≥ **kh√¥ng b·ªã sai l·ªách b·ªüi s·ª± kh√°c bi·ªát k√≠ch th∆∞·ªõc** (not biased by the size difference) gi·ªØa c√°c bi·ªÉu di·ªÖn.
- **T√≥m l·∫°i:** Kho·∫£ng c√°ch Euclide kh√¥ng l√Ω t∆∞·ªüng cho c√°c t√†i li·ªáu c√≥ k√≠ch th∆∞·ªõc kh√°c nhau. S·ª± t∆∞∆°ng ƒë·ªìng cosin s·ª≠ d·ª•ng g√≥c v√† do ƒë√≥ **kh√¥ng ph·ª• thu·ªôc v√†o k√≠ch th∆∞·ªõc** (independent of the size) c·ªßa c√°c corpus.

---
### **Cosine Similarity**
---

- Ph·∫ßn n√†y h∆∞·ªõng d·∫´n c√°ch t√≠nh **t√≠ch d·∫•u ch·∫•m** (dot product) v√† **ƒë·ªãnh m·ª©c** (norm) c·ªßa vect∆°. Khi bi·∫øt hai ƒëi·ªÅu n√†y, b·∫°n s·∫Ω c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c **ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cosin** (cosine similarity score).
- B·∫°n s·∫Ω h·ªçc c√°ch t√≠nh cosin c·ªßa **g√≥c trong** (inner angle) c·ªßa hai vect∆° v√† hi·ªÉu gi√° tr·ªã t∆∞∆°ng ƒë·ªìng cosin li√™n quan nh∆∞ th·∫ø n√†o ƒë·∫øn s·ª± **gi·ªëng nhau c·ªßa c√°c h∆∞·ªõng** (similarity of the directions).
- C·∫ßn nh·ªõ l·∫°i c√°c ƒë·ªãnh nghƒ©a t·ª´ ƒë·∫°i s·ªë:
    1.  **ƒê·ªãnh m·ª©c (Norm)** (hay **ƒë·ªô l·ªõn - magnitude**) c·ªßa m·ªôt vect∆°: ƒê∆∞·ª£c ƒë·ªãnh nghƒ©a l√† **cƒÉn b·∫≠c hai c·ªßa t·ªïng c√°c ph·∫ßn t·ª≠ b√¨nh ph∆∞∆°ng** (square root of the sum of its squared elements) c·ªßa n√≥.
    > Norm Equation

    $$\|\vec{v}\| = \sqrt{\sum_{i=1}^{n} v_{i}^{2}}$$

    2.  **T√≠ch ƒëi·ªÉm (Dot product)** gi·ªØa hai vect∆°: L√† **t·ªïng t√≠ch gi·ªØa c√°c ph·∫ßn t·ª≠ c·ªßa ch√∫ng** (sum of the products between their elements) trong m·ªói chi·ªÅu
    + V√≠ d·ª•: S·ª≠ d·ª•ng hai corpora (th·ªÉ t√≠ch n√¥ng nghi·ªáp 'v' v√† kho l·ªãch s·ª≠ 'w') v·ªõi c√°c chi·ªÅu l√† "b·ªánh" v√† "tr·ª©ng". G√≥c gi·ªØa ch√∫ng l√† Beta.
    ƒê√¢y l√† m√£ LaTeX cho c√¥ng th·ª©c trong ·∫£nh c·ªßa b·∫°n:
    > Dot Product Equation

    $$\vec{v} \cdot \vec{w} = \sum_{i=1}^{n} v_{i} \cdot w_{i}$$

#### **C√¥ng th·ª©c T∆∞∆°ng ƒë·ªìng Cosin:** 
- Cosin c·ªßa g√≥c Beta b·∫±ng **t√≠ch ch·∫•m** gi·ªØa c√°c vect∆° chia cho **t√≠ch c·ªßa hai ƒë·ªãnh m·ª©c** (dot product between the vectors divided by the product of the two norms).
> Cosine Similarity Equation

$$\cos(\beta) = \frac{v \cdot w}{||v|| \times ||w||}$$

- Khi thay th·∫ø c√°c gi√° tr·ªã (T·ª≠ s·ªë: t√≠ch ƒëi·ªÉm; M·∫´u s·ªë: t√≠ch c·ªßa c√°c ƒë·ªãnh m·ª©c), v√≠ d·ª• cho k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªìng cosin l√† 0.87.

#### **Gi·∫£i th√≠ch √Ω nghƒ©a s·ªë li·ªáu:**
> V√≠ d·ª• v·ªÅ s·ª± t∆∞∆°ng ƒë·ªìng v√† s·ª± kh√¥ng t∆∞∆°ng ƒë·ªìng c·ªßa 2 vector

![09_Example_Consine_Similarity](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/09_Example_Consine_Similarity.png)

- (ƒê·ªëi v·ªõi kh√¥ng gian vect∆° ch·ªâ c√≥ gi√° tr·ªã d∆∞∆°ng b·∫°n ƒë√£ th·∫•y cho ƒë·∫øn nay):
- **Tr·ª±c giao (Orthogonal)** (g√≥c 90 ƒë·ªô): Cosin = 0. (Nghƒ©a l√† ch√∫ng **kh√¥ng gi·ªëng nhau t·ªëi ƒëa** - maximally dissimilar).
- **C√πng h∆∞·ªõng (Same direction)** (g√≥c 0 ƒë·ªô): Cosin = 1.

#### **K·∫øt lu·∫≠n:** 
- Khi cosin c·ªßa g√≥c ti·∫øn g·∫ßn ƒë·∫øn 1, h∆∞·ªõng c·ªßa ch√∫ng c√†ng g·∫ßn.

#### **ƒêi·ªÉm r√∫t ra ch√≠nh:**
- S·ªë li·ªáu n√†y t·ª∑ l·ªá thu·∫≠n v·ªõi s·ª± gi·ªëng nhau gi·ªØa c√°c h∆∞·ªõng c·ªßa vect∆°.
- ƒê·ªëi v·ªõi c√°c kh√¥ng gian vect∆° d∆∞∆°ng ƒë√£ th·∫•y, s·ª± t∆∞∆°ng ƒë·ªìng cosin c√≥ gi√° tr·ªã t·ª´ 0 ƒë·∫øn 1.
#### **T√≥m l·∫°i:**
- T∆∞∆°ng ƒë·ªìng cosin c·ªßa m·ªôt vect∆° v·ªõi ch√≠nh n√≥ = 1.
- N·∫øu c√°c vect∆° vu√¥ng g√≥c (perpendicular) = 0.
- C√°c vect∆° t∆∞∆°ng t·ª± c√≥ ƒëi·ªÉm (score) cao h∆°n.

---
### **Manipulating Word in Vectors Spaces**
---

- Ph·∫ßn n√†y h∆∞·ªõng d·∫´n c√°ch **thao t√°c vect∆°** (vector manipulation) b·∫±ng c√°ch s·ª≠ d·ª•ng **s·ªë h·ªçc vect∆° ƒë∆°n gi·∫£n** (simple vector arithmetic), c·ª• th·ªÉ l√† c·ªông v√† tr·ª´ vect∆°.
- M·ª•c ti√™u l√† s·ª≠" d·ª•ng c√°c ph√©p to√°n n√†y ƒë·ªÉ **suy ra c√°c m·ªëi quan h·ªá kh√¥ng x√°c ƒë·ªãnh** (infer unknown relationships) gi·ªØa c√°c t·ª´, v√≠ d·ª• nh∆∞ d·ª± ƒëo√°n th·ªß ƒë√¥ c·ªßa m·ªôt qu·ªëc gia.

#### Quy tr√¨nh

> V√≠ d·ª• T√¨m Th·ªß ƒë√¥

![10_Example_Manipulating_Word_in_Vectors_Spaces](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/10_Example_Manipulating_Word_in_Vectors_Spaces.png)

1.  **Thi·∫øt l·∫≠p:** Gi·∫£ s·ª≠ b·∫°n c√≥ m·ªôt kh√¥ng gian vect∆° (v√≠ d·ª•: 2D) ch·ª©a c√°c qu·ªëc gia v√† th·ªß ƒë√¥. B·∫°n bi·∫øt th·ªß ƒë√¥ c·ªßa **Hoa K·ª≥ (USA)** l√† **Washington DC** v√† mu·ªën t√¨m th·ªß ƒë√¥ c·ªßa **Nga (Russia)**.
> Kh√¥ng gian Vector 2D c·ªßa v√≠ d·ª• tr√™n 

![11_Example_Manipulating_Word_in_Vectors_Spaces](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/11_Example_Manipulating_Word_in_Vectors_Spaces.png)

2.  **T√¨m Vect∆° M·ªëi quan h·ªá:**
    - ƒê·∫ßu ti√™n, b·∫°n t√¨m "vect∆° m·ªëi quan h·ªá" (relationship vector) k·∫øt n·ªëi qu·ªëc gia v·ªõi th·ªß ƒë√¥ c·ªßa n√≥.
    - C√°ch l√†m: L·∫•y **s·ª± kh√°c bi·ªát** (difference) gi·ªØa vect∆° c·ªßa th·ªß ƒë√¥ ƒë√£ bi·∫øt v√† qu·ªëc gia t∆∞∆°ng ·ª©ng (v√≠ d·ª•: $V_{\text{Washington}} - V_{\text{USA}}$).
    - Vect∆° k·∫øt qu·∫£ n√†y bi·ªÉu th·ªã "c·∫ßn di chuy·ªÉn bao nhi√™u" t·ª´ m·ªôt qu·ªëc gia ƒë·ªÉ ƒë·∫øn th·ªß ƒë√¥ c·ªßa n√≥.
3.  **√Åp d·ª•ng Vect∆° M·ªëi quan h·ªá (D·ª± ƒëo√°n):**
    - ƒê·ªÉ t√¨m th·ªß ƒë√¥ c·ªßa Nga, b·∫°n **t√≠nh t·ªïng** (sum) bi·ªÉu di·ªÖn vect∆° c·ªßa "Nga" v·ªõi vect∆° m·ªëi quan h·ªá v·ª´a t√¨m ƒë∆∞·ª£c ·ªü b∆∞·ªõc tr∆∞·ªõc (v√≠ d·ª•: $V_{\text{Russia}} + (V_{\text{Washington}} - V_{\text{USA}})$).
4.  **T√¨m k·∫øt qu·∫£ g·∫ßn nh·∫•t:**
    - K·∫øt qu·∫£ c·ªßa ph√©p c·ªông l√† m·ªôt vect∆° m·ªõi (v√≠ d·ª•: [10, 4]).
    - Tuy nhi√™n, c√≥ th·ªÉ kh√¥ng c√≥ th√†nh ph·ªë n√†o ch√≠nh x√°c t·∫°i v·ªã tr√≠ ƒë√≥.
    - B·∫°n ph·∫£i t√¨m th√†nh ph·ªë **gi·ªëng nh·∫•t (most similar)** (g·∫ßn nh·∫•t) v·ªõi vect∆° [10, 4] b·∫±ng c√°ch so s√°nh n√≥ v·ªõi t·∫•t c·∫£ c√°c vect∆° th√†nh ph·ªë kh√°c, s·ª≠ d·ª•ng **kho·∫£ng c√°ch Euclide** (Euclidean distance) ho·∫∑c **ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cosin** (cosine similarity).
    - Trong v√≠ d·ª•, vect∆° g·∫ßn nh·∫•t l√† "Moscow".

#### K·∫øt lu·∫≠n

- **M·∫•u ch·ªët:** Qu√° tr√¨nh n√†y ch·ªâ hi·ªáu qu·∫£ khi b·∫°n c√≥ m·ªôt kh√¥ng gian vect∆° n∆°i c√°c bi·ªÉu di·ªÖn (vect∆°) **n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a t∆∞∆°ng ƒë·ªëi** (capture the relative meaning) c·ªßa c√°c t·ª´.
- **S·ª± ph√¢n c·ª•m (Clustering):** C√°c t·ª´ xu·∫•t hi·ªán ·ªü nh·ªØng n∆°i (ng·ªØ c·∫£nh) t∆∞∆°ng t·ª± s·∫Ω ƒë∆∞·ª£c m√£ h√≥a theo c√°ch t∆∞∆°ng t·ª±. B·∫°n c√≥ th·ªÉ t·∫≠n d·ª•ng ƒëi·ªÅu n√†y ƒë·ªÉ t√¨m c√°c m·∫´u (v√≠ d·ª•: c√°c t·ª´ g·∫ßn nh·∫•t v·ªõi "b√°c sƒ©" (doctor) c√≥ th·ªÉ l√† "y t√°" (nurse), "b√°c sƒ© ph·∫´u thu·∫≠t" (surgeon)...).
- ƒê·ªÉ ch√∫ng ta bi·∫øt m·ªëi quan h·ªá kh√¥ng x√°c ƒë·ªãnh gi·ªØa c√°c t·ª´, -> b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c m·ªëi quan h·ªá ƒë√£ bi·∫øt gi·ªØa nh·ªØng ng∆∞·ªùi kh√°c. 

---
### **Visualization and PCA**
---

- Th∆∞·ªùng th√¨ b·∫°n s·∫Ω c√≥ c√°c **vect∆° ·ªü k√≠ch th∆∞·ªõc r·∫•t cao** (high dimensions). B·∫°n mu·ªën **gi·∫£m chi·ªÅu** (reduce the dimension) c·ªßa ch√∫ng xu·ªëng **hai chi·ªÅu** (two dimensions) ƒë·ªÉ c√≥ th·ªÉ v·∫Ω (plot) tr√™n tr·ª•c XY.
- B·∫°n s·∫Ω h·ªçc v·ªÅ **Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh (Principal Component Analysis - PCA)**, thu·∫≠t to√°n cho ph√©p b·∫°n l√†m ƒëi·ªÅu n√†y.
- PCA ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ **h√¨nh dung (visualize)** c√°c bi·ªÉu di·ªÖn vect∆° c√≥ k√≠ch th∆∞·ªõc cao.

#### ƒê·ªông l·ª±c (Tr·ª±c quan h√≥a)
> Bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a

![12_Example_Visualization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/12_Example_Visualization.png)

- Gi·∫£ s·ª≠ b·∫°n c√≥ bi·ªÉu di·ªÖn vect∆° trong kh√¥ng gian chi·ªÅu cao, v√† b·∫°n bi·∫øt r·∫±ng c√°c t·ª´ "d·∫ßu kh√≠" (oil and gas) v√† "th√†nh ph·ªë v√† th·ªã tr·∫•n" (city and town) c√≥ li√™n quan.
- B·∫°n mu·ªën xem li·ªáu bi·ªÉu di·ªÖn c·ªßa m√¨nh c√≥ n·∫Øm b·∫Øt ƒë∆∞·ª£c m·ªëi quan h·ªá ƒë√≥ hay kh√¥ng.
- **Gi·∫£m k√≠ch th∆∞·ªõc** (Dimensionality reduction) l√† l·ª±a ch·ªçn ho√†n h·∫£o cho nhi·ªám v·ª• n√†y.
- B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng PCA ƒë·ªÉ l·∫•y bi·ªÉu di·ªÖn trong kh√¥ng gian c√≥ **√≠t chi·ªÅu h∆°n** (fewer dimensions) (v√≠ d·ª•: ba t√≠nh nƒÉng tr·ªü xu·ªëng).
- N·∫øu b·∫°n nh·∫≠n ƒë∆∞·ª£c **bi·ªÉu di·ªÖn hai chi·ªÅu**, b·∫°n c√≥ th·ªÉ v·∫Ω h√¨nh ·∫£nh c·ªßa c√°c t·ª´.
- Trong h√¨nh ·∫£nh ƒë√≥, b·∫°n c√≥ th·ªÉ th·∫•y li·ªáu c√°c t·ª´ li√™n quan (nh∆∞ "d·∫ßu kh√≠" v√† "th√†nh ph·ªë v√† th·ªã tr·∫•n") c√≥ ƒë∆∞·ª£c **t·∫≠p h·ª£p (clustered)** l·∫°i v·ªõi nhau hay kh√¥ng.
- B·∫°n th·∫≠m ch√≠ c√≥ th·ªÉ t√¨m th·∫•y c√°c m·ªëi quan h·ªá kh√°c m√† b·∫°n kh√¥ng mong ƒë·ª£i.


#### C√°ch th·ª©c ho·∫°t ƒë·ªông (T·ªïng quan)

- ƒê·ªÉ ƒë∆°n gi·∫£n, h√£y x√©t m·ªôt kh√¥ng gian **hai chi·ªÅu** (2D) m√† b·∫°n mu·ªën gi·∫£m xu·ªëng **m·ªôt t√≠nh nƒÉng** (1D).
- ƒê·∫ßu ti√™n, PCA s·∫Ω t√¨m m·ªôt t·∫≠p h·ª£p c√°c **t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan (uncorrelated features)**.
- Sau ƒë√≥, n√≥ **chi·∫øu (project)** d·ªØ li·ªáu c·ªßa b·∫°n v√†o kh√¥ng gian m·ªôt chi·ªÅu, c·ªë g·∫Øng **gi·ªØ l·∫°i c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët (retain as much information as possible)**.

#### T√≥m t·∫Øt

- PCA l√† m·ªôt thu·∫≠t to√°n **gi·∫£m k√≠ch th∆∞·ªõc** (dimensionality reduction) c√≥ th·ªÉ t√¨m th·∫•y c√°c **t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan**.
- N√≥ r·∫•t h·ªØu √≠ch cho vi·ªác **tr·ª±c quan h√≥a d·ªØ li·ªáu** (visualizing your data).
- N√≥ cho ph√©p b·∫°n bi·∫øn m·ªôt **vect∆° chi·ªÅu d** (d-dimensional vector) th√†nh hai chi·ªÅu ƒë·ªÉ t·∫°o ra m·ªôt **bi·ªÉu ƒë·ªì** (plot).

---
### **PCA Algorithmn**
---

- Ph·∫ßn n√†y n√≥i v·ªÅ **gi√° tr·ªã ri√™ng (eigenvalues)** v√† **vect∆° ri√™ng (eigenvectors)**, v√† c√°ch s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ **gi·∫£m k√≠ch th∆∞·ªõc (reduce the dimension)** c·ªßa c√°c t√≠nh nƒÉng.
- M·ª•c ti√™u l√† c√≥ ƒë∆∞·ª£c c√°c **t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan (uncorrelated features)** v√† **gi·ªØ c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët** (keep as much information as possible) t·ª´ vi·ªác nh√∫ng ban ƒë·∫ßu.

> Gi·∫£m chi·ªÅu b·∫±ng PCA 

![13_Visualization_PCA](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/13_Visualization_PCA.png)


- Quy tr√¨nh gi·∫£m k√≠ch th∆∞·ªõc b·∫±ng **PCA** (Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh):
    1.  B·∫Øt ƒë·∫ßu v·ªõi kh√¥ng gian vect∆° ban ƒë·∫ßu.
    2.  L·∫•y c√°c t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan cho d·ªØ li·ªáu.
    3.  **Chi·∫øu (project)** d·ªØ li·ªáu v√†o m·ªôt s·ªë t√≠nh nƒÉng mong mu·ªën, gi·ªØ l·∫°i nhi·ªÅu th√¥ng tin nh·∫•t.
- Trong PCA, **Eigenvector** (vect∆° ri√™ng) c·ªßa **ma tr·∫≠n ƒë·ªìng ph∆∞∆°ng sai (covariance matrix)** t·ª´ d·ªØ li·ªáu c·ªßa b·∫°n cung c·∫•p **h∆∞·ªõng (direction)** c·ªßa c√°c t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan. -> Gi·∫£i ph√°p ki·∫øm uncorrelated features
- **Eigenvalues** (gi√° tr·ªã ri√™ng) l√† **bi·∫øn th·ªÉ (variance)** c·ªßa t·∫≠p d·ªØ li·ªáu trong m·ªói t√≠nh nƒÉng m·ªõi ƒë√≥.
- ƒê·ªÉ th·ª±c hi·ªán PCA, b·∫°n c·∫ßn l·∫•y Eigenvector v√† Eigenvalues t·ª´ ma tr·∫≠n ph∆∞∆°ng sai ƒë·ªìng c·ªßa d·ªØ li·ªáu. -> Gi·∫£i ph√°p cho vi·ªác gi·ªØ c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët. 

> Thu·∫≠t to√°n PCA

![14_PCA_Algorithmn](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/14_PCA_Algorithmn.png)


- **B∆∞·ªõc 1: L·∫•y c√°c t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan.**
    + **B√¨nh th∆∞·ªùng h√≥a (normalize)** d·ªØ li·ªáu.
    + L·∫•y **ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (covariance matrix)**.
    + Th·ª±c hi·ªán **Ph√¢n h·ªßy gi√° tr·ªã ƒë∆°n l·∫ª (Singular Value Decomposition - SVD)**.
    + SVD tr·∫£ v·ªÅ ba ma tr·∫≠n: Ma tr·∫≠n ƒë·∫ßu ti√™n (k√Ω hi·ªáu l√† **U**) ch·ª©a c√°c **Eigenvector** (x·∫øp ch·ªìng l√™n nhau theo c·ªôt), v√† ma tr·∫≠n th·ª© hai (k√Ω hi·ªáu l√† **S**) c√≥ c√°c **Eigenvalue** tr√™n ƒë∆∞·ªùng ch√©o.
    + (SVD ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai trong nhi·ªÅu th∆∞ vi·ªán l·∫≠p tr√¨nh).
- **B∆∞·ªõc 2: Chi·∫øu d·ªØ li·ªáu (Project the data).**
    + S·ª≠ d·ª•ng Eigenvector (U) v√† Eigenvalue (S).
    + Th·ª±c hi·ªán **t√≠ch ch·∫•m (dot product)** gi·ªØa ma tr·∫≠n ch·ª©a c√°c **nh√∫ng t·ª´ (word embeddings)** c·ªßa b·∫°n v√† **N c·ªôt ƒë·∫ßu ti√™n (first N columns)** c·ªßa ma tr·∫≠n U.
    + **N** l√† s·ªë chi·ªÅu b·∫°n mu·ªën c√≥ ·ªü cu·ªëi (v√≠ d·ª•: hai chi·ªÅu ƒë·ªÉ **h√¨nh dung - visualization**).
- **L∆∞u √Ω quan tr·ªçng:** C√°c Eigenvector v√† Eigenvalue ph·∫£i ƒë∆∞·ª£c **s·∫Øp x·∫øp (sorted)** theo Eigenvalue theo **th·ª© t·ª± gi·∫£m d·∫ßn (descending order)** ƒë·ªÉ ƒë·∫£m b·∫£o gi·ªØ l·∫°i nhi·ªÅu th√¥ng tin nh·∫•t (h·∫ßu h·∫øt c√°c th∆∞ vi·ªán t·ª± ƒë·ªông l√†m ƒëi·ªÅu n√†y).
- **T√≥m l·∫°i:** 
    + Eigenvector cho h∆∞·ªõng c·ªßa c√°c t√≠nh nƒÉng kh√¥ng t∆∞∆°ng quan
    + Eigenvalue cho bi·∫øt bi·∫øn th·ªÉ. 
    + (Dot product) T√≠ch ch·∫•m s·∫Ω chi·∫øu d·ªØ li·ªáu l√™n m·ªôt kh√¥ng gian vect∆° m·ªõi.

- **C√°c b∆∞·ªõc ƒë·ªÉ t√≠nh to√°n PCA:**

    + Chu·∫©n h√≥a trung b√¨nh (Mean normalize) d·ªØ li·ªáu c·ªßa b·∫°n.
    + T√≠nh to√°n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (covariance matrix).
    + T√≠nh to√°n SVD (Ph√¢n r√£ gi√° tr·ªã suy bi·∫øn) tr√™n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa b·∫°n. Ph√©p t√≠nh n√†y tr·∫£ v·ªÅ $[U S V] = svd(\Sigma)$. Ba ma tr·∫≠n $U$, $S$, $V$ ƒë∆∞·ª£c v·∫Ω ·ªü tr√™n. $U$ ƒë∆∞·ª£c g√°n nh√£n l√† vector ri√™ng (eigenvectors), v√† $S$ ƒë∆∞·ª£c g√°n nh√£n l√† gi√° tr·ªã ri√™ng (eigenvalues).
    + Sau ƒë√≥, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng $n$ c·ªôt ƒë·∫ßu ti√™n c·ªßa vector $U$, ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi b·∫±ng c√°ch nh√¢n $XU[:, 0:n]$.

---
### **The Rotation Matrix (Optional Reading)**
---

#### **Ph√©p quay ng∆∞·ª£c chi·ªÅu kim ƒë·ªìng h·ªì (Counterclockwise Rotation)**

- N·∫øu b·∫°n mu·ªën quay m·ªôt vector $r$ v·ªõi t·ªça ƒë·ªô $(x, y)$ v√† g√≥c $\alpha$ ng∆∞·ª£c chi·ªÅu kim ƒë·ªìng h·ªì m·ªôt g√≥c $\beta$ ƒë·ªÉ ƒë∆∞·ª£c vector $r'$ v·ªõi t·ªça ƒë·ªô $(x', y')$ th√¨ ta c√≥:

$$x = r * \cos(\alpha)$$
$$y = r * \sin(\alpha)$$
$$x' = r' * \cos(\alpha + \beta)$$
$$y' = r' * \sin(\alpha + \beta)$$
- Ph√©p c·ªông l∆∞·ª£ng gi√°c cho ta:

$$\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)$$

$$\sin(\alpha + \beta) = \cos(\alpha)\sin(\beta) + \sin(\alpha)\cos(\beta)$$
- ƒê·ªÉ xem ch·ª©ng minh, h√£y xem [ph·∫ßn trang Wikipedia n√†y](https://en.wikipedia.org/wiki/Proofs_of_trigonometric_identities#Angle_sum_identities).



- [Ch·ª©ng minh: C√¥ng th·ª©c c·ªông g√≥c L∆∞·ª£ng gi√°c (SIN v√† COS)](https://www.youtube.com/watch?v=i_F-s2G-xDc)
Video n√†y gi·∫£i th√≠ch c√°ch ch·ª©ng minh c√°c c√¥ng th·ª©c c·ªông g√≥c cho sin v√† cosin, v·ªën l√† c√°c c√¥ng th·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong ·∫£nh c·ªßa b·∫°n.

- V√¨ ƒë·ªô d√†i c·ªßa vector kh√¥ng ƒë·ªïi,

$$x' = r * \cos(\alpha)\cos(\beta) - r * \sin(\alpha)\sin(\beta)$$
$$y' = r * \cos(\alpha)\sin(\beta) + r * \sin(\alpha)\cos(\beta)$$

- ƒêi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi:

$$x' = x * \cos(\beta) - y * \sin(\beta)$$
$$y' = x * \sin(\beta) + y * \cos(\beta)$$

- Vi·∫øt d∆∞·ªõi d·∫°ng ph√©p nh√¢n ma tr·∫≠n v·ªõi **vector h√†ng**, ta c√≥:

$$[x', y'] = [x, y] \cdot \begin{bmatrix} \cos(\beta) & \sin(\beta) \\ -\sin(\beta) & \cos(\beta) \end{bmatrix}$$

- v·ªõi ma tr·∫≠n quay b·∫±ng,

$$R = \begin{bmatrix} \cos(\beta) & \sin(\beta) \\ -\sin(\beta) & \cos(\beta) \end{bmatrix}$$

- Vi·∫øt d∆∞·ªõi d·∫°ng ph√©p nh√¢n ma tr·∫≠n v·ªõi **vector c·ªôt**, ta c√≥:

$$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix}$$

- v·ªõi ma tr·∫≠n quay b·∫±ng,

$$R = \begin{bmatrix} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{bmatrix}$$

- L∆∞u √Ω r·∫±ng v·ªã tr√≠ c·ªßa $-\sin(\beta)$ trong ma tr·∫≠n quay ƒë√£ thay ƒë·ªïi.

#### **Ph√©p quay c√πng chi·ªÅu kim ƒë·ªìng h·ªì (Clockwise Rotation)**

- N·∫øu ph√©p quay l√† c√πng chi·ªÅu kim ƒë·ªìng h·ªì, th√¨ ma tr·∫≠n quay ƒë·ªÉ nh√¢n v·ªõi **vector h√†ng** tr·ªü th√†nh,

$$R = \begin{bmatrix} \cos(-\beta) & \sin(-\beta) \\ -\sin(-\beta) & \cos(-\beta) \end{bmatrix}$$

- V√¨ $\sin(-\beta) = -\sin(\beta)$ v√† $\cos(-\beta) = \cos(\beta)$

- ƒëi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi

$$R = \begin{bmatrix} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{bmatrix}$$

- V√¨ v·∫≠y, ph√©p quay c√πng chi·ªÅu kim ƒë·ªìng h·ªì c·ªßa m·ªôt vector $[x, y]$ c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn l√†,

$$[x', y'] = [x, y] \cdot \begin{bmatrix} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{bmatrix}$$

- Ma tr·∫≠n quay ƒë·ªÉ nh√¢n v·ªõi **vector c·ªôt** tr·ªü th√†nh,

$$R = \begin{bmatrix} \cos(-\beta) & -\sin(-\beta) \\ \sin(-\beta) & \cos(-\beta) \end{bmatrix}$$

- t∆∞∆°ng ƒë∆∞∆°ng v·ªõi,

$$R = \begin{bmatrix} \cos(\beta) & \sin(\beta) \\ -\sin(\beta) & \cos(\beta) \end{bmatrix}$$
- V√¨ v·∫≠y, ph√©p quay c√πng chi·ªÅu kim ƒë·ªìng h·ªì c·ªßa m·ªôt vector 

$$\begin{bmatrix} x \\ y \end{bmatrix}$$

- c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn l√†,

$$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\beta) & \sin(\beta) \\ -\sin(\beta) & \cos(\beta) \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix}$$

**T√°c gi·∫£:** Reinoud Bosch

### **Gi·∫£i m·ªôt s·ªë b√†i t·∫≠p ƒë·ªÉ hi·ªÉu c√°ch t√≠nh Euclidian v√† Cosine Similarity**

> C√¢u 1: T√≠nh Euclidian

![15_C√¢u_1](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/15_C√¢u_1.png)


T·∫•t nhi√™n, ƒë√¢y l√† c√°ch t√≠nh chi ti·∫øt.

ƒê√°p √°n **5.91608** l√† ch√≠nh x√°c.

#### üìê C√¥ng th·ª©c Kho·∫£ng c√°ch Euclidean (Euclidean Distance)

Kho·∫£ng c√°ch Euclidean gi·ªØa hai vector $A = (A_1, A_2, A_3)$ v√† $B = (B_1, B_2, B_3)$ ƒë∆∞·ª£c t√≠nh b·∫±ng c√¥ng th·ª©c:

$$d(A, B) = \sqrt{(B_1 - A_1)^2 + (B_2 - A_2)^2 + (B_3 - A_3)^2}$$

N√≥i ƒë∆°n gi·∫£n, ƒë√≥ l√†:
1.  T√≠nh **hi·ªáu** ·ªü t·ª´ng chi·ªÅu.
2.  **B√¨nh ph∆∞∆°ng** c√°c k·∫øt qu·∫£ hi·ªáu ƒë√≥.
3.  **C·ªông** t·∫•t c·∫£ l·∫°i.
4.  L·∫•y **cƒÉn b·∫≠c hai** c·ªßa t·ªïng cu·ªëi c√πng.

---

#### üìù C√°c b∆∞·ªõc t√≠nh to√°n

V·ªõi hai vector c·ªßa b·∫°n:
* $A = (1, 2, 3)$
* $B = (4, 7, 2)$

**1. T√≠nh hi·ªáu (B - A):**
* $B_1 - A_1 = 4 - 1 = 3$
* $B_2 - A_2 = 7 - 2 = 5$
* $B_3 - A_3 = 2 - 3 = -1$

**2. B√¨nh ph∆∞∆°ng c√°c hi·ªáu:**
* $(3)^2 = 9$
* $(5)^2 = 25$
* $(-1)^2 = 1$

**3. C·ªông c√°c k·∫øt qu·∫£ b√¨nh ph∆∞∆°ng:**
$$9 + 25 + 1 = 35$$

**4. L·∫•y cƒÉn b·∫≠c hai:**
$$d(A, B) = \sqrt{35}$$

Khi t√≠nh gi√° tr·ªã th·∫≠p ph√¢n:
$$\sqrt{35} \approx 5.916079...$$

Gi√° tr·ªã n√†y kh·ªõp v·ªõi ƒë√°p √°n ƒë∆∞·ª£c ch·ªçn l√† **5.91608**.

> C√¢u 2: T√≠nh Cosine Similarity

![16_C√¢u_2](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Image_Module_01/M1_W3/16_C√¢u_2.png)

- H∆∞·ªõng d·∫´n gi·∫£i quy·∫øt b√†i to√°n

Ch√†o b·∫°n, h∆∞·ªõng d·∫´n trong √¥ m√†u xanh l√† ho√†n to√†n ch√≠nh x√°c.

B√†i to√°n n√†y d·ª±a tr√™n m·ªôt kh√°i ni·ªám ph·ªï bi·∫øn trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n g·ªçi l√† "vector analogy" (t∆∞∆°ng t·ª± vector). √ù t∆∞·ªüng l√† m·ªëi quan h·ªá gi·ªØa m·ªôt qu·ªëc gia v√† th·ªß ƒë√¥ c·ªßa n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng m·ªôt vector.

M·ªëi quan h·ªá n√†y c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh b·∫±ng:
$V_{\text{quan h·ªá}} = \text{Vector}(\text{USA}) - \text{Vector}(\text{Washington})$

Ch√∫ng ta c√≥ th·ªÉ √°p d·ª•ng vector quan h·ªá n√†y cho m·ªôt th·ªß ƒë√¥ kh√°c (Ankara) ƒë·ªÉ t√¨m ra qu·ªëc gia t∆∞∆°ng ·ª©ng:
$\text{Vector}(\text{Qu·ªëc gia c·∫ßn t√¨m}) \approx V_{\text{quan h·ªá}} + \text{Vector}(\text{Ankara})$

K·∫øt h·ª£p l·∫°i, ch√∫ng ta c√≥ c√¥ng th·ª©c m√† ƒë·ªÅ b√†i g·ª£i √Ω:
**$\text{Vector}(\text{Qu·ªëc gia c·∫ßn t√¨m}) = (\text{USA} - \text{Washington}) + \text{Ankara}$**

Sau khi t√≠nh ra "Vector qu·ªëc gia c·∫ßn t√¨m" n√†y, ch√∫ng ta s·∫Ω d√πng **cosine similarity** ƒë·ªÉ so s√°nh n√≥ v·ªõi c√°c vector qu·ªëc gia ƒë√£ cho ($Turkey$, $Russia$, $Japan$) v√† ch·ªçn qu·ªëc gia c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t.

---

#### 1. T√≠nh "Vector Qu·ªëc gia c·∫ßn t√¨m" (Target Vector)

H√£y g·ªçi vector n√†y l√† $V_{\text{target}}$.

* $USA = (5, 6)$
* $Washington = (10, 5)$
* $Ankara = (9, 1)$

$$V_{\text{target}} = ( (5, 6) - (10, 5) ) + (9, 1)$$

* **B∆∞·ªõc 1: Tr·ª´ vector**
    $$(5, 6) - (10, 5) = (5 - 10, 6 - 5) = (-5, 1)$$

* **B∆∞·ªõc 2: C·ªông vector**
    $$V_{\text{target}} = (-5, 1) + (9, 1) = (-5 + 9, 1 + 1) = (4, 2)$$

V·∫≠y, vector ch√∫ng ta c·∫ßn t√¨m l√† **$V_{\text{target}} = (4, 2)$**.

---

#### 2. So s√°nh $V_{\text{target}}$ v·ªõi c√°c Qu·ªëc gia

B√¢y gi·ªù, ch√∫ng ta s·∫Ω t√≠nh cosine similarity gi·ªØa $V_{\text{target}} = (4, 2)$ v·ªõi t·ª´ng vector qu·ªëc gia.

C√¥ng th·ª©c: $\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$

* **ƒê·ªô l·ªõn c·ªßa $V_{\text{target}} = (4, 2)$:**
    $\|V_{\text{target}}\| = \sqrt{4^2 + 2^2} = \sqrt{16 + 4} = \sqrt{20}$

##### A. So s√°nh v·ªõi Turkey = (3, 1)
* **T√≠ch v√¥ h∆∞·ªõng:** $(4 \times 3) + (2 \times 1) = 12 + 2 = 14$
* **ƒê·ªô l·ªõn:** $\|Turkey\| = \sqrt{3^2 + 1^2} = \sqrt{9 + 1} = \sqrt{10}$
* **Cosine Similarity:**
    $$\cos(\theta) = \frac{14}{\sqrt{20} \times \sqrt{10}} = \frac{14}{\sqrt{200}} \approx 0.9899$$

##### B. So s√°nh v·ªõi Russia = (5, 5)
* **T√≠ch v√¥ h∆∞·ªõng:** $(4 \times 5) + (2 \times 5) = 20 + 10 = 30$
* **ƒê·ªô l·ªõn:** $\|Russia\| = \sqrt{5^2 + 5^2} = \sqrt{25 + 25} = \sqrt{50}$
* **Cosine Similarity:**
    $$\cos(\theta) = \frac{30}{\sqrt{20} \times \sqrt{50}} = \frac{30}{\sqrt{1000}} \approx 0.9487$$

##### C. So s√°nh v·ªõi Japan = (4, 3)
* **T√≠ch v√¥ h∆∞·ªõng:** $(4 \times 4) + (2 \times 3) = 16 + 6 = 22$
* **ƒê·ªô l·ªõn:** $\|Japan\| = \sqrt{4^2 + 3^2} = \sqrt{16 + 9} = \sqrt{25} = 5$
* **Cosine Similarity:**
    $$\cos(\theta) = \frac{22}{\sqrt{20} \times 5} = \frac{22}{\sqrt{20} \times \sqrt{25}} = \frac{22}{\sqrt{500}} \approx 0.9839$$

---

#### 3. K·∫øt lu·∫≠n

So s√°nh c√°c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng:
* **Turkey:** 0.9899 (Cao nh·∫•t)
* **Japan:** 0.9839
* **Russia:** 0.9487

Vector c·ªßa **Turkey** c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine cao nh·∫•t v·ªõi vector (USA - Washington) + Ankara. Do ƒë√≥, ƒë√°p √°n ch√≠nh x√°c l√† **Turkey**.