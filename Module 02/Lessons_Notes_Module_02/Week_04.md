# **Module 02 - Natural Language Processing with Probabilistic Models**
## **Week 4: Word Embeddings with Neural Network**
---
### **Overview**
---

Ná»™i dung tuáº§n nÃ y táº­p trung vÃ o **word vectors**, cÃ²n Ä‘Æ°á»£c gá»i lÃ  **word embeddings**, vÃ  cÃ¡ch `training` (huáº¥n luyá»‡n) chÃºng tá»« Ä‘áº§u.

#### Understanding Word Vectors

- **Word vectors** ráº¥t cáº§n thiáº¿t cho cÃ¡c á»©ng dá»¥ng khÃ¡c nhau trong `natural language processing` (`NLP`), cháº³ng háº¡n nhÆ° `sentiment analysis` (phÃ¢n tÃ­ch tÃ¬nh cáº£m) vÃ  `machine translation` (dá»‹ch mÃ¡y).
- ChÃºng cho phÃ©p biá»ƒu diá»…n sá»‘ há»c cá»§a cÃ¡c tá»«, táº¡o Ä‘iá»u kiá»‡n cho viá»‡c sá»­ dá»¥ng chÃºng trong cÃ¡c `mathematical models`.

#### Training Word Vectors

- KhÃ³a há»c sáº½ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p táº¡o **word embeddings**, bao gá»“m **continuous bag-of-words model** (`CBOW`).
- CÃ¡c ká»¹ thuáº­t khÃ¡c nhÆ° **GloVe** vÃ  **Word2Vec** cÅ©ng sáº½ Ä‘Æ°á»£c Ä‘á» cáº­p, nhÆ°ng trá»ng tÃ¢m sáº½ lÃ  **continuous bag-of-words model**.

#### Preparing Text for Machine Learning

- NgÆ°á»i há»c sáº½ biáº¿t cÃ¡ch biáº¿n Ä‘á»•i dá»¯ liá»‡u vÄƒn báº£n thÃ nh má»™t `training set` phÃ¹ há»£p cho cÃ¡c `machine learning models`.
- Lá»i khuyÃªn thá»±c táº¿ sáº½ Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ lÃ m viá»‡c vá»›i cÃ¡c `text corpora` Ä‘a dáº¡ng, cháº³ng háº¡n nhÆ° sÃ¡ch vÃ  `tweets`.

> **Word embeddings** (nhÃºng tá»«) Ä‘Æ°á»£c sá»­ dá»¥ng trong háº§u háº¿t cÃ¡c `NLP applications`. Báº¥t cá»© khi nÃ o báº¡n xá»­ lÃ½ vÄƒn báº£n, trÆ°á»›c tiÃªn báº¡n pháº£i tÃ¬m cÃ¡ch Ä‘á»ƒ `encode` (mÃ£ hÃ³a) cÃ¡c tá»« dÆ°á»›i dáº¡ng sá»‘. `Word embedding` lÃ  má»™t ká»¹ thuáº­t ráº¥t phá»• biáº¿n cho phÃ©p báº¡n lÃ m Ä‘iá»u Ä‘Ã³.

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i `applications` (á»©ng dá»¥ng) cá»§a `word embeddings` mÃ  báº¡n sáº½ cÃ³ thá»ƒ triá»ƒn khai khi hoÃ n thÃ nh chuyÃªn ngÃ nh nÃ y.

![01_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/01_Overview.png)

![02_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/02_Overview.png)

#### Má»¥c tiÃªu há»c táº­p trong tuáº§n

Äáº¿n cuá»‘i tuáº§n nÃ y, báº¡n sáº½ cÃ³ thá»ƒ:

- XÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m chÃ­nh cá»§a **word representations** (biá»ƒu diá»…n tá»«).
- Táº¡o ra **word embeddings**.
- Chuáº©n bá»‹ vÄƒn báº£n cho **machine learning**.
- Triá»ƒn khai **continuous bag-of-words model**.

---
### **Basic Word Representations**
---
Ná»™i dung táº­p trung vÃ o viá»‡c biá»ƒu diá»…n cÃ¡c tá»« trong má»™t `vocabulary` (tá»« vá»±ng) báº±ng cÃ¡c `numerical vectors` (vÃ©c-tÆ¡ sá»‘), cá»¥ thá»ƒ thÃ´ng qua khÃ¡i niá»‡m **one-hot vectors**.

#### Understanding One-Hot Vectors

- Má»—i tá»« trong má»™t `vocabulary` Ä‘Æ°á»£c gÃ¡n má»™t sá»‘ nguyÃªn (`integer`) duy nháº¥t, nhÆ°ng phÆ°Æ¡ng phÃ¡p nÃ y thiáº¿u **semantic meaning** (Ã½ nghÄ©a ngá»¯ nghÄ©a).
- **One-hot vectors** biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng cÃ¡c `binary vectors` (vÃ©c-tÆ¡ nhá»‹ phÃ¢n), trong Ä‘Ã³ '1' cho biáº¿t sá»± hiá»‡n diá»‡n cá»§a má»™t tá»« vÃ  '0' cho biáº¿t sá»± váº¯ng máº·t.

#### Advantages and Limitations of One-Hot Vectors

- **One-hot vectors** Ä‘Æ¡n giáº£n vÃ  khÃ´ng ngá»¥ Ã½ báº¥t ká»³ má»‘i quan há»‡ nÃ o giá»¯a cÃ¡c tá»«.
- Tuy nhiÃªn, chÃºng cÃ³ thá»ƒ trá»Ÿ nÃªn ráº¥t lá»›n vÃ  **khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a** hoáº·c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»«, dáº«n Ä‘áº¿n nhá»¯ng háº¡n cháº¿ trong cÃ¡c `natural language processing tasks`.

#### Transition to Word Embeddings

- Cuá»™c tháº£o luáº­n táº¡o tiá»n Ä‘á» cho viá»‡c giá»›i thiá»‡u **word embeddings**, cÃ¡i mÃ  nháº±m má»¥c Ä‘Ã­ch giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ cá»§a `one-hot vectors` báº±ng cÃ¡ch náº¯m báº¯t cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a (`semantic relationships`) giá»¯a cÃ¡c tá»«.

> CÃ¡c biá»ƒu diá»…n tá»« cÆ¡ báº£n cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh cÃ¡c dáº¡ng sau:

- **Integers** (Sá»‘ nguyÃªn)
- **One-hot vectors**
- **Word embeddings**

![03_Basic_Word_Representations](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/03_Basic_Word_Representations.png)

> á» bÃªn trÃ¡i, báº¡n cÃ³ má»™t vÃ­ dá»¥ trong Ä‘Ã³ báº¡n sá»­ dá»¥ng sá»‘ nguyÃªn (`integers`) Ä‘á»ƒ biá»ƒu diá»…n má»™t tá»«. Váº¥n Ä‘á» á»Ÿ Ä‘Ã³ lÃ  khÃ´ng cÃ³ lÃ½ do gÃ¬ khiáº¿n tá»« nÃ y tÆ°Æ¡ng á»©ng vá»›i má»™t sá»‘ lá»›n hÆ¡n tá»« khÃ¡c. Äá»ƒ kháº¯c phá»¥c váº¥n Ä‘á» nÃ y, chÃºng ta giá»›i thiá»‡u **one hot vectors** (sÆ¡ Ä‘á»“ bÃªn pháº£i). Äá»ƒ triá»ƒn khai `one hot vectors`, báº¡n pháº£i `initialize` (khá»Ÿi táº¡o) má»™t `vector` toÃ n sá»‘ khÃ´ng (`zeros`) cÃ³ **dimension $V$** vÃ  sau Ä‘Ã³ Ä‘áº·t sá»‘ **1** vÃ o `index` tÆ°Æ¡ng á»©ng vá»›i tá»« báº¡n Ä‘ang biá»ƒu diá»…n.

> **Æ¯u Ä‘iá»ƒm** (`Pros`) cá»§a `one-hot vectors`:
- ÄÆ¡n giáº£n.
- KhÃ´ng yÃªu cáº§u thá»© tá»± ngá»¥ Ã½ (`implied ordering`).

> **NhÆ°á»£c Ä‘iá»ƒm** (`Cons`) cá»§a `one-hot vectors`:
- Ráº¥t lá»›n (`huge`).
- KhÃ´ng `encode` (mÃ£ hÃ³a) Ä‘Æ°á»£c Ã½ nghÄ©a (`meaning`).

---
### **Word Embeddings**
---

Ná»™i dung nÃ y táº­p trung vÃ o khÃ¡i niá»‡m **word embeddings**, má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ `encode` (mÃ£ hÃ³a) Ã½ nghÄ©a cá»§a cÃ¡c tá»« trong má»™t **low-dimensional vector space** (khÃ´ng gian vÃ©c-tÆ¡ chiá»u tháº¥p).

#### Understanding Word Embeddings

- **Word embeddings** biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng cÃ¡c `vectors` theo cÃ¡ch náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a cá»§a chÃºng, cho phÃ©p so sÃ¡nh dá»±a trÃªn sá»± gáº§n gÅ©i trong `vector space`.
- CÃ¡c tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh vá»‹ dá»c theo hai trá»¥c: má»™t cho **sentiment** (tá»« tÃ­ch cá»±c Ä‘áº¿n tiÃªu cá»±c) vÃ  má»™t cho **concreteness** (tá»« cá»¥ thá»ƒ Ä‘áº¿n trá»«u tÆ°á»£ng).

#### Creating Word Vectors

- Má»™t `two-dimensional vector` (vÃ©c-tÆ¡ hai chiá»u) cÃ³ thá»ƒ biá»ƒu diá»…n cÃ¡c tá»«, trong Ä‘Ã³ cÃ¡c `coordinates` (tá»a Ä‘á»™) chá»‰ ra `sentiment` vÃ  má»©c Ä‘á»™ trá»«u tÆ°á»£ng cá»§a chÃºng.
- Biá»ƒu diá»…n nÃ y cho phÃ©p xÃ¡c Ä‘á»‹nh sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»«, cháº³ng háº¡n nhÆ° "happy" vÃ  "excited" gáº§n nhau hÆ¡n "paper."

#### Applications and Importance

- **Word embeddings** táº¡o Ä‘iá»u kiá»‡n cho cÃ¡c `natural language processing` (`NLP`) `tasks` khÃ¡c nhau, bao gá»“m **analogies** (sá»± tÆ°Æ¡ng tá»±) vÃ  **sentence meaning encoding** (mÃ£ hÃ³a Ã½ nghÄ©a cÃ¢u).
- BÃ i giáº£ng nháº¥n máº¡nh ráº±ng viá»‡c táº¡o `word embeddings` lÃ  má»™t má»¥c tiÃªu chÃ­nh cá»§a mÃ´-Ä‘un nÃ y, dáº«n Ä‘áº¿n cÃ¡c `NLP applications` phá»©c táº¡p hÆ¡n nhÆ° **question answering** vÃ  **translation**.

> Váº­y táº¡i sao láº¡i sá»­ dá»¥ng **word embeddings**? HÃ£y cÃ¹ng xem.

![04_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/04_Word_Embeddings.png)

> Tá»« sÆ¡ Ä‘á»“ trÃªn, báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng khi `encode` (mÃ£ hÃ³a) má»™t tá»« trong khÃ´ng gian **2D**, cÃ¡c tá»« tÆ°Æ¡ng tá»± cÃ³ xu hÆ°á»›ng náº±m gáº§n nhau. CÃ³ láº½ **coordinate** Ä‘áº§u tiÃªn Ä‘áº¡i diá»‡n cho viá»‡c má»™t tá»« lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c. **Coordinate** thá»© hai cho báº¡n biáº¿t tá»« Ä‘Ã³ lÃ  **abstract** (trá»«u tÆ°á»£ng) hay **concrete** (cá»¥ thá»ƒ). ÄÃ¢y chá»‰ lÃ  má»™t vÃ­ dá»¥, trong tháº¿ giá»›i thá»±c, báº¡n sáº½ tÃ¬m tháº¥y cÃ¡c `embeddings` vá»›i hÃ ng trÄƒm **dimensions** (chiá»u). Báº¡n cÃ³ thá»ƒ coi má»—i **coordinate** lÃ  má»™t con sá»‘ cho báº¡n biáº¿t Ä‘iá»u gÃ¬ Ä‘Ã³ vá» tá»« Ä‘Ã³.

> Æ¯u Ä‘iá»ƒm cá»§a Word Embeddings

- **Low dimensions** (Chiá»u tháº¥p) (Ã­t hÆ¡n $V$, kÃ­ch thÆ°á»›c `vocabulary`).
- Cho phÃ©p báº¡n `encode` (mÃ£ hÃ³a) Ã½ nghÄ©a (`meaning`).

---
### **How to Create Word Embeddings**
---

Ná»™i dung nÃ y táº­p trung vÃ o quÃ¡ trÃ¬nh táº¡o **word embeddings** trong `natural language processing` (`NLP`).

#### CÃ¡c ThÃ nh pháº§n Thiáº¿t yáº¿u

Äá»ƒ táº¡o **word embeddings** cáº§n hai thÃ nh pháº§n chÃ­nh:

- **Corpus** (Kho ngá»¯ liá»‡u) vÄƒn báº£n.
- **Embedding method** (PhÆ°Æ¡ng phÃ¡p nhÃºng).

`Corpus` pháº£i liÃªn quan Ä‘áº¿n ngá»¯ cáº£nh. VÃ­ dá»¥, Ä‘á»ƒ táº¡o `Shakespearean embeddings`, báº¡n cáº§n sá»­ dá»¥ng vÄƒn báº£n gá»‘c cá»§a Shakespeare chá»© khÃ´ng pháº£i chá»‰ lÃ  cÃ¡c ghi chÃº tÃ³m táº¯t.

#### Táº§m quan trá»ng cá»§a Context

- **Context** (Ngá»¯ cáº£nh) Ä‘á» cáº­p Ä‘áº¿n cÃ¡c tá»« xung quanh cung cáº¥p Ã½ nghÄ©a cho má»—i **word embedding**.
- Má»™t `vocabulary list` Ä‘Æ¡n giáº£n lÃ  khÃ´ng Ä‘á»§; cáº§n cÃ³ má»™t `corpus` toÃ n diá»‡n Ä‘á»ƒ náº¯m báº¯t cÃ¡c sáº¯c thÃ¡i ngá»¯ nghÄ©a.

#### PhÆ°Æ¡ng phÃ¡p vÃ  GiÃ¡m sÃ¡t

- **Embedding method**, thÆ°á»ng dá»±a trÃªn cÃ¡c `machine learning models`, táº¡o ra **word embeddings** tá»« `corpus`.
- `Learning task` cÃ³ thá»ƒ lÃ  **self-supervised** (tá»± giÃ¡m sÃ¡t), táº­n dá»¥ng dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n trong khi `model` tá»± cung cáº¥p ngá»¯ cáº£nh cá»§a riÃªng nÃ³ Ä‘á»ƒ giÃ¡m sÃ¡t.

#### Hyperparameters vÃ  Biá»ƒu diá»…n ToÃ¡n há»c

- **Word embeddings** cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh báº±ng **hyperparameters** (siÃªu tham sá»‘), cháº³ng háº¡n nhÆ° **dimension** (chiá»u) cá»§a cÃ¡c `embedding vectors`, thÆ°á»ng dao Ä‘á»™ng tá»« hÃ ng trÄƒm Ä‘áº¿n hÃ ng nghÃ¬n.
- `Corpus` pháº£i Ä‘Æ°á»£c biáº¿n Ä‘á»•i thÃ nh má»™t **biá»ƒu diá»…n toÃ¡n há»c** phÃ¹ há»£p cho `model`, thÆ°á»ng sá»­ dá»¥ng **integer-based indices** hoáº·c **one-hot vectors**.

Ná»™i dung sáº¯p tá»›i sáº½ giá»›i thiá»‡u cÃ¡c `word embedding methods` khÃ¡c nhau, bao gá»“m cÃ¡ch tiáº¿p cáº­n **continuous bag-of-words** (`CBOW`), cÃ¡i mÃ  sáº½ Ä‘Æ°á»£c triá»ƒn khai trong bÃ i táº­p tiáº¿p theo.

> Äá»ƒ táº¡o **word embeddings**, báº¡n luÃ´n cáº§n má»™t **corpus** (kho ngá»¯ liá»‡u) vÄƒn báº£n vÃ  má»™t **embedding method** (phÆ°Æ¡ng phÃ¡p nhÃºng). **Context** (Ngá»¯ cáº£nh) cá»§a má»™t tá»« cho báº¡n biáº¿t loáº¡i tá»« nÃ o cÃ³ xu hÆ°á»›ng xáº£y ra gáº§n tá»« cá»¥ thá»ƒ Ä‘Ã³. **Context** lÃ  quan trá»ng vÃ¬ Ä‘Ã¢y lÃ  yáº¿u tá»‘ sáº½ mang láº¡i Ã½ nghÄ©a cho má»—i `word embedding`.


#### PhÆ°Æ¡ng phÃ¡p Embeddings vÃ  Tá»± giÃ¡m sÃ¡t

> CÃ³ nhiá»u loáº¡i phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ cho phÃ©p báº¡n há»c cÃ¡c **word embeddings**. `Machine learning model` thá»±c hiá»‡n má»™t `learning task` (nhiá»‡m vá»¥ há»c táº­p), vÃ  sáº£n pháº©m phá»¥ chÃ­nh cá»§a `task` nÃ y lÃ  cÃ¡c `word embeddings`. `Task` cÃ³ thá»ƒ lÃ  há»c cÃ¡ch dá»± Ä‘oÃ¡n má»™t tá»« dá»±a trÃªn cÃ¡c tá»« xung quanh trong má»™t cÃ¢u cá»§a `corpus`, nhÆ° trong trÆ°á»ng há»£p cá»§a **continuous bag-of-words** (`CBOW`).

> `Task` lÃ  **self-supervised** (tá»± giÃ¡m sÃ¡t): nÃ³ vá»«a lÃ  **unsupervised** (khÃ´ng giÃ¡m sÃ¡t) á»Ÿ chá»— dá»¯ liá»‡u Ä‘áº§u vÃ o â€” `corpus` â€” lÃ  **unlabelled** (khÃ´ng cÃ³ nhÃ£n), vÃ  vá»«a lÃ  **supervised** (cÃ³ giÃ¡m sÃ¡t) á»Ÿ chá»— báº£n thÃ¢n dá»¯ liá»‡u cung cáº¥p `context` cáº§n thiáº¿t mÃ  thÃ´ng thÆ°á»ng sáº½ táº¡o thÃ nh cÃ¡c `labels` (nhÃ£n).

> Khi `training word vectors`, cÃ³ má»™t sá»‘ **hyperparameters** (siÃªu tham sá»‘) báº¡n cáº§n Ä‘iá»u chá»‰nh (vÃ­ dá»¥: **dimension** (chiá»u) cá»§a `word vector`).

![05_How_to_Create_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/05_How_to_Create_Word_Embeddings.png)

---
### **Word Embedding Methods**
---

Ná»™i dung nÃ y táº­p trung vÃ o cÃ¡c **word embedding methods** khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng trong `natural language processing`.

#### MÃ´ hÃ¬nh Word2Vec

* **Word2Vec** sá»­ dá»¥ng má»™t **shallow neural network** (máº¡ng nÆ¡-ron nÃ´ng) vá»›i hai `architectures` (kiáº¿n trÃºc): **continuous bag-of-words** (**CBOW**) vÃ  **continuous skip-gram**.
    * **CBOW** dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u dá»±a trÃªn cÃ¡c tá»« xung quanh.
    * **Skip-gram** dá»± Ä‘oÃ¡n cÃ¡c tá»« xung quanh tá»« má»™t tá»« `input` cho trÆ°á»›c.

#### CÃ¡c Ká»¹ thuáº­t Embeddings NÃ¢ng cao

* **GloVe** (`Global Vectors`) phÃ¢n tÃ­ch ma tráº­n **word co-occurrence matrix** (Ä‘á»“ng xuáº¥t hiá»‡n tá»«) Ä‘á»ƒ náº¯m báº¯t Ã½ nghÄ©a cá»§a tá»«.
* **FastText** cáº£i tiáº¿n `skip-gram` báº±ng cÃ¡ch biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng **character n-grams**, cho phÃ©p nÃ³ xá»­ lÃ½ hiá»‡u quáº£ cÃ¡c **unseen words** (tá»« chÆ°a tá»«ng tháº¥y).

### Contextual Word Embeddings

* CÃ¡c `models` tiÃªn tiáº¿n nhÆ° **BERT**, **ELMo**, vÃ  **GPT-2** táº¡o ra cÃ¡c `embeddings` khÃ¡c nhau cho cÃ¡c tá»« dá»±a trÃªn **context** (ngá»¯ cáº£nh) cá»§a chÃºng, há»— trá»£ **polysemy** (Ä‘a nghÄ©a).
* CÃ¡c `models` nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y dÆ°á»›i dáº¡ng **pretrained versions** (phiÃªn báº£n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c) trá»±c tuyáº¿n vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c **fine-tuned** (tinh chá»‰nh) vá»›i cÃ¡c `corpora` cá»¥ thá»ƒ Ä‘á»ƒ cÃ³ `performance` tá»‘t hÆ¡n.

### ğŸ“š PhÆ°Æ¡ng phÃ¡p Word Embedding

#### PhÆ°Æ¡ng phÃ¡p Cá»• Ä‘iá»ƒn (`Classical Methods`)

* **word2vec** (Google, 2013):
    * **Continuous bag-of-words (CBOW)**: `model` há»c cÃ¡ch **dá»± Ä‘oÃ¡n** tá»« trung tÃ¢m (`center word`) cho trÆ°á»›c cÃ¡c `context words` (tá»« ngá»¯ cáº£nh).
    * **Continuous skip-gram / Skip-gram with negative sampling (SGNS)**: `model` há»c cÃ¡ch **dá»± Ä‘oÃ¡n** cÃ¡c tá»« xung quanh (`surrounding words`) cho trÆ°á»›c má»™t tá»« `input`.

* **Global Vectors (GloVe)** (Stanford, 2014): PhÃ¢n tÃ­ch `logarithm` cá»§a **word co-occurrence matrix** (ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»«) cá»§a `corpus`, tÆ°Æ¡ng tá»± nhÆ° `count matrix` báº¡n Ä‘Ã£ sá»­ dá»¥ng trÆ°á»›c Ä‘Ã¢y.
* **fastText** (Facebook, 2016): Dá»±a trÃªn `skip-gram model` vÃ  tÃ­nh Ä‘áº¿n cáº¥u trÃºc cá»§a tá»« báº±ng cÃ¡ch biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng `n-gram` cá»§a kÃ½ tá»±. NÃ³ há»— trá»£ cÃ¡c tá»« **out-of-vocabulary (OOV)**.

#### Deep Learning, Contextual Embeddings

Trong cÃ¡c `models` tiÃªn tiáº¿n hÆ¡n nÃ y, cÃ¡c tá»« cÃ³ cÃ¡c `embeddings` khÃ¡c nhau tÃ¹y thuá»™c vÃ o **context** (ngá»¯ cáº£nh) cá»§a chÃºng. Báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng cÃ¡c `pre-trained embeddings` (embeddings Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c) cho cÃ¡c `models` sau:

* **BERT** (Google, 2018)
* **ELMo** (Allen Institute for AI, 2018)
* **GPT-2** (OpenAI, 2018)

---
### **Continuous Bag-of-Words Model**
---

Ná»™i dung táº­p trung vÃ o viá»‡c triá»ƒn khai **continuous bag-of-words model** (**CBOW**) Ä‘á»ƒ táº¡o `word embeddings` trong `natural language processing`.

#### QuÃ¡ trÃ¬nh tá»•ng thá»ƒ cá»§a Word Embeddings

* **Word embeddings** Ä‘Æ°á»£c táº¡o ra thÃ´ng qua má»™t `machine learning model` há»c tá»« má»™t `corpus` (kho ngá»¯ liá»‡u).
* **Continuous bag-of-words model** dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u (**center word**) dá»±a trÃªn cÃ¡c **context words** (tá»« ngá»¯ cáº£nh) xung quanh nÃ³.

#### Táº¡o Dá»¯ liá»‡u Huáº¥n luyá»‡n

* **Context words** Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  cÃ¡c tá»« bao quanh má»™t **center word**, vá»›i má»™t **hyperparameter $C$** xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng `context words` (bÃ¡n kÃ­nh cá»­a sá»• ngá»¯ cáº£nh).
* `Model` sá»­ dá»¥ng **sliding windows** (cá»­a sá»• trÆ°á»£t) Ä‘á»ƒ táº¡o cÃ¡c `training examples` (vÃ­ dá»¥ huáº¥n luyá»‡n), trong Ä‘Ã³ `context words` lÃ  `inputs` vÃ  `center word` lÃ  `target` (má»¥c tiÃªu) Ä‘á»ƒ dá»± Ä‘oÃ¡n.

#### Kiáº¿n trÃºc MÃ´ hÃ¬nh vÃ  Há»c táº­p

* **Model architecture** bao gá»“m `context words` lÃ  `inputs` vÃ  `center words` lÃ  `outputs`.
* Khi `model` há»c, nÃ³ táº¡o ra `word embeddings` nhÆ° má»™t sáº£n pháº©m phá»¥ cá»§a `prediction task` (nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n), náº¯m báº¯t Ä‘Æ°á»£c **semantic relationships** (má»‘i quan há»‡ ngá»¯ nghÄ©a) giá»¯a cÃ¡c tá»«.

> Äá»ƒ táº¡o **word embeddings**, báº¡n cáº§n má»™t `corpus` vÃ  má»™t `learning algorithm` (thuáº­t toÃ¡n há»c táº­p). Sáº£n pháº©m phá»¥ cá»§a `task` nÃ y sáº½ lÃ  má»™t táº­p há»£p cÃ¡c `word embeddings`. Trong trÆ°á»ng há»£p cá»§a **continuous bag-of-words model** (**CBOW**), `objective` (má»¥c tiÃªu) cá»§a `task` lÃ  **dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u** dá»±a trÃªn cÃ¡c tá»« xung quanh nÃ³.

![06_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/06_Continuous_Bag-of-Words_Model.png)

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t **visualization** (hÃ¬nh áº£nh trá»±c quan) cho báº¡n tháº¥y `model` hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.

![07_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/07_Continuous_Bag-of-Words_Model.png)

> NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, **window size** (kÃ­ch thÆ°á»›c cá»­a sá»•) trong hÃ¬nh áº£nh phÃ­a trÃªn lÃ  5. **Context size** (kÃ­ch thÆ°á»›c ngá»¯ cáº£nh), $C$, lÃ  2. $C$ thÆ°á»ng cho báº¡n biáº¿t cÃ³ bao nhiÃªu tá»« trÆ°á»›c hoáº·c sau **center word** (tá»« trung tÃ¢m) mÃ  `model` sáº½ sá»­ dá»¥ng Ä‘á»ƒ Ä‘Æ°a ra **prediction** (dá»± Ä‘oÃ¡n).

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t **visualization** khÃ¡c cho tháº¥y tá»•ng quan vá» `model`.

![08_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/08_Continuous_Bag-of-Words_Model.png)

---
### **Cleaning and Tokenization**
---

Ná»™i dung táº­p trung vÃ o cÃ¡c quy trÃ¬nh **cleaning** (lÃ m sáº¡ch) vÃ  **tokenization** (táº¡o token) trong `natural language processing` (`NLP`).

#### Cleaning vÃ  Tokenization

- CÃ¡c tá»« nÃªn Ä‘Æ°á»£c xá»­ lÃ½ dÆ°á»›i dáº¡ng **case insensitive** (khÃ´ng phÃ¢n biá»‡t chá»¯ hoa/thÆ°á»ng), nghÄ©a lÃ  chÃºng nÃªn Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh má»™t Ä‘á»‹nh dáº¡ng duy nháº¥t (chá»¯ thÆ°á»ng hoáº·c chá»¯ hoa) Ä‘á»ƒ Ä‘á»“ng nháº¥t.
- **Punctuation** (Dáº¥u cÃ¢u) cáº§n Ä‘Æ°á»£c xá»­ lÃ½ cáº©n tháº­n; dáº¥u cÃ¢u gÃ¢y ngáº¯t quÃ£ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng má»™t `special word` (tá»« Ä‘áº·c biá»‡t) duy nháº¥t, trong khi dáº¥u cÃ¢u khÃ´ng gÃ¢y ngáº¯t quÃ£ng cÃ³ thá»ƒ bá»‹ bá» qua.

#### Xá»­ lÃ½ Sá»‘ vÃ  KÃ½ tá»± Äáº·c biá»‡t

- **Numbers** (Sá»‘) cÃ³ thá»ƒ bá»‹ bá» Ä‘i náº¿u chÃºng khÃ´ng quan trá»ng, nhÆ°ng cÃ¡c sá»‘ quan trá»ng nÃªn Ä‘Æ°á»£c giá»¯ láº¡i hoáº·c thay tháº¿ báº±ng má»™t `special token` nhÆ° **\<NUMBER\>**.
- CÃ¡c **Special characters** (KÃ½ tá»± Ä‘áº·c biá»‡t), cháº³ng háº¡n nhÆ° kÃ½ hiá»‡u toÃ¡n há»c vÃ  `emojis`, nÃªn Ä‘Æ°á»£c quáº£n lÃ½ dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan cá»§a chÃºng vá»›i `model`.

### VÃ­ dá»¥ Thá»±c hÃ nh

Má»™t `Python example` minh há»a cÃ¡ch `clean` má»™t `corpus` báº±ng cÃ¡ch gá»™p `punctuation` vÃ  **tokenizing** vÄƒn báº£n báº±ng cÃ¡ch sá»­ dá»¥ng `NLTK library`, táº¡o ra má»™t `array of tokens` sáºµn sÃ ng Ä‘á»ƒ phÃ¢n tÃ­ch thÃªm.

Äiá»u nÃ y táº¡o tiá»n Ä‘á» cho chá»§ Ä‘á» tiáº¿p theo vá» **continuous bag-of-words model**.

> TrÆ°á»›c khi triá»ƒn khai báº¥t ká»³ thuáº­t toÃ¡n `natural language processing` (`NLP`) nÃ o, báº¡n cÃ³ thá»ƒ muá»‘n `clean` (lÃ m sáº¡ch) dá»¯ liá»‡u vÃ  `tokenize` (táº¡o token) nÃ³. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i Ä‘iá»u cáº§n lÆ°u Ã½ khi xá»­ lÃ½ `data` cá»§a báº¡n.

![09_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/09_Cleaning_and_Tokenization.png)

> Báº¡n cÃ³ thá»ƒ `clean data` báº±ng `Python` nhÆ° sau:

![10_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/10_Cleaning_and_Tokenization.png)

> Báº¡n cÃ³ thá»ƒ thÃªm bao nhiÃªu Ä‘iá»u kiá»‡n tÃ¹y thÃ­ch vÃ o cÃ¡c dÃ²ng tÆ°Æ¡ng á»©ng vá»›i hÃ¬nh chá»¯ nháº­t mÃ u xanh lÃ¡ cÃ¢y phÃ­a trÃªn.

---
### **Sliding Window of Words in Python**
---

Ná»™i dung táº­p trung vÃ o viá»‡c trÃ­ch xuáº¥t **center words** (tá»« trung tÃ¢m) vÃ  **context words** (tá»« ngá»¯ cáº£nh) Ä‘á»ƒ `training` (huáº¥n luyá»‡n) **continuous bag-of-words model** trong `natural language processing`.

#### TrÃ­ch xuáº¥t Center vÃ  Context Words

* QuÃ¡ trÃ¬nh báº¯t Ä‘áº§u vá»›i má»™t `cleaned and tokenized corpus` (kho ngá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch vÃ  táº¡o token), Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t `array` (máº£ng) cÃ¡c tá»«.
* Má»™t `function` (hÃ m) gá»i lÃ  `get_windows` Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a Ä‘á»ƒ trÃ­ch xuáº¥t `center words` vÃ  `context words` cá»§a chÃºng dá»±a trÃªn má»™t `context size` Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh.

#### Triá»ƒn khai Function

* `Function` nháº­n má»™t `array of words` vÃ  má»™t **context size ($C$)**, cÃ¡i mÃ  xÃ¡c Ä‘á»‹nh cÃ³ bao nhiÃªu tá»« sáº½ Ä‘Æ°á»£c xem xÃ©t á»Ÿ má»—i bÃªn cá»§a `center word`.
* NÃ³ `initialize` (khá»Ÿi táº¡o) má»™t vÃ²ng láº·p Ä‘á»ƒ láº·p qua `array`, trÃ­ch xuáº¥t `center words` vÃ  cÃ¡c `context words` tÆ°Æ¡ng á»©ng cá»§a chÃºng.

#### Sá»­ dá»¥ng Function

* `Function` sá»­ dá»¥ng **yield keyword** Ä‘á»ƒ tráº£ vá» cÃ¡c giÃ¡ trá»‹ má»™t cÃ¡ch **iteratively** (láº·p láº¡i), cho phÃ©p **data generation** (táº¡o dá»¯ liá»‡u) hiá»‡u quáº£.
* Má»™t vÃ²ng láº·p Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hiá»ƒn thá»‹ `context` vÃ  `center words`, cÃ¡i mÃ  cáº§n thiáº¿t cho **continuous bag-of-words model**.

Tá»•ng quan, bÃ i giáº£ng nÃ y cung cáº¥p má»™t cÃ¡ch tiáº¿p cáº­n thá»±c táº¿ Ä‘á»ƒ chuáº©n bá»‹ `data` cho `training word embeddings` báº±ng `Python`.

![11_Sliding_Window_of_Words_in_Python](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/11_Sliding_Window_of_Words_in_Python.png)

> `Code` phÃ­a trÃªn cho tháº¥y má»™t `function` (hÃ m) nháº­n hai `parameters` (tham sá»‘).

* `Words`: má»™t `list` (danh sÃ¡ch) cÃ¡c tá»«.
* $C$: **context size** (kÃ­ch thÆ°á»›c ngá»¯ cáº£nh).

> ChÃºng ta báº¯t Ä‘áº§u báº±ng cÃ¡ch Ä‘áº·t $i$ báº±ng $C$. Sau Ä‘Ã³, chÃºng ta tÃ¡ch **center\_word** (tá»« trung tÃ¢m) vÃ  **context\_words** (cÃ¡c tá»« ngá»¯ cáº£nh). ChÃºng ta sau Ä‘Ã³ **yield** (tráº£ vá») cÃ¡c giÃ¡ trá»‹ nÃ y vÃ  **increment** (tÄƒng) $i$ lÃªn.


---
### **Transforming Words into Vectors**
---

Ná»™i dung táº­p trung vÃ o viá»‡c chuáº©n bá»‹ `data` (dá»¯ liá»‡u) cho **continuous bag-of-words model** (**CBOW**) trong `natural language processing`.

#### Chuáº©n bá»‹ Dá»¯ liá»‡u cho MÃ´ hÃ¬nh CBOW

* **Context vÃ  Central Words**: QuÃ¡ trÃ¬nh báº¯t Ä‘áº§u báº±ng viá»‡c xÃ¡c Ä‘á»‹nh **context words** vÃ  **central word** (tá»« trung tÃ¢m) tá»« má»™t `sliding window` (cá»­a sá»• trÆ°á»£t) trÃªn `corpus` (kho ngá»¯ liá»‡u).
* **Vocabulary Creation**: Má»™t **vocabulary** (tá»« vá»±ng) Ä‘Æ°á»£c hÃ¬nh thÃ nh tá»« cÃ¡c tá»« Ä‘á»™c nháº¥t (`unique words`) trong `corpus`, sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o **one-hot vectors** cho cÃ¡c `central words`.

#### Biá»ƒu diá»…n Vector

* **One-Hot Encoding**: Má»—i tá»« trong `vocabulary` Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng **one-hot vector**, trong Ä‘Ã³ '1' cho biáº¿t sá»± hiá»‡n diá»‡n cá»§a má»™t tá»« vÃ  '0' cho biáº¿t sá»± váº¯ng máº·t.
* **Averaging Context Vectors**: Äá»‘i vá»›i **context words**, má»™t `vector` duy nháº¥t Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch **averaging** (tÃ­nh trung bÃ¬nh) cÃ¡c `one-hot vectors` cá»§a má»—i `context word`, cung cáº¥p má»™t biá»ƒu diá»…n cho `context`.

#### Chuáº©n bá»‹ Dá»¯ liá»‡u Huáº¥n luyá»‡n

* **Final Vector Representation**: CÃ¡c `final vectors` (vÃ©c-tÆ¡ cuá»‘i cÃ¹ng) cho cáº£ `central words` vÃ  `context words` Ä‘Æ°á»£c chuáº©n bá»‹ Ä‘á»ƒ `training` (huáº¥n luyá»‡n) **CBOW model**.
* **Transition to Model Learning**: Vá»›i `data` Ä‘Ã£ Ä‘Æ°á»£c biá»ƒu diá»…n Ä‘áº§y Ä‘á»§, bÆ°á»›c tiáº¿p theo lÃ  tÃ¬m hiá»ƒu vá» **architecture** (kiáº¿n trÃºc) cá»§a **CBOW model** vÃ  Ã¡p dá»¥ng cÃ¡c ká»¹ nÄƒng vÃ o cÃ¡c bÃ i táº­p sáº¯p tá»›i.

> Äá»ƒ biáº¿n Ä‘á»•i cÃ¡c **context vectors** (vÃ©c-tÆ¡ ngá»¯ cáº£nh) thÃ nh má»™t **single vector** (vÃ©c-tÆ¡ Ä‘Æ¡n láº»), báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ´ng thá»©c/phÆ°Æ¡ng phÃ¡p sau:

![12_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/12_Transforming_Words_into_Vectors.png)

> NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, chÃºng ta báº¯t Ä‘áº§u vá»›i cÃ¡c **one-hot vectors** cho cÃ¡c tá»« ngá»¯ cáº£nh vÃ  biáº¿n Ä‘á»•i chÃºng thÃ nh má»™t **single vector** báº±ng cÃ¡ch láº¥y **average** (trung bÃ¬nh). Káº¿t quáº£ lÃ  báº¡n nháº­n Ä‘Æ°á»£c cÃ¡c `vectors` sau Ä‘Ã¢y mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cho viá»‡c **training** (huáº¥n luyá»‡n) cá»§a mÃ¬nh.

![13_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/13_Transforming_Words_into_Vectors.png)

---
### **Architecture of the CBOW Model**
---

Ná»™i dung táº­p trung vÃ o **architecture** (kiáº¿n trÃºc) cá»§a **Continuous Bag of Words** (**CBOW**) `model` Ä‘Æ°á»£c sá»­ dá»¥ng trong `natural language processing`.

#### Tá»•ng quan Kiáº¿n trÃºc

* **CBOW model** bao gá»“m má»™t **shallow dense neural network** (máº¡ng nÆ¡-ron dÃ y Ä‘áº·c nÃ´ng) vá»›i má»™t `input layer`, má»™t `hidden layer` (lá»›p áº©n), vÃ  má»™t `output layer`.
* `Input` lÃ  má»™t `vector` cá»§a **context words** (cÃ¡c tá»« ngá»¯ cáº£nh), trong khi `output` lÃ  **center word** (tá»« trung tÃ¢m) Ä‘Æ°á»£c dá»± Ä‘oÃ¡n, cáº£ hai Ä‘á»u cÃ³ kÃ­ch thÆ°á»›c theo **vocabulary** ($V$).

#### Chi tiáº¿t cÃ¡c Lá»›p

* KÃ­ch thÆ°á»›c cá»§a `hidden layer` Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi **dimension** (chiá»u) Ä‘Ã£ chá»n cá»§a **word embeddings** ($N$), thÆ°á»ng dao Ä‘á»™ng tá»« $100$ Ä‘áº¿n $1,000$.
* `Network` lÃ  **fully connected** (káº¿t ná»‘i Ä‘áº§y Ä‘á»§), vá»›i cÃ¡c **weight matrices** ($W_1$ vÃ  $W_2$) vÃ  **bias vectors** ($b_1$ vÃ  $b_2$) mÃ  `model` há»c trong quÃ¡ trÃ¬nh `training`.

#### HÃ m KÃ­ch hoáº¡t (Activation Functions)

* `Hidden layer` sá»­ dá»¥ng **Rectified Linear Units** (**ReLU**) `activation function` (hÃ m kÃ­ch hoáº¡t).
* `Output layer` sá»­ dá»¥ng **softmax function** Ä‘á»ƒ Ä‘Æ°a ra **predictions** (dá»± Ä‘oÃ¡n).

Báº£n tÃ³m táº¯t nÃ y cung cáº¥p sá»± hiá»ƒu biáº¿t ngáº¯n gá»n vá» cáº¥u trÃºc vÃ  cÃ¡c thÃ nh pháº§n cá»§a `CBOW model`.

> `Architecture` (Kiáº¿n trÃºc) cho **CBOW model** cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° sau:

![14_Architecture_of_the_CBOW_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/14_Architecture_of_the_CBOW_Model.png)

> Báº¡n cÃ³ má»™t `input`, $X$, lÃ  giÃ¡ trá»‹ trung bÃ¬nh (`average`) cá»§a táº¥t cáº£ cÃ¡c **context vectors** (vÃ©c-tÆ¡ ngá»¯ cáº£nh). Sau Ä‘Ã³, báº¡n nhÃ¢n nÃ³ vá»›i $W_1$ vÃ  cá»™ng thÃªm $b_1$. Káº¿t quáº£ nÃ y Ä‘i qua má»™t **ReLU function** Ä‘á»ƒ táº¡o ra **hidden layer** (lá»›p áº©n) cá»§a báº¡n. Lá»›p Ä‘Ã³ sau Ä‘Ã³ Ä‘Æ°á»£c nhÃ¢n vá»›i $W_2$ vÃ  báº¡n cá»™ng thÃªm $b_2$. Káº¿t quáº£ nÃ y Ä‘i qua má»™t **softmax** Ä‘á»ƒ cung cáº¥p cho báº¡n má»™t **distribution** (phÃ¢n phá»‘i) trÃªn $V$ (kÃ­ch thÆ°á»›c `vocabulary`). Báº¡n chá»n `vocabulary word` tÆ°Æ¡ng á»©ng vá»›i **arg-max** cá»§a `output`.

---
### **Architecture of the CBOW Model: Dimensions**
---

Ná»™i dung táº­p trung vÃ o viá»‡c hiá»ƒu **dimensions** (chiá»u) cá»§a cÃ¡c lá»›p trong má»™t `neural network model`, cá»¥ thá»ƒ lÃ  **continuous bag of words** (**CBOW**) `model`.

#### Kiáº¿n trÃºc Neural Network

* `Input layer` Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t **column vector** ($x$) vá»›i cÃ¡c sá»‘ khÃ´ng (`zeros`), trong Ä‘Ã³ $V$ lÃ  **vocabulary size** (kÃ­ch thÆ°á»›c tá»« vá»±ng). ($x$ cÃ³ dimension $V \times 1$).
* `Hidden layer` ($h$) Ä‘Æ°á»£c tÃ­nh báº±ng **weighted sum** ($W_1 x + b_1$), trong Ä‘Ã³ $W_1$ lÃ  **weight matrix** (ma tráº­n trá»ng sá»‘) (dimension $N \times V$) vÃ  $b_1$ lÃ  **bias vector** (vÃ©c-tÆ¡ Ä‘á»™ lá»‡ch) (dimension $N \times 1$). ($N$ lÃ  embedding dimension).

#### TÃ­nh toÃ¡n Output

* CÃ¡c giÃ¡ trá»‹ **output layer** Ä‘Æ°á»£c suy ra tá»« `hidden layer` ($h$) báº±ng cÃ¡ch sá»­ dá»¥ng ($W_2 h + b_2$), trong Ä‘Ã³ $W_2$ lÃ  **weight matrix** cho `output layer` (dimension $V \times N$) vÃ  $b_2$ lÃ  **bias vector** tÆ°Æ¡ng á»©ng (dimension $V \times 1$).
* `Output` cuá»‘i cÃ¹ng ($\hat{y}$) Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng cÃ¡ch Ã¡p dá»¥ng **softmax activation function** (hÃ m kÃ­ch hoáº¡t softmax) cho cÃ¡c giÃ¡ trá»‹ `output layer`. ($\hat{y}$ cÃ³ dimension $V \times 1$).

#### Xá»­ lÃ½ cÃ¡c Loáº¡i Vector

* Náº¿u sá»­ dá»¥ng **row vectors** (vÃ©c-tÆ¡ hÃ ng) thay vÃ¬ **column vectors** (vÃ©c-tÆ¡ cá»™t), cÃ¡c phÃ©p tÃ­nh `matrix` pháº£i Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tÆ°Æ¡ng á»©ng, cháº³ng háº¡n nhÆ° **transposing matrices** (chuyá»ƒn vá»‹ ma tráº­n) trong quÃ¡ trÃ¬nh nhÃ¢n.
* Hiá»ƒu rÃµ cÃ¡c `dimensions` nÃ y lÃ  ráº¥t quan trá»ng Ä‘á»ƒ trÃ¡nh cÃ¡c lá»—i **dimension mismatch errors** (lá»—i khÃ´ng khá»›p chiá»u) trong cÃ¡c `programming assignments`.

> CÃ¡c phÆ°Æ¡ng trÃ¬nh cho `model` trÆ°á»›c lÃ :

$$z_1 = W_1 x + b_1$$

$$h = \text{ReLU}(z_1)$$

$$z_2 = W_2 h + b_2$$

$$\hat{y} = \text{softmax}(z_2)$$

> á» Ä‘Ã¢y, báº¡n cÃ³ thá»ƒ tháº¥y cÃ¡c **dimensions** (chiá»u):

![15_Architecture_of_the_CBOW_Model_Dimensions](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/15_Architecture_of_the_CBOW_Model_Dimensions.png)

> HÃ£y Ä‘áº£m báº£o ráº±ng báº¡n xem ká»¹ cÃ¡c phÃ©p **matrix multiplications** (nhÃ¢n ma tráº­n) vÃ  hiá»ƒu táº¡i sao cÃ¡c **dimensions** (chiá»u) láº¡i há»£p lÃ½.

---
### **Architecture of the CBOW Model: Dimensions 2**
---

Ná»™i dung táº­p trung vÃ o khÃ¡i niá»‡m **batch processing** (xá»­ lÃ½ theo lÃ´) trong `Continuous Bag of Words` (**CBOW**) `model` Ä‘Æ°á»£c sá»­ dá»¥ng trong `neural networks`.

#### Batch Processing trong CBOW

* Thay vÃ¬ cung cáº¥p cÃ¡c `individual examples` (vÃ­ dá»¥ riÃªng láº»), nhiá»u `input examples` cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»“ng thá»i, Ä‘iá»u nÃ y giÃºp tÄƒng tá»‘c quÃ¡ trÃ¬nh há»c táº­p.
* **Batch size** ($M$) lÃ  má»™t **hyperparameter** (siÃªu tham sá»‘) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong quÃ¡ trÃ¬nh `training`, cho phÃ©p hÃ¬nh thÃ nh má»™t **matrix** ($X$) tá»« cÃ¡c `input vectors` nÃ y.

#### CÃ¡c PhÃ©p toÃ¡n Matrix

* CÃ¡c giÃ¡ trá»‹ **hidden layer** ($H$) Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch Ã¡p dá»¥ng **ReLU activation function** cho `weighted input matrix` ($Z_1$), cÃ¡i mÃ  bao gá»“m má»™t **bias matrix** ($B_1$).
* **Output matrix** ($\hat{Y}$) Ä‘Æ°á»£c suy ra tá»« `hidden layer` vÃ  bao gá»“m má»™t **replicated bias matrix** ($B_2$), biáº¿n Ä‘á»•i cÃ¡c `input vectors` thÃ nh cÃ¡c `output vectors` tÆ°Æ¡ng á»©ng.

#### HÃ m KÃ­ch hoáº¡t

* BÃ i giáº£ng gá»£i Ã½ vá» viá»‡c giá»›i thiá»‡u cÃ¡c **activation functions** (hÃ m kÃ­ch hoáº¡t) Ä‘Æ°á»£c sá»­ dá»¥ng trong `CBOW model`, cho tháº¥y ngÆ°á»i há»c Ä‘ang tiáº¿n tá»›i viá»‡c xÃ¢y dá»±ng má»™t `model` chá»©c nÄƒng.

> Khi xá»­ lÃ½ **batch input** (Ä‘áº§u vÃ o theo lÃ´), báº¡n cÃ³ thá»ƒ **stack** (xáº¿p chá»“ng) cÃ¡c vÃ­ dá»¥ thÃ nh cÃ¡c **columns** (cá»™t). Sau Ä‘Ã³, báº¡n cÃ³ thá»ƒ tiáº¿n hÃ nh nhÃ¢n cÃ¡c **matrices** (ma tráº­n) nhÆ° sau:

![16_Architecture_of_the_CBOW_Model_Dimensions_2](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/16_Architecture_of_the_CBOW_Model_Dimensions_2.png)

> Trong sÆ¡ Ä‘á»“ phÃ­a trÃªn, báº¡n cÃ³ thá»ƒ tháº¥y cÃ¡c **dimensions** (chiá»u) cá»§a má»—i **matrix**. LÆ°u Ã½ ráº±ng $\hat{Y}$ cá»§a báº¡n cÃ³ **dimension** $V$ nhÃ¢n $M$. Má»—i **column** lÃ  **prediction** (dá»± Ä‘oÃ¡n) cá»§a `column` tÆ°Æ¡ng á»©ng vá»›i cÃ¡c **context words**. VÃ¬ váº­y, `column` Ä‘áº§u tiÃªn trong $\hat{Y}$ lÃ  **prediction** tÆ°Æ¡ng á»©ng vá»›i `column` Ä‘áº§u tiÃªn cá»§a $X$.

---
### **Architecture of the CBOW Model: Activation Functions**
---

Ná»™i dung nÃ y táº­p trung vÃ o hai **activation functions** (hÃ m kÃ­ch hoáº¡t) quan trá»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong trÃ­ tuá»‡ nhÃ¢n táº¡o: **Rectified Linear Unit** (`ReLU`) vÃ  **Softmax function**.

#### âš™ï¸ ReLU Function

* **ReLU** lÃ  má»™t **activation function** Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, nÃ³ chá»‰ kÃ­ch hoáº¡t má»™t **neuron** khi `weighted input` lÃ  dÆ°Æ¡ng, thiáº¿t láº­p táº¥t cáº£ cÃ¡c `inputs` Ã¢m vá» 0.
* CÃ´ng thá»©c cá»§a `ReLU` lÃ :

$$f(z) = \max(0, z)$$

* VÃ­ dá»¥, náº¿u `input vector` chá»©a cÃ¡c giÃ¡ trá»‹ Ã¢m, nhá»¯ng giÃ¡ trá»‹ Ä‘Ã³ sáº½ trá»Ÿ thÃ nh sá»‘ 0 trong `output`, trong khi cÃ¡c giÃ¡ trá»‹ dÆ°Æ¡ng váº«n giá»¯ nguyÃªn.

#### ğŸ“Š Softmax Function

* **Softmax function** nháº­n má»™t `vector of real numbers` (vÃ©c-tÆ¡ cÃ¡c sá»‘ thá»±c) lÃ m `input` vÃ  `output` ra má»™t **probability distribution** (phÃ¢n phá»‘i xÃ¡c suáº¥t), trong Ä‘Ã³ tá»•ng cÃ¡c giÃ¡ trá»‹ báº±ng má»™t.

* CÃ´ng thá»©c cho `Softmax` (Ä‘á»‘i vá»›i pháº§n tá»­ thá»© $i$ trong vector $z$) lÃ :

$$\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

* NÃ³ Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c váº¥n Ä‘á» **multi-class classification** (phÃ¢n loáº¡i Ä‘a lá»›p), vÃ¬ nÃ³ cung cáº¥p `probabilities` cá»§a má»—i `class`, cho phÃ©p giáº£i thÃ­ch `model's predictions`.

TÃ³m láº¡i, `ReLU` giÃºp quáº£n lÃ½ kÃ­ch hoáº¡t **neuron** á»Ÿ `hidden layer`, trong khi `Softmax` lÃ  cáº§n thiáº¿t Ä‘á»ƒ táº¡o **probabilities** á»Ÿ `output layer` trong cÃ¡c `classification tasks`.

> ReLU function

**ReLU function** (`Rectified Linear Unit`), lÃ  má»™t trong nhá»¯ng `activation functions` phá»• biáº¿n nháº¥t. Khi báº¡n Ä‘Æ°a má»™t `vector`, cá»¥ thá»ƒ lÃ  $x$, vÃ o má»™t `ReLU function`. Báº¡n káº¿t thÃºc vá»›i phÃ©p tÃ­nh:

$$x = \max(0, x)$$

ÄÃ¢y lÃ  hÃ¬nh váº½ minh há»a `ReLU`.

![17_Architecture_of_the_CBOW_Model_Dimensions_AF](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/17_Architecture_of_the_CBOW_Model_Dimensions_AF.png)

> Softmax function

**Softmax function** nháº­n má»™t `vector` vÃ  biáº¿n Ä‘á»•i nÃ³ thÃ nh má»™t **probability distribution** (phÃ¢n phá»‘i xÃ¡c suáº¥t). VÃ­ dá»¥, cho trÆ°á»›c `vector` $z$ sau, báº¡n cÃ³ thá»ƒ biáº¿n Ä‘á»•i nÃ³ thÃ nh má»™t **probability distribution** nhÆ° sau.

![18_Architecture_of_the_CBOW_Model_Dimensions_AF](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/18_Architecture_of_the_CBOW_Model_Dimensions_AF.png)

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, báº¡n cÃ³ thá»ƒ tÃ­nh `probability` ($\hat{y}_i$) cá»§a pháº§n tá»­ $i$ nhÆ° sau:

$$\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^{V} e^{z_j}}$$

Trong Ä‘Ã³ $V$ lÃ  kÃ­ch thÆ°á»›c cá»§a `vector` $z$ (tá»©c lÃ  kÃ­ch thÆ°á»›c `vocabulary`).

---
### **Training a CBOW Model: Cost Function**
---

Ná»™i dung nÃ y táº­p trung vÃ o **cost function** (hÃ m chi phÃ­) cho **Softmax** trong `machine learning`, Ä‘áº·c biá»‡t trong bá»‘i cáº£nh dá»± Ä‘oÃ¡n tá»« báº±ng **continuous bag of words model** (**CBOW**).

#### Tá»•ng quan Cost Function

* **Cost function** ráº¥t cáº§n thiáº¿t Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t trong nhá»¯ng tá»« cÃ³ thá»ƒ cÃ³ báº±ng cÃ¡ch tá»‘i thiá»ƒu hÃ³a má»™t chi phÃ­ cá»¥ thá»ƒ.
* Má»™t **training example** (vÃ­ dá»¥ huáº¥n luyá»‡n) Ä‘Æ¡n láº» bao gá»“m má»™t `input`, má»™t **true target** (má»¥c tiÃªu thá»±c táº¿), vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a `model`.

#### Loss Function vÃ  Tham sá»‘

* **Loss function** (hÃ m máº¥t mÃ¡t) Ä‘o lÆ°á»ng sai sá»‘ giá»¯a `prediction` vÃ  **true value** cho má»™t `training example`.
* Trong **CBOW model**, cÃ¡c **parameters** (tham sá»‘) Ä‘Æ°á»£c Ä‘iá»u chá»‰nh bao gá»“m **weight matrices** ($W_1, W_2$) vÃ  **bias factors** ($b_1, b_2$).

#### Cross Entropy Loss

* **Cross entropy loss** (máº¥t mÃ¡t entropy chÃ©o) thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c **classification models** vÃ  cÃ³ liÃªn quan Ä‘áº¿n lá»›p `output Softmax`.
* CÃ´ng thá»©c cho **cross entropy loss** ($J$) liÃªn quan Ä‘áº¿n tá»•ng Ã¢m cá»§a tÃ­ch giá»¯a **true value** ($y_i$) vÃ  `log` cá»§a **predicted value** ($\hat{y}_i$):
$$J = - \sum_{i=1}^{V} y_i \log(\hat{y}_i)$$

#### VÃ­ dá»¥ Dá»± Ä‘oÃ¡n

* **Loss function** thÆ°á»Ÿng cho cÃ¡c `predictions` Ä‘Ãºng vÃ  pháº¡t cÃ¡c `predictions` khÃ´ng chÃ­nh xÃ¡c (cho tháº¥y `loss` tÄƒng lÃªn vá»›i cÃ¡c `predictions` khÃ´ng chÃ­nh xÃ¡c), vá»›i Ã½ nghÄ©a Ä‘á»‘i vá»›i **model performance**.

![19_Training_a_CBOW_Model_Cost_Function](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/19_Training_a_CBOW_Model_Cost_Function.png)

> Má»©c **chi phÃ­ (cost)** lÃ  $4.61$ trong vÃ­ dá»¥ trÃªn lÃ  do **mÃ´ hÃ¬nh Ä‘Ã£ dá»± Ä‘oÃ¡n má»™t xÃ¡c suáº¥t ráº¥t tháº¥p cho tá»« Ä‘Ãºng (true target)**.

> GiÃ¡ trá»‹ $4.61$ khÃ´ng pháº£i lÃ  ngáº«u nhiÃªn; nÃ³ thá»ƒ hiá»‡n má»‘i quan há»‡ nghá»‹ch Ä‘áº£o giá»¯a chi phÃ­ vÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n thÃ´ng qua hÃ m $\log$ tá»± nhiÃªn ($\ln$).

DÆ°á»›i Ä‘Ã¢y lÃ  lÃ½ do chi tiáº¿t:

#### 1. CÃ´ng thá»©c ÄÆ¡n giáº£n hÃ³a

CÃ´ng thá»©c **Cross-Entropy Loss** cho má»™t vÃ­ dá»¥ huáº¥n luyá»‡n Ä‘Æ¡n láº», nÆ¡i **true target ($y$)** lÃ  má»™t **one-hot vector** (chá»‰ cÃ³ 1 á»Ÿ vá»‹ trÃ­ tá»« Ä‘Ãºng $k^*$) Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hÃ³a thÃ nh:

$$J = - \sum_{k=1}^{V} y_k \log(\hat{y}_k) = - \log(\hat{y}_{k^*})$$

Trong Ä‘Ã³:
* $V$ lÃ  kÃ­ch thÆ°á»›c tá»« vá»±ng.
* $y_{k^*}$ lÃ  $1$ (xÃ¡c suáº¥t $100\%$ ráº±ng tá»« $k^*$ lÃ  Ä‘Ãºng).
* $\hat{y}_{k^*}$ lÃ  xÃ¡c suáº¥t mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cho tá»« Ä‘Ãºng $k^*$.

#### 2. TÃ­nh toÃ¡n XÃ¡c suáº¥t Dá»± Ä‘oÃ¡n

Náº¿u chi phÃ­ Ä‘Æ°á»£c tÃ­nh lÃ  $J = 4.61$ (sá»­ dá»¥ng $\log$ tá»± nhiÃªn, $\ln$, lÃ  tiÃªu chuáº©n):

$$4.61 = - \ln(\hat{y}_{k^*})$$

ChÃºng ta cÃ³ thá»ƒ giáº£i phÆ°Æ¡ng trÃ¬nh nÃ y Ä‘á»ƒ tÃ¬m xÃ¡c suáº¥t dá»± Ä‘oÃ¡n $\hat{y}_{k^*}$:

$$\ln(\hat{y}_{k^*}) = -4.61$$
$$\hat{y}_{k^*} = e^{-4.61} \approx 0.010$$

#### Káº¿t luáº­n

GiÃ¡ trá»‹ $4.61$ cho tháº¥y **mÃ´ hÃ¬nh chá»‰ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t khoáº£ng $0.01$ (tá»©c $1\%$) cho tá»« láº½ ra pháº£i lÃ  Ä‘Ã¡p Ã¡n Ä‘Ãºng** trong vÃ­ dá»¥ nÃ y. Chi phÃ­ cao (nhÆ° $4.61$) lÃ  cÃ¡ch **loss function** trá»«ng pháº¡t mÃ´ hÃ¬nh vÃ¬ Ä‘Ã£ Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n sai lá»‡ch cao (xÃ¡c suáº¥t tháº¥p) cho káº¿t quáº£ Ä‘Ãºng.

---
### **Training a CBOW Model: Forward Propagation**
---

Ná»™i dung táº­p trung vÃ o quÃ¡ trÃ¬nh **forward propagation** (lan truyá»n tiáº¿n) trong `Continuous Bag-of-Words` (**CBOW**) `model` Ä‘Æ°á»£c sá»­ dá»¥ng trong `neural networks`.

#### Tá»•ng quan Forward Propagation

* **Forward propagation** bao gá»“m viá»‡c truyá»n cÃ¡c giÃ¡ trá»‹ `input` qua `neural network` tá»« `input` Ä‘áº¿n `output`, tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ á»Ÿ má»—i lá»›p.
* Má»™t **batch of examples** (lÃ´ vÃ­ dá»¥) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t `matrix`, vÃ  **output matrix** Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch lan truyá»n `input` nÃ y qua `network`.

#### TÃ­nh toÃ¡n Cost

* **Cost function** (hÃ m chi phÃ­) lÃ  má»™t pháº§n má»Ÿ rá»™ng cá»§a **loss function** (hÃ m máº¥t mÃ¡t), Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng sai sá»‘ cho má»™t `batch of training examples`.
* **Cross-entropy cost** cho má»™t `batch` lÃ  **mean** (giÃ¡ trá»‹ trung bÃ¬nh) cá»§a cÃ¡c **cross-entropy losses** riÃªng láº» cho má»—i vÃ­ dá»¥, cho phÃ©p hÃ¬nh dung `cost` nhÆ° lÃ  má»™t giÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c `losses`.

#### QuÃ¡ trÃ¬nh Optimization

* Sau khi tÃ­nh toÃ¡n `cost`, **back propagation** (lan truyá»n ngÆ°á»£c) vÃ  **gradient descent** (giáº£m Ä‘á»™ dá»‘c) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c `parameters` (tham sá»‘) cá»§a `network` nháº±m cáº£i thiá»‡n `predictions`.
* CÃ¡c bÆ°á»›c tiáº¿p theo bao gá»“m `training word vectors` báº±ng cÃ¡ch sá»­ dá»¥ng **cost function** Ä‘á»ƒ nÃ¢ng cao `model's performance`.

> Forward Propagation (Lan truyá»n tiáº¿n)

> **Forward Propagation** Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

$$Z_1 = W_1 X + B_1$$

$$H = \text{ReLU}(Z_1)$$

$$Z_2 = W_2 H + B_2$$

$$\hat{Y} = \text{softmax}(Z_2)$$

> Trong Ä‘Ã³ $X$ lÃ  ma tráº­n `input` (Ä‘áº§u vÃ o theo lÃ´), $W_1, W_2$ lÃ  ma tráº­n `weights` (trá»ng sá»‘), $B_1, B_2$ lÃ  ma tráº­n `bias` (Ä‘á»™ lá»‡ch), $H$ lÃ  **hidden layer** (lá»›p áº©n), vÃ  $\hat{Y}$ lÃ  ma tráº­n dá»± Ä‘oÃ¡n `output` (`predicted output matrix`).

> Trong hÃ¬nh áº£nh dÆ°á»›i Ä‘Ã¢y, báº¡n báº¯t Ä‘áº§u tá»« bÃªn trÃ¡i vÃ  **forward propagate** (lan truyá»n tiáº¿n) suá»‘t tá»›i bÃªn pháº£i.

![20_Training_a_CBOW_Model_FP](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/20_Training_a_CBOW_Model_FP.png)

> Batch Cost Function (HÃ m chi phÃ­ theo lÃ´)

> Äá»ƒ tÃ­nh **loss** (tá»•n tháº¥t) cá»§a má»™t **batch** (lÃ´), báº¡n pháº£i tÃ­nh cÃ´ng thá»©c sau. CÃ´ng thá»©c nÃ y lÃ  giÃ¡ trá»‹ trung bÃ¬nh (`mean`) cá»§a cÃ¡c **Cross-Entropy losses** trÃªn $M$ vÃ­ dá»¥ trong `batch`:

$$J_{\text{batch}} = -\frac{1}{M}\sum_{i=1}^{M}\sum_{j=1}^{V}y_{j}^{(i)}\log\hat{y}_{j}^{(i)}$$

> Trong Ä‘Ã³:

* $M$: KÃ­ch thÆ°á»›c `batch`.
* $V$: KÃ­ch thÆ°á»›c `vocabulary`.
* $y_{j}^{(i)}$: GiÃ¡ trá»‹ thá»±c táº¿ (`actual`) cá»§a tá»« thá»© $j$ trong vÃ­ dá»¥ thá»© $i$.
* $\hat{y}_{j}^{(i)}$: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n (`predicted`) cá»§a tá»« thá»© $j$ trong vÃ­ dá»¥ thá»© $i$.

> Cho ma tráº­n **predicted center word** cá»§a báº¡n ($\hat{Y}$) vÃ  ma tráº­n **actual center word** ($Y_{\text{true}}$), báº¡n cÃ³ thá»ƒ tÃ­nh `loss`.

![21_Training_a_CBOW_Model_FP](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/21_Training_a_CBOW_Model_FP.png)

---
### **Training a CBOW Model: Backpropagation and Gradient Descent**
---

Ná»™i dung nÃ y táº­p trung vÃ o cÃ¡c ká»¹ thuáº­t Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a **cost** (chi phÃ­) trong `neural networks`, cá»¥ thá»ƒ thÃ´ng qua **backpropagation** vÃ  **gradient descent**.

#### Backpropagation (Lan truyá»n ngÆ°á»£c)

* **Backpropagation** lÃ  má»™t `algorithm` (thuáº­t toÃ¡n) tÃ­nh toÃ¡n cÃ¡c **partial derivatives** (Ä‘áº¡o hÃ m riÃªng) cá»§a `cost` Ä‘á»‘i vá»›i cÃ¡c `weights` (trá»ng sá»‘) vÃ  `biases` (Ä‘á»™ lá»‡ch) cá»§a `neural network`.
* NÃ³ sá»­ dá»¥ng **chain rule** (quy táº¯c chuá»—i) cho cÃ¡c Ä‘áº¡o hÃ m, báº¯t Ä‘áº§u tá»« `output layer` vÃ  tÃ­nh toÃ¡n ngÆ°á»£c trá»Ÿ láº¡i qua cÃ¡c lá»›p.

#### Gradient Descent (Giáº£m Ä‘á»™ dá»‘c)

* **Gradient Descent** lÃ  má»™t phÆ°Æ¡ng phÃ¡p Ä‘iá»u chá»‰nh cÃ¡c `weights` vÃ  `biases` báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c **gradients** (Ä‘á»™ dá»‘c) Ä‘Ã£ tÃ­nh toÃ¡n Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a `cost`.
* **Learning rate** ($\alpha$) lÃ  má»™t **hyperparameter** (siÃªu tham sá»‘) kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c cá»§a cÃ¡c `updates` (cáº­p nháº­t) Ä‘á»‘i vá»›i cÃ¡c `weights` vÃ  `biases`.

#### CÃ´ng thá»©c cáº­p nháº­t Trá»ng sá»‘ vÃ  Äá»™ lá»‡ch

CÃ¡c cÃ´ng thá»©c sau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u chá»‰nh `weights` ($W$) vÃ  `biases` ($b$) trong má»—i bÆ°á»›c láº·p:

* **Cáº­p nháº­t Trá»ng sá»‘:**

$$W := W - \alpha \frac{\partial J}{\partial W}$$

* **Cáº­p nháº­t Äá»™ lá»‡ch:**

$$b := b - \alpha \frac{\partial J}{\partial b}$$

`Learning rates` **nhá» hÆ¡n** cho phÃ©p `updates` dáº§n dáº§n vÃ  chÃ­nh xÃ¡c, trong khi `rates` **lá»›n hÆ¡n** cho phÃ©p `updates` nhanh hÆ¡n, nhÆ°ng cÃ³ nguy cÆ¡ bá» lá»¡ Ä‘iá»ƒm tá»‘i thiá»ƒu.

Báº£n tÃ³m táº¯t nÃ y gÃ³i gá»n cÃ¡c khÃ¡i niá»‡m chÃ­nh liÃªn quan Ä‘áº¿n `training` má»™t **continuous bag of words model** (`CBOW`) trong bá»‘i cáº£nh `neural networks`.

QuÃ¡ trÃ¬nh **Backpropagation** (lan truyá»n ngÆ°á»£c) vÃ  **Gradient Descent** (giáº£m Ä‘á»™ dá»‘c) Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:

> Backpropagation (Lan truyá»n ngÆ°á»£c)

> **Backpropagation** lÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n **partial derivatives** (Ä‘áº¡o hÃ m riÃªng) cá»§a hÃ m chi phÃ­ (`cost function`) $J_{\text{batch}}$ Ä‘á»‘i vá»›i táº¥t cáº£ cÃ¡c tham sá»‘ (`parameters`) cá»§a mÃ´ hÃ¬nh: $\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2$.

> Khi thá»±c hiá»‡n **back-prop** trong mÃ´ hÃ¬nh **CBOW** nÃ y, báº¡n cáº§n tÃ­nh toÃ¡n cÃ¡c Ä‘áº¡o hÃ m sau:

$$\frac{\partial J_{\text{batch}}}{\partial \mathbf{W}_1}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{W}_2}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{b}_1}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{b}_2}$$

> Gradient Descent (Giáº£m Ä‘á»™ dá»‘c)

> **Gradient Descent** sá»­ dá»¥ng cÃ¡c Ä‘áº¡o hÃ m Ä‘Ã£ tÃ­nh á»Ÿ trÃªn Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘, nháº±m tá»‘i thiá»ƒu hÃ³a chi phÃ­ $J_{\text{batch}}$. CÃ¡c cÃ´ng thá»©c cáº­p nháº­t Ä‘Æ°á»£c láº·p láº¡i (`iterate`) nhÆ° sau:

$$\mathbf{W}_{1} := \mathbf{W}_{1} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{W}_{1}}$$

$$\mathbf{W}_{2} := \mathbf{W}_{2} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{W}_{2}}$$

$$\mathbf{b}_{1} := \mathbf{b}_{1} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{b}_{1}}$$

$$\mathbf{b}_{2} := \mathbf{b}_{2} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{b}_{2}}$$

> **Learning rate** ($\alpha$) lÃ  má»™t **hyperparameter** (siÃªu tham sá»‘) quan trá»ng kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ há»c:

* **$\alpha$ nhá» hÆ¡n** cho phÃ©p cÃ¡c cáº­p nháº­t **gradual** (dáº§n dáº§n) Ä‘á»‘i vá»›i cÃ¡c `weights` vÃ  `biases`.
* **$\alpha$ lá»›n hÆ¡n** cho phÃ©p cáº­p nháº­t **faster** (nhanh hÆ¡n).

> **LÆ°u Ã½:** Náº¿u $\alpha$ quÃ¡ lá»›n, báº¡n cÃ³ thá»ƒ **vÆ°á»£t quÃ¡** Ä‘iá»ƒm tá»‘i thiá»ƒu vÃ  khÃ´ng há»c Ä‘Æ°á»£c gÃ¬; náº¿u nÃ³ quÃ¡ nhá», `model` cá»§a báº¡n sáº½ máº¥t ráº¥t nhiá»u thá»i gian Ä‘á»ƒ `training`.

---
### **Extracting Word Embedding Vectors**
---


---
### **Evaluating Word Embeddings: Intrinsic Evaluation**
---


---
### **Evaluating Word Embeddings: Extrinsic Evaluation**
---



