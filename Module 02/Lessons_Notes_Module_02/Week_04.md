# **Module 02 - Natural Language Processing with Probabilistic Models**
## **Week 4: Word Embeddings with Neural Network**
---
### **Overview**
---

Ná»™i dung tuáº§n nÃ y táº­p trung vÃ o **word vectors**, cÃ²n Ä‘Æ°á»£c gá»i lÃ  **word embeddings**, vÃ  cÃ¡ch `training` (huáº¥n luyá»‡n) chÃºng tá»« Ä‘áº§u.

#### Understanding Word Vectors

- **Word vectors** ráº¥t cáº§n thiáº¿t cho cÃ¡c á»©ng dá»¥ng khÃ¡c nhau trong `natural language processing` (`NLP`), cháº³ng háº¡n nhÆ° `sentiment analysis` (phÃ¢n tÃ­ch tÃ¬nh cáº£m) vÃ  `machine translation` (dá»‹ch mÃ¡y).
- ChÃºng cho phÃ©p biá»ƒu diá»…n sá»‘ há»c cá»§a cÃ¡c tá»«, táº¡o Ä‘iá»u kiá»‡n cho viá»‡c sá»­ dá»¥ng chÃºng trong cÃ¡c `mathematical models`.

#### Training Word Vectors

- KhÃ³a há»c sáº½ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p táº¡o **word embeddings**, bao gá»“m **continuous bag-of-words model** (`CBOW`).
- CÃ¡c ká»¹ thuáº­t khÃ¡c nhÆ° **GloVe** vÃ  **Word2Vec** cÅ©ng sáº½ Ä‘Æ°á»£c Ä‘á» cáº­p, nhÆ°ng trá»ng tÃ¢m sáº½ lÃ  **continuous bag-of-words model**.

#### Preparing Text for Machine Learning

- NgÆ°á»i há»c sáº½ biáº¿t cÃ¡ch biáº¿n Ä‘á»•i dá»¯ liá»‡u vÄƒn báº£n thÃ nh má»™t `training set` phÃ¹ há»£p cho cÃ¡c `machine learning models`.
- Lá»i khuyÃªn thá»±c táº¿ sáº½ Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ lÃ m viá»‡c vá»›i cÃ¡c `text corpora` Ä‘a dáº¡ng, cháº³ng háº¡n nhÆ° sÃ¡ch vÃ  `tweets`.

> **Word embeddings** (nhÃºng tá»«) Ä‘Æ°á»£c sá»­ dá»¥ng trong háº§u háº¿t cÃ¡c `NLP applications`. Báº¥t cá»© khi nÃ o báº¡n xá»­ lÃ½ vÄƒn báº£n, trÆ°á»›c tiÃªn báº¡n pháº£i tÃ¬m cÃ¡ch Ä‘á»ƒ `encode` (mÃ£ hÃ³a) cÃ¡c tá»« dÆ°á»›i dáº¡ng sá»‘. `Word embedding` lÃ  má»™t ká»¹ thuáº­t ráº¥t phá»• biáº¿n cho phÃ©p báº¡n lÃ m Ä‘iá»u Ä‘Ã³.

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i `applications` (á»©ng dá»¥ng) cá»§a `word embeddings` mÃ  báº¡n sáº½ cÃ³ thá»ƒ triá»ƒn khai khi hoÃ n thÃ nh chuyÃªn ngÃ nh nÃ y.

![01_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/01_Overview.png)

![02_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/02_Overview.png)

#### Má»¥c tiÃªu há»c táº­p trong tuáº§n

Äáº¿n cuá»‘i tuáº§n nÃ y, báº¡n sáº½ cÃ³ thá»ƒ:

- XÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m chÃ­nh cá»§a **word representations** (biá»ƒu diá»…n tá»«).
- Táº¡o ra **word embeddings**.
- Chuáº©n bá»‹ vÄƒn báº£n cho **machine learning**.
- Triá»ƒn khai **continuous bag-of-words model**.

---
### **Basic Word Representations**
---
Ná»™i dung táº­p trung vÃ o viá»‡c biá»ƒu diá»…n cÃ¡c tá»« trong má»™t `vocabulary` (tá»« vá»±ng) báº±ng cÃ¡c `numerical vectors` (vÃ©c-tÆ¡ sá»‘), cá»¥ thá»ƒ thÃ´ng qua khÃ¡i niá»‡m **one-hot vectors**.

#### Understanding One-Hot Vectors

- Má»—i tá»« trong má»™t `vocabulary` Ä‘Æ°á»£c gÃ¡n má»™t sá»‘ nguyÃªn (`integer`) duy nháº¥t, nhÆ°ng phÆ°Æ¡ng phÃ¡p nÃ y thiáº¿u **semantic meaning** (Ã½ nghÄ©a ngá»¯ nghÄ©a).
- **One-hot vectors** biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng cÃ¡c `binary vectors` (vÃ©c-tÆ¡ nhá»‹ phÃ¢n), trong Ä‘Ã³ '1' cho biáº¿t sá»± hiá»‡n diá»‡n cá»§a má»™t tá»« vÃ  '0' cho biáº¿t sá»± váº¯ng máº·t.

#### Advantages and Limitations of One-Hot Vectors

- **One-hot vectors** Ä‘Æ¡n giáº£n vÃ  khÃ´ng ngá»¥ Ã½ báº¥t ká»³ má»‘i quan há»‡ nÃ o giá»¯a cÃ¡c tá»«.
- Tuy nhiÃªn, chÃºng cÃ³ thá»ƒ trá»Ÿ nÃªn ráº¥t lá»›n vÃ  **khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a** hoáº·c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»«, dáº«n Ä‘áº¿n nhá»¯ng háº¡n cháº¿ trong cÃ¡c `natural language processing tasks`.

#### Transition to Word Embeddings

- Cuá»™c tháº£o luáº­n táº¡o tiá»n Ä‘á» cho viá»‡c giá»›i thiá»‡u **word embeddings**, cÃ¡i mÃ  nháº±m má»¥c Ä‘Ã­ch giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ cá»§a `one-hot vectors` báº±ng cÃ¡ch náº¯m báº¯t cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a (`semantic relationships`) giá»¯a cÃ¡c tá»«.

> CÃ¡c biá»ƒu diá»…n tá»« cÆ¡ báº£n cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh cÃ¡c dáº¡ng sau:

- **Integers** (Sá»‘ nguyÃªn)
- **One-hot vectors**
- **Word embeddings**

![03_Basic_Word_Representations](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/03_Basic_Word_Representations.png)

> á» bÃªn trÃ¡i, báº¡n cÃ³ má»™t vÃ­ dá»¥ trong Ä‘Ã³ báº¡n sá»­ dá»¥ng sá»‘ nguyÃªn (`integers`) Ä‘á»ƒ biá»ƒu diá»…n má»™t tá»«. Váº¥n Ä‘á» á»Ÿ Ä‘Ã³ lÃ  khÃ´ng cÃ³ lÃ½ do gÃ¬ khiáº¿n tá»« nÃ y tÆ°Æ¡ng á»©ng vá»›i má»™t sá»‘ lá»›n hÆ¡n tá»« khÃ¡c. Äá»ƒ kháº¯c phá»¥c váº¥n Ä‘á» nÃ y, chÃºng ta giá»›i thiá»‡u **one hot vectors** (sÆ¡ Ä‘á»“ bÃªn pháº£i). Äá»ƒ triá»ƒn khai `one hot vectors`, báº¡n pháº£i `initialize` (khá»Ÿi táº¡o) má»™t `vector` toÃ n sá»‘ khÃ´ng (`zeros`) cÃ³ **dimension $V$** vÃ  sau Ä‘Ã³ Ä‘áº·t sá»‘ **1** vÃ o `index` tÆ°Æ¡ng á»©ng vá»›i tá»« báº¡n Ä‘ang biá»ƒu diá»…n.

> **Æ¯u Ä‘iá»ƒm** (`Pros`) cá»§a `one-hot vectors`:
- ÄÆ¡n giáº£n.
- KhÃ´ng yÃªu cáº§u thá»© tá»± ngá»¥ Ã½ (`implied ordering`).

> **NhÆ°á»£c Ä‘iá»ƒm** (`Cons`) cá»§a `one-hot vectors`:
- Ráº¥t lá»›n (`huge`).
- KhÃ´ng `encode` (mÃ£ hÃ³a) Ä‘Æ°á»£c Ã½ nghÄ©a (`meaning`).

---
### **Word Embeddings**
---

Ná»™i dung nÃ y táº­p trung vÃ o khÃ¡i niá»‡m **word embeddings**, má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ `encode` (mÃ£ hÃ³a) Ã½ nghÄ©a cá»§a cÃ¡c tá»« trong má»™t **low-dimensional vector space** (khÃ´ng gian vÃ©c-tÆ¡ chiá»u tháº¥p).

#### Understanding Word Embeddings

- **Word embeddings** biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng cÃ¡c `vectors` theo cÃ¡ch náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a cá»§a chÃºng, cho phÃ©p so sÃ¡nh dá»±a trÃªn sá»± gáº§n gÅ©i trong `vector space`.
- CÃ¡c tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh vá»‹ dá»c theo hai trá»¥c: má»™t cho **sentiment** (tá»« tÃ­ch cá»±c Ä‘áº¿n tiÃªu cá»±c) vÃ  má»™t cho **concreteness** (tá»« cá»¥ thá»ƒ Ä‘áº¿n trá»«u tÆ°á»£ng).

#### Creating Word Vectors

- Má»™t `two-dimensional vector` (vÃ©c-tÆ¡ hai chiá»u) cÃ³ thá»ƒ biá»ƒu diá»…n cÃ¡c tá»«, trong Ä‘Ã³ cÃ¡c `coordinates` (tá»a Ä‘á»™) chá»‰ ra `sentiment` vÃ  má»©c Ä‘á»™ trá»«u tÆ°á»£ng cá»§a chÃºng.
- Biá»ƒu diá»…n nÃ y cho phÃ©p xÃ¡c Ä‘á»‹nh sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»«, cháº³ng háº¡n nhÆ° "happy" vÃ  "excited" gáº§n nhau hÆ¡n "paper."

#### Applications and Importance

- **Word embeddings** táº¡o Ä‘iá»u kiá»‡n cho cÃ¡c `natural language processing` (`NLP`) `tasks` khÃ¡c nhau, bao gá»“m **analogies** (sá»± tÆ°Æ¡ng tá»±) vÃ  **sentence meaning encoding** (mÃ£ hÃ³a Ã½ nghÄ©a cÃ¢u).
- BÃ i giáº£ng nháº¥n máº¡nh ráº±ng viá»‡c táº¡o `word embeddings` lÃ  má»™t má»¥c tiÃªu chÃ­nh cá»§a mÃ´-Ä‘un nÃ y, dáº«n Ä‘áº¿n cÃ¡c `NLP applications` phá»©c táº¡p hÆ¡n nhÆ° **question answering** vÃ  **translation**.

> Váº­y táº¡i sao láº¡i sá»­ dá»¥ng **word embeddings**? HÃ£y cÃ¹ng xem.

![04_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/04_Word_Embeddings.png)

> Tá»« sÆ¡ Ä‘á»“ trÃªn, báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng khi `encode` (mÃ£ hÃ³a) má»™t tá»« trong khÃ´ng gian **2D**, cÃ¡c tá»« tÆ°Æ¡ng tá»± cÃ³ xu hÆ°á»›ng náº±m gáº§n nhau. CÃ³ láº½ **coordinate** Ä‘áº§u tiÃªn Ä‘áº¡i diá»‡n cho viá»‡c má»™t tá»« lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c. **Coordinate** thá»© hai cho báº¡n biáº¿t tá»« Ä‘Ã³ lÃ  **abstract** (trá»«u tÆ°á»£ng) hay **concrete** (cá»¥ thá»ƒ). ÄÃ¢y chá»‰ lÃ  má»™t vÃ­ dá»¥, trong tháº¿ giá»›i thá»±c, báº¡n sáº½ tÃ¬m tháº¥y cÃ¡c `embeddings` vá»›i hÃ ng trÄƒm **dimensions** (chiá»u). Báº¡n cÃ³ thá»ƒ coi má»—i **coordinate** lÃ  má»™t con sá»‘ cho báº¡n biáº¿t Ä‘iá»u gÃ¬ Ä‘Ã³ vá» tá»« Ä‘Ã³.

> Æ¯u Ä‘iá»ƒm cá»§a Word Embeddings

- **Low dimensions** (Chiá»u tháº¥p) (Ã­t hÆ¡n $V$, kÃ­ch thÆ°á»›c `vocabulary`).
- Cho phÃ©p báº¡n `encode` (mÃ£ hÃ³a) Ã½ nghÄ©a (`meaning`).

---
### **How to Create Word Embeddings**
---

Ná»™i dung nÃ y táº­p trung vÃ o quÃ¡ trÃ¬nh táº¡o **word embeddings** trong `natural language processing` (`NLP`).

#### CÃ¡c ThÃ nh pháº§n Thiáº¿t yáº¿u

Äá»ƒ táº¡o **word embeddings** cáº§n hai thÃ nh pháº§n chÃ­nh:

- **Corpus** (Kho ngá»¯ liá»‡u) vÄƒn báº£n.
- **Embedding method** (PhÆ°Æ¡ng phÃ¡p nhÃºng).

`Corpus` pháº£i liÃªn quan Ä‘áº¿n ngá»¯ cáº£nh. VÃ­ dá»¥, Ä‘á»ƒ táº¡o `Shakespearean embeddings`, báº¡n cáº§n sá»­ dá»¥ng vÄƒn báº£n gá»‘c cá»§a Shakespeare chá»© khÃ´ng pháº£i chá»‰ lÃ  cÃ¡c ghi chÃº tÃ³m táº¯t.

#### Táº§m quan trá»ng cá»§a Context

- **Context** (Ngá»¯ cáº£nh) Ä‘á» cáº­p Ä‘áº¿n cÃ¡c tá»« xung quanh cung cáº¥p Ã½ nghÄ©a cho má»—i **word embedding**.
- Má»™t `vocabulary list` Ä‘Æ¡n giáº£n lÃ  khÃ´ng Ä‘á»§; cáº§n cÃ³ má»™t `corpus` toÃ n diá»‡n Ä‘á»ƒ náº¯m báº¯t cÃ¡c sáº¯c thÃ¡i ngá»¯ nghÄ©a.

#### PhÆ°Æ¡ng phÃ¡p vÃ  GiÃ¡m sÃ¡t

- **Embedding method**, thÆ°á»ng dá»±a trÃªn cÃ¡c `machine learning models`, táº¡o ra **word embeddings** tá»« `corpus`.
- `Learning task` cÃ³ thá»ƒ lÃ  **self-supervised** (tá»± giÃ¡m sÃ¡t), táº­n dá»¥ng dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n trong khi `model` tá»± cung cáº¥p ngá»¯ cáº£nh cá»§a riÃªng nÃ³ Ä‘á»ƒ giÃ¡m sÃ¡t.

#### Hyperparameters vÃ  Biá»ƒu diá»…n ToÃ¡n há»c

- **Word embeddings** cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh báº±ng **hyperparameters** (siÃªu tham sá»‘), cháº³ng háº¡n nhÆ° **dimension** (chiá»u) cá»§a cÃ¡c `embedding vectors`, thÆ°á»ng dao Ä‘á»™ng tá»« hÃ ng trÄƒm Ä‘áº¿n hÃ ng nghÃ¬n.
- `Corpus` pháº£i Ä‘Æ°á»£c biáº¿n Ä‘á»•i thÃ nh má»™t **biá»ƒu diá»…n toÃ¡n há»c** phÃ¹ há»£p cho `model`, thÆ°á»ng sá»­ dá»¥ng **integer-based indices** hoáº·c **one-hot vectors**.

Ná»™i dung sáº¯p tá»›i sáº½ giá»›i thiá»‡u cÃ¡c `word embedding methods` khÃ¡c nhau, bao gá»“m cÃ¡ch tiáº¿p cáº­n **continuous bag-of-words** (`CBOW`), cÃ¡i mÃ  sáº½ Ä‘Æ°á»£c triá»ƒn khai trong bÃ i táº­p tiáº¿p theo.

> Äá»ƒ táº¡o **word embeddings**, báº¡n luÃ´n cáº§n má»™t **corpus** (kho ngá»¯ liá»‡u) vÄƒn báº£n vÃ  má»™t **embedding method** (phÆ°Æ¡ng phÃ¡p nhÃºng). **Context** (Ngá»¯ cáº£nh) cá»§a má»™t tá»« cho báº¡n biáº¿t loáº¡i tá»« nÃ o cÃ³ xu hÆ°á»›ng xáº£y ra gáº§n tá»« cá»¥ thá»ƒ Ä‘Ã³. **Context** lÃ  quan trá»ng vÃ¬ Ä‘Ã¢y lÃ  yáº¿u tá»‘ sáº½ mang láº¡i Ã½ nghÄ©a cho má»—i `word embedding`.


#### PhÆ°Æ¡ng phÃ¡p Embeddings vÃ  Tá»± giÃ¡m sÃ¡t

> CÃ³ nhiá»u loáº¡i phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ cho phÃ©p báº¡n há»c cÃ¡c **word embeddings**. `Machine learning model` thá»±c hiá»‡n má»™t `learning task` (nhiá»‡m vá»¥ há»c táº­p), vÃ  sáº£n pháº©m phá»¥ chÃ­nh cá»§a `task` nÃ y lÃ  cÃ¡c `word embeddings`. `Task` cÃ³ thá»ƒ lÃ  há»c cÃ¡ch dá»± Ä‘oÃ¡n má»™t tá»« dá»±a trÃªn cÃ¡c tá»« xung quanh trong má»™t cÃ¢u cá»§a `corpus`, nhÆ° trong trÆ°á»ng há»£p cá»§a **continuous bag-of-words** (`CBOW`).

> `Task` lÃ  **self-supervised** (tá»± giÃ¡m sÃ¡t): nÃ³ vá»«a lÃ  **unsupervised** (khÃ´ng giÃ¡m sÃ¡t) á»Ÿ chá»— dá»¯ liá»‡u Ä‘áº§u vÃ o â€” `corpus` â€” lÃ  **unlabelled** (khÃ´ng cÃ³ nhÃ£n), vÃ  vá»«a lÃ  **supervised** (cÃ³ giÃ¡m sÃ¡t) á»Ÿ chá»— báº£n thÃ¢n dá»¯ liá»‡u cung cáº¥p `context` cáº§n thiáº¿t mÃ  thÃ´ng thÆ°á»ng sáº½ táº¡o thÃ nh cÃ¡c `labels` (nhÃ£n).

> Khi `training word vectors`, cÃ³ má»™t sá»‘ **hyperparameters** (siÃªu tham sá»‘) báº¡n cáº§n Ä‘iá»u chá»‰nh (vÃ­ dá»¥: **dimension** (chiá»u) cá»§a `word vector`).

![05_How_to_Create_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/05_How_to_Create_Word_Embeddings.png)

---
### **Word Embedding Methods**
---

Ná»™i dung nÃ y táº­p trung vÃ o cÃ¡c **word embedding methods** khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng trong `natural language processing`.

#### MÃ´ hÃ¬nh Word2Vec

* **Word2Vec** sá»­ dá»¥ng má»™t **shallow neural network** (máº¡ng nÆ¡-ron nÃ´ng) vá»›i hai `architectures` (kiáº¿n trÃºc): **continuous bag-of-words** (**CBOW**) vÃ  **continuous skip-gram**.
    * **CBOW** dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u dá»±a trÃªn cÃ¡c tá»« xung quanh.
    * **Skip-gram** dá»± Ä‘oÃ¡n cÃ¡c tá»« xung quanh tá»« má»™t tá»« `input` cho trÆ°á»›c.

#### CÃ¡c Ká»¹ thuáº­t Embeddings NÃ¢ng cao

* **GloVe** (`Global Vectors`) phÃ¢n tÃ­ch ma tráº­n **word co-occurrence matrix** (Ä‘á»“ng xuáº¥t hiá»‡n tá»«) Ä‘á»ƒ náº¯m báº¯t Ã½ nghÄ©a cá»§a tá»«.
* **FastText** cáº£i tiáº¿n `skip-gram` báº±ng cÃ¡ch biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng **character n-grams**, cho phÃ©p nÃ³ xá»­ lÃ½ hiá»‡u quáº£ cÃ¡c **unseen words** (tá»« chÆ°a tá»«ng tháº¥y).

### Contextual Word Embeddings

* CÃ¡c `models` tiÃªn tiáº¿n nhÆ° **BERT**, **ELMo**, vÃ  **GPT-2** táº¡o ra cÃ¡c `embeddings` khÃ¡c nhau cho cÃ¡c tá»« dá»±a trÃªn **context** (ngá»¯ cáº£nh) cá»§a chÃºng, há»— trá»£ **polysemy** (Ä‘a nghÄ©a).
* CÃ¡c `models` nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y dÆ°á»›i dáº¡ng **pretrained versions** (phiÃªn báº£n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c) trá»±c tuyáº¿n vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c **fine-tuned** (tinh chá»‰nh) vá»›i cÃ¡c `corpora` cá»¥ thá»ƒ Ä‘á»ƒ cÃ³ `performance` tá»‘t hÆ¡n.

### ğŸ“š PhÆ°Æ¡ng phÃ¡p Word Embedding

#### PhÆ°Æ¡ng phÃ¡p Cá»• Ä‘iá»ƒn (`Classical Methods`)

* **word2vec** (Google, 2013):
    * **Continuous bag-of-words (CBOW)**: `model` há»c cÃ¡ch **dá»± Ä‘oÃ¡n** tá»« trung tÃ¢m (`center word`) cho trÆ°á»›c cÃ¡c `context words` (tá»« ngá»¯ cáº£nh).
    * **Continuous skip-gram / Skip-gram with negative sampling (SGNS)**: `model` há»c cÃ¡ch **dá»± Ä‘oÃ¡n** cÃ¡c tá»« xung quanh (`surrounding words`) cho trÆ°á»›c má»™t tá»« `input`.

* **Global Vectors (GloVe)** (Stanford, 2014): PhÃ¢n tÃ­ch `logarithm` cá»§a **word co-occurrence matrix** (ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»«) cá»§a `corpus`, tÆ°Æ¡ng tá»± nhÆ° `count matrix` báº¡n Ä‘Ã£ sá»­ dá»¥ng trÆ°á»›c Ä‘Ã¢y.
* **fastText** (Facebook, 2016): Dá»±a trÃªn `skip-gram model` vÃ  tÃ­nh Ä‘áº¿n cáº¥u trÃºc cá»§a tá»« báº±ng cÃ¡ch biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng `n-gram` cá»§a kÃ½ tá»±. NÃ³ há»— trá»£ cÃ¡c tá»« **out-of-vocabulary (OOV)**.

#### Deep Learning, Contextual Embeddings

Trong cÃ¡c `models` tiÃªn tiáº¿n hÆ¡n nÃ y, cÃ¡c tá»« cÃ³ cÃ¡c `embeddings` khÃ¡c nhau tÃ¹y thuá»™c vÃ o **context** (ngá»¯ cáº£nh) cá»§a chÃºng. Báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng cÃ¡c `pre-trained embeddings` (embeddings Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c) cho cÃ¡c `models` sau:

* **BERT** (Google, 2018)
* **ELMo** (Allen Institute for AI, 2018)
* **GPT-2** (OpenAI, 2018)

---
### **Continuous Bag-of-Words Model**
---

Ná»™i dung táº­p trung vÃ o viá»‡c triá»ƒn khai **continuous bag-of-words model** (**CBOW**) Ä‘á»ƒ táº¡o `word embeddings` trong `natural language processing`.

#### QuÃ¡ trÃ¬nh tá»•ng thá»ƒ cá»§a Word Embeddings

* **Word embeddings** Ä‘Æ°á»£c táº¡o ra thÃ´ng qua má»™t `machine learning model` há»c tá»« má»™t `corpus` (kho ngá»¯ liá»‡u).
* **Continuous bag-of-words model** dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u (**center word**) dá»±a trÃªn cÃ¡c **context words** (tá»« ngá»¯ cáº£nh) xung quanh nÃ³.

#### Táº¡o Dá»¯ liá»‡u Huáº¥n luyá»‡n

* **Context words** Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  cÃ¡c tá»« bao quanh má»™t **center word**, vá»›i má»™t **hyperparameter $C$** xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng `context words` (bÃ¡n kÃ­nh cá»­a sá»• ngá»¯ cáº£nh).
* `Model` sá»­ dá»¥ng **sliding windows** (cá»­a sá»• trÆ°á»£t) Ä‘á»ƒ táº¡o cÃ¡c `training examples` (vÃ­ dá»¥ huáº¥n luyá»‡n), trong Ä‘Ã³ `context words` lÃ  `inputs` vÃ  `center word` lÃ  `target` (má»¥c tiÃªu) Ä‘á»ƒ dá»± Ä‘oÃ¡n.

#### Kiáº¿n trÃºc MÃ´ hÃ¬nh vÃ  Há»c táº­p

* **Model architecture** bao gá»“m `context words` lÃ  `inputs` vÃ  `center words` lÃ  `outputs`.
* Khi `model` há»c, nÃ³ táº¡o ra `word embeddings` nhÆ° má»™t sáº£n pháº©m phá»¥ cá»§a `prediction task` (nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n), náº¯m báº¯t Ä‘Æ°á»£c **semantic relationships** (má»‘i quan há»‡ ngá»¯ nghÄ©a) giá»¯a cÃ¡c tá»«.

> Äá»ƒ táº¡o **word embeddings**, báº¡n cáº§n má»™t `corpus` vÃ  má»™t `learning algorithm` (thuáº­t toÃ¡n há»c táº­p). Sáº£n pháº©m phá»¥ cá»§a `task` nÃ y sáº½ lÃ  má»™t táº­p há»£p cÃ¡c `word embeddings`. Trong trÆ°á»ng há»£p cá»§a **continuous bag-of-words model** (**CBOW**), `objective` (má»¥c tiÃªu) cá»§a `task` lÃ  **dá»± Ä‘oÃ¡n má»™t tá»« bá»‹ thiáº¿u** dá»±a trÃªn cÃ¡c tá»« xung quanh nÃ³.

![06_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/06_Continuous_Bag-of-Words_Model.png)

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t **visualization** (hÃ¬nh áº£nh trá»±c quan) cho báº¡n tháº¥y `model` hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.

![07_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/07_Continuous_Bag-of-Words_Model.png)

> NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, **window size** (kÃ­ch thÆ°á»›c cá»­a sá»•) trong hÃ¬nh áº£nh phÃ­a trÃªn lÃ  5. **Context size** (kÃ­ch thÆ°á»›c ngá»¯ cáº£nh), $C$, lÃ  2. $C$ thÆ°á»ng cho báº¡n biáº¿t cÃ³ bao nhiÃªu tá»« trÆ°á»›c hoáº·c sau **center word** (tá»« trung tÃ¢m) mÃ  `model` sáº½ sá»­ dá»¥ng Ä‘á»ƒ Ä‘Æ°a ra **prediction** (dá»± Ä‘oÃ¡n).

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t **visualization** khÃ¡c cho tháº¥y tá»•ng quan vá» `model`.

![08_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/08_Continuous_Bag-of-Words_Model.png)

---
### **Cleaning and Tokenization**
---

Ná»™i dung táº­p trung vÃ o cÃ¡c quy trÃ¬nh **cleaning** (lÃ m sáº¡ch) vÃ  **tokenization** (táº¡o token) trong `natural language processing` (`NLP`).

#### Cleaning vÃ  Tokenization

- CÃ¡c tá»« nÃªn Ä‘Æ°á»£c xá»­ lÃ½ dÆ°á»›i dáº¡ng **case insensitive** (khÃ´ng phÃ¢n biá»‡t chá»¯ hoa/thÆ°á»ng), nghÄ©a lÃ  chÃºng nÃªn Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh má»™t Ä‘á»‹nh dáº¡ng duy nháº¥t (chá»¯ thÆ°á»ng hoáº·c chá»¯ hoa) Ä‘á»ƒ Ä‘á»“ng nháº¥t.
- **Punctuation** (Dáº¥u cÃ¢u) cáº§n Ä‘Æ°á»£c xá»­ lÃ½ cáº©n tháº­n; dáº¥u cÃ¢u gÃ¢y ngáº¯t quÃ£ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng má»™t `special word` (tá»« Ä‘áº·c biá»‡t) duy nháº¥t, trong khi dáº¥u cÃ¢u khÃ´ng gÃ¢y ngáº¯t quÃ£ng cÃ³ thá»ƒ bá»‹ bá» qua.

#### Xá»­ lÃ½ Sá»‘ vÃ  KÃ½ tá»± Äáº·c biá»‡t

- **Numbers** (Sá»‘) cÃ³ thá»ƒ bá»‹ bá» Ä‘i náº¿u chÃºng khÃ´ng quan trá»ng, nhÆ°ng cÃ¡c sá»‘ quan trá»ng nÃªn Ä‘Æ°á»£c giá»¯ láº¡i hoáº·c thay tháº¿ báº±ng má»™t `special token` nhÆ° **\<NUMBER\>**.
- CÃ¡c **Special characters** (KÃ½ tá»± Ä‘áº·c biá»‡t), cháº³ng háº¡n nhÆ° kÃ½ hiá»‡u toÃ¡n há»c vÃ  `emojis`, nÃªn Ä‘Æ°á»£c quáº£n lÃ½ dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan cá»§a chÃºng vá»›i `model`.

### VÃ­ dá»¥ Thá»±c hÃ nh

Má»™t `Python example` minh há»a cÃ¡ch `clean` má»™t `corpus` báº±ng cÃ¡ch gá»™p `punctuation` vÃ  **tokenizing** vÄƒn báº£n báº±ng cÃ¡ch sá»­ dá»¥ng `NLTK library`, táº¡o ra má»™t `array of tokens` sáºµn sÃ ng Ä‘á»ƒ phÃ¢n tÃ­ch thÃªm.

Äiá»u nÃ y táº¡o tiá»n Ä‘á» cho chá»§ Ä‘á» tiáº¿p theo vá» **continuous bag-of-words model**.

> TrÆ°á»›c khi triá»ƒn khai báº¥t ká»³ thuáº­t toÃ¡n `natural language processing` (`NLP`) nÃ o, báº¡n cÃ³ thá»ƒ muá»‘n `clean` (lÃ m sáº¡ch) dá»¯ liá»‡u vÃ  `tokenize` (táº¡o token) nÃ³. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i Ä‘iá»u cáº§n lÆ°u Ã½ khi xá»­ lÃ½ `data` cá»§a báº¡n.

![09_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/09_Cleaning_and_Tokenization.png)

> Báº¡n cÃ³ thá»ƒ `clean data` báº±ng `Python` nhÆ° sau:

![10_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/10_Cleaning_and_Tokenization.png)

> Báº¡n cÃ³ thá»ƒ thÃªm bao nhiÃªu Ä‘iá»u kiá»‡n tÃ¹y thÃ­ch vÃ o cÃ¡c dÃ²ng tÆ°Æ¡ng á»©ng vá»›i hÃ¬nh chá»¯ nháº­t mÃ u xanh lÃ¡ cÃ¢y phÃ­a trÃªn.

---
### **Sliding Window of Words in Python**
---

Ná»™i dung táº­p trung vÃ o viá»‡c trÃ­ch xuáº¥t **center words** (tá»« trung tÃ¢m) vÃ  **context words** (tá»« ngá»¯ cáº£nh) Ä‘á»ƒ `training` (huáº¥n luyá»‡n) **continuous bag-of-words model** trong `natural language processing`.

#### TrÃ­ch xuáº¥t Center vÃ  Context Words

* QuÃ¡ trÃ¬nh báº¯t Ä‘áº§u vá»›i má»™t `cleaned and tokenized corpus` (kho ngá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch vÃ  táº¡o token), Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t `array` (máº£ng) cÃ¡c tá»«.
* Má»™t `function` (hÃ m) gá»i lÃ  `get_windows` Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a Ä‘á»ƒ trÃ­ch xuáº¥t `center words` vÃ  `context words` cá»§a chÃºng dá»±a trÃªn má»™t `context size` Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh.

#### Triá»ƒn khai Function

* `Function` nháº­n má»™t `array of words` vÃ  má»™t **context size ($C$)**, cÃ¡i mÃ  xÃ¡c Ä‘á»‹nh cÃ³ bao nhiÃªu tá»« sáº½ Ä‘Æ°á»£c xem xÃ©t á»Ÿ má»—i bÃªn cá»§a `center word`.
* NÃ³ `initialize` (khá»Ÿi táº¡o) má»™t vÃ²ng láº·p Ä‘á»ƒ láº·p qua `array`, trÃ­ch xuáº¥t `center words` vÃ  cÃ¡c `context words` tÆ°Æ¡ng á»©ng cá»§a chÃºng.

#### Sá»­ dá»¥ng Function

* `Function` sá»­ dá»¥ng **yield keyword** Ä‘á»ƒ tráº£ vá» cÃ¡c giÃ¡ trá»‹ má»™t cÃ¡ch **iteratively** (láº·p láº¡i), cho phÃ©p **data generation** (táº¡o dá»¯ liá»‡u) hiá»‡u quáº£.
* Má»™t vÃ²ng láº·p Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hiá»ƒn thá»‹ `context` vÃ  `center words`, cÃ¡i mÃ  cáº§n thiáº¿t cho **continuous bag-of-words model**.

Tá»•ng quan, bÃ i giáº£ng nÃ y cung cáº¥p má»™t cÃ¡ch tiáº¿p cáº­n thá»±c táº¿ Ä‘á»ƒ chuáº©n bá»‹ `data` cho `training word embeddings` báº±ng `Python`.

![11_Sliding_Window_of_Words_in_Python](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/11_Sliding_Window_of_Words_in_Python.png)

> `Code` phÃ­a trÃªn cho tháº¥y má»™t `function` (hÃ m) nháº­n hai `parameters` (tham sá»‘).

* `Words`: má»™t `list` (danh sÃ¡ch) cÃ¡c tá»«.
* $C$: **context size** (kÃ­ch thÆ°á»›c ngá»¯ cáº£nh).

> ChÃºng ta báº¯t Ä‘áº§u báº±ng cÃ¡ch Ä‘áº·t $i$ báº±ng $C$. Sau Ä‘Ã³, chÃºng ta tÃ¡ch **center\_word** (tá»« trung tÃ¢m) vÃ  **context\_words** (cÃ¡c tá»« ngá»¯ cáº£nh). ChÃºng ta sau Ä‘Ã³ **yield** (tráº£ vá») cÃ¡c giÃ¡ trá»‹ nÃ y vÃ  **increment** (tÄƒng) $i$ lÃªn.



---
### **Transforming Words into Vectors**
---



---
### **Architecture of the CBOW Model**
---


---
### **Architecture of the CBOW Model: Dimensions**
---

---
### **Architecture of the CBOW Model: Dimensions 2**
---

---
### **Architecture of the CBOW Model: Activation Functions**
---


---
### **Training a CBOW Model: Cost Function**
---

---
### **Training a CBOW Model: Forward Propagation**
---


---
### **Training a CBOW Model: Backpropagation and Gradient Descent**
---


---
### **Extracting Word Embedding Vectors**
---


---
### **Evaluating Word Embeddings: Intrinsic Evaluation**
---


---
### **Evaluating Word Embeddings: Extrinsic Evaluation**
---



