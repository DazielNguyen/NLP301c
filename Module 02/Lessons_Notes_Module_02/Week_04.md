# **Module 02 - Natural Language Processing with Probabilistic Models**
## **Week 4: Word Embeddings with Neural Network**
---
### **Overview**
---

N·ªôi dung tu·∫ßn n√†y t·∫≠p trung v√†o **word vectors**, c√≤n ƒë∆∞·ª£c g·ªçi l√† **word embeddings**, v√† c√°ch `training` (hu·∫•n luy·ªán) ch√∫ng t·ª´ ƒë·∫ßu.

#### Understanding Word Vectors

- **Word vectors** r·∫•t c·∫ßn thi·∫øt cho c√°c ·ª©ng d·ª•ng kh√°c nhau trong `natural language processing` (`NLP`), ch·∫≥ng h·∫°n nh∆∞ `sentiment analysis` (ph√¢n t√≠ch t√¨nh c·∫£m) v√† `machine translation` (d·ªãch m√°y).
- Ch√∫ng cho ph√©p bi·ªÉu di·ªÖn s·ªë h·ªçc c·ªßa c√°c t·ª´, t·∫°o ƒëi·ªÅu ki·ªán cho vi·ªác s·ª≠ d·ª•ng ch√∫ng trong c√°c `mathematical models`.

#### Training Word Vectors

- Kh√≥a h·ªçc s·∫Ω ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c ph∆∞∆°ng ph√°p t·∫°o **word embeddings**, bao g·ªìm **continuous bag-of-words model** (`CBOW`).
- C√°c k·ªπ thu·∫≠t kh√°c nh∆∞ **GloVe** v√† **Word2Vec** c≈©ng s·∫Ω ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p, nh∆∞ng tr·ªçng t√¢m s·∫Ω l√† **continuous bag-of-words model**.

#### Preparing Text for Machine Learning

- Ng∆∞·ªùi h·ªçc s·∫Ω bi·∫øt c√°ch bi·∫øn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n th√†nh m·ªôt `training set` ph√π h·ª£p cho c√°c `machine learning models`.
- L·ªùi khuy√™n th·ª±c t·∫ø s·∫Ω ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ l√†m vi·ªác v·ªõi c√°c `text corpora` ƒëa d·∫°ng, ch·∫≥ng h·∫°n nh∆∞ s√°ch v√† `tweets`.

> **Word embeddings** (nh√∫ng t·ª´) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·∫ßu h·∫øt c√°c `NLP applications`. B·∫•t c·ª© khi n√†o b·∫°n x·ª≠ l√Ω vƒÉn b·∫£n, tr∆∞·ªõc ti√™n b·∫°n ph·∫£i t√¨m c√°ch ƒë·ªÉ `encode` (m√£ h√≥a) c√°c t·ª´ d∆∞·ªõi d·∫°ng s·ªë. `Word embedding` l√† m·ªôt k·ªπ thu·∫≠t r·∫•t ph·ªï bi·∫øn cho ph√©p b·∫°n l√†m ƒëi·ªÅu ƒë√≥.

> D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i `applications` (·ª©ng d·ª•ng) c·ªßa `word embeddings` m√† b·∫°n s·∫Ω c√≥ th·ªÉ tri·ªÉn khai khi ho√†n th√†nh chuy√™n ng√†nh n√†y.

![01_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/01_Overview.png)

![02_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/02_Overview.png)

#### M·ª•c ti√™u h·ªçc t·∫≠p trong tu·∫ßn

ƒê·∫øn cu·ªëi tu·∫ßn n√†y, b·∫°n s·∫Ω c√≥ th·ªÉ:

- X√°c ƒë·ªãnh c√°c kh√°i ni·ªám ch√≠nh c·ªßa **word representations** (bi·ªÉu di·ªÖn t·ª´).
- T·∫°o ra **word embeddings**.
- Chu·∫©n b·ªã vƒÉn b·∫£n cho **machine learning**.
- Tri·ªÉn khai **continuous bag-of-words model**.

---
### **Basic Word Representations**
---
N·ªôi dung t·∫≠p trung v√†o vi·ªác bi·ªÉu di·ªÖn c√°c t·ª´ trong m·ªôt `vocabulary` (t·ª´ v·ª±ng) b·∫±ng c√°c `numerical vectors` (v√©c-t∆° s·ªë), c·ª• th·ªÉ th√¥ng qua kh√°i ni·ªám **one-hot vectors**.

#### Understanding One-Hot Vectors

- M·ªói t·ª´ trong m·ªôt `vocabulary` ƒë∆∞·ª£c g√°n m·ªôt s·ªë nguy√™n (`integer`) duy nh·∫•t, nh∆∞ng ph∆∞∆°ng ph√°p n√†y thi·∫øu **semantic meaning** (√Ω nghƒ©a ng·ªØ nghƒ©a).
- **One-hot vectors** bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng c√°c `binary vectors` (v√©c-t∆° nh·ªã ph√¢n), trong ƒë√≥ '1' cho bi·∫øt s·ª± hi·ªán di·ªán c·ªßa m·ªôt t·ª´ v√† '0' cho bi·∫øt s·ª± v·∫Øng m·∫∑t.

#### Advantages and Limitations of One-Hot Vectors

- **One-hot vectors** ƒë∆°n gi·∫£n v√† kh√¥ng ng·ª• √Ω b·∫•t k·ª≥ m·ªëi quan h·ªá n√†o gi·ªØa c√°c t·ª´.
- Tuy nhi√™n, ch√∫ng c√≥ th·ªÉ tr·ªü n√™n r·∫•t l·ªõn v√† **kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a** ho·∫∑c s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c t·ª´, d·∫´n ƒë·∫øn nh·ªØng h·∫°n ch·∫ø trong c√°c `natural language processing tasks`.

#### Transition to Word Embeddings

- Cu·ªôc th·∫£o lu·∫≠n t·∫°o ti·ªÅn ƒë·ªÅ cho vi·ªác gi·ªõi thi·ªáu **word embeddings**, c√°i m√† nh·∫±m m·ª•c ƒë√≠ch gi·∫£i quy·∫øt nh·ªØng h·∫°n ch·∫ø c·ªßa `one-hot vectors` b·∫±ng c√°ch n·∫Øm b·∫Øt c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a (`semantic relationships`) gi·ªØa c√°c t·ª´.

> C√°c bi·ªÉu di·ªÖn t·ª´ c∆° b·∫£n c√≥ th·ªÉ ƒë∆∞·ª£c ph√¢n lo·∫°i th√†nh c√°c d·∫°ng sau:

- **Integers** (S·ªë nguy√™n)
- **One-hot vectors**
- **Word embeddings**

![03_Basic_Word_Representations](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/03_Basic_Word_Representations.png)

> ·ªû b√™n tr√°i, b·∫°n c√≥ m·ªôt v√≠ d·ª• trong ƒë√≥ b·∫°n s·ª≠ d·ª•ng s·ªë nguy√™n (`integers`) ƒë·ªÉ bi·ªÉu di·ªÖn m·ªôt t·ª´. V·∫•n ƒë·ªÅ ·ªü ƒë√≥ l√† kh√¥ng c√≥ l√Ω do g√¨ khi·∫øn t·ª´ n√†y t∆∞∆°ng ·ª©ng v·ªõi m·ªôt s·ªë l·ªõn h∆°n t·ª´ kh√°c. ƒê·ªÉ kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ n√†y, ch√∫ng ta gi·ªõi thi·ªáu **one hot vectors** (s∆° ƒë·ªì b√™n ph·∫£i). ƒê·ªÉ tri·ªÉn khai `one hot vectors`, b·∫°n ph·∫£i `initialize` (kh·ªüi t·∫°o) m·ªôt `vector` to√†n s·ªë kh√¥ng (`zeros`) c√≥ **dimension $V$** v√† sau ƒë√≥ ƒë·∫∑t s·ªë **1** v√†o `index` t∆∞∆°ng ·ª©ng v·ªõi t·ª´ b·∫°n ƒëang bi·ªÉu di·ªÖn.

> **∆Øu ƒëi·ªÉm** (`Pros`) c·ªßa `one-hot vectors`:
- ƒê∆°n gi·∫£n.
- Kh√¥ng y√™u c·∫ßu th·ª© t·ª± ng·ª• √Ω (`implied ordering`).

> **Nh∆∞·ª£c ƒëi·ªÉm** (`Cons`) c·ªßa `one-hot vectors`:
- R·∫•t l·ªõn (`huge`).
- Kh√¥ng `encode` (m√£ h√≥a) ƒë∆∞·ª£c √Ω nghƒ©a (`meaning`).

---
### **Word Embeddings**
---

N·ªôi dung n√†y t·∫≠p trung v√†o kh√°i ni·ªám **word embeddings**, m·ªôt ph∆∞∆°ng ph√°p ƒë·ªÉ `encode` (m√£ h√≥a) √Ω nghƒ©a c·ªßa c√°c t·ª´ trong m·ªôt **low-dimensional vector space** (kh√¥ng gian v√©c-t∆° chi·ªÅu th·∫•p).

#### Understanding Word Embeddings

- **Word embeddings** bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng c√°c `vectors` theo c√°ch n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a c·ªßa ch√∫ng, cho ph√©p so s√°nh d·ª±a tr√™n s·ª± g·∫ßn g≈©i trong `vector space`.
- C√°c t·ª´ c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·ªãnh v·ªã d·ªçc theo hai tr·ª•c: m·ªôt cho **sentiment** (t·ª´ t√≠ch c·ª±c ƒë·∫øn ti√™u c·ª±c) v√† m·ªôt cho **concreteness** (t·ª´ c·ª• th·ªÉ ƒë·∫øn tr·ª´u t∆∞·ª£ng).

#### Creating Word Vectors

- M·ªôt `two-dimensional vector` (v√©c-t∆° hai chi·ªÅu) c√≥ th·ªÉ bi·ªÉu di·ªÖn c√°c t·ª´, trong ƒë√≥ c√°c `coordinates` (t·ªça ƒë·ªô) ch·ªâ ra `sentiment` v√† m·ª©c ƒë·ªô tr·ª´u t∆∞·ª£ng c·ªßa ch√∫ng.
- Bi·ªÉu di·ªÖn n√†y cho ph√©p x√°c ƒë·ªãnh s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c t·ª´, ch·∫≥ng h·∫°n nh∆∞ "happy" v√† "excited" g·∫ßn nhau h∆°n "paper."

#### Applications and Importance

- **Word embeddings** t·∫°o ƒëi·ªÅu ki·ªán cho c√°c `natural language processing` (`NLP`) `tasks` kh√°c nhau, bao g·ªìm **analogies** (s·ª± t∆∞∆°ng t·ª±) v√† **sentence meaning encoding** (m√£ h√≥a √Ω nghƒ©a c√¢u).
- B√†i gi·∫£ng nh·∫•n m·∫°nh r·∫±ng vi·ªác t·∫°o `word embeddings` l√† m·ªôt m·ª•c ti√™u ch√≠nh c·ªßa m√¥-ƒëun n√†y, d·∫´n ƒë·∫øn c√°c `NLP applications` ph·ª©c t·∫°p h∆°n nh∆∞ **question answering** v√† **translation**.

> V·∫≠y t·∫°i sao l·∫°i s·ª≠ d·ª•ng **word embeddings**? H√£y c√πng xem.

![04_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/04_Word_Embeddings.png)

> T·ª´ s∆° ƒë·ªì tr√™n, b·∫°n c√≥ th·ªÉ th·∫•y r·∫±ng khi `encode` (m√£ h√≥a) m·ªôt t·ª´ trong kh√¥ng gian **2D**, c√°c t·ª´ t∆∞∆°ng t·ª± c√≥ xu h∆∞·ªõng n·∫±m g·∫ßn nhau. C√≥ l·∫Ω **coordinate** ƒë·∫ßu ti√™n ƒë·∫°i di·ªán cho vi·ªác m·ªôt t·ª´ l√† t√≠ch c·ª±c hay ti√™u c·ª±c. **Coordinate** th·ª© hai cho b·∫°n bi·∫øt t·ª´ ƒë√≥ l√† **abstract** (tr·ª´u t∆∞·ª£ng) hay **concrete** (c·ª• th·ªÉ). ƒê√¢y ch·ªâ l√† m·ªôt v√≠ d·ª•, trong th·∫ø gi·ªõi th·ª±c, b·∫°n s·∫Ω t√¨m th·∫•y c√°c `embeddings` v·ªõi h√†ng trƒÉm **dimensions** (chi·ªÅu). B·∫°n c√≥ th·ªÉ coi m·ªói **coordinate** l√† m·ªôt con s·ªë cho b·∫°n bi·∫øt ƒëi·ªÅu g√¨ ƒë√≥ v·ªÅ t·ª´ ƒë√≥.

> ∆Øu ƒëi·ªÉm c·ªßa Word Embeddings

- **Low dimensions** (Chi·ªÅu th·∫•p) (√≠t h∆°n $V$, k√≠ch th∆∞·ªõc `vocabulary`).
- Cho ph√©p b·∫°n `encode` (m√£ h√≥a) √Ω nghƒ©a (`meaning`).

---
### **How to Create Word Embeddings**
---

N·ªôi dung n√†y t·∫≠p trung v√†o qu√° tr√¨nh t·∫°o **word embeddings** trong `natural language processing` (`NLP`).

#### C√°c Th√†nh ph·∫ßn Thi·∫øt y·∫øu

ƒê·ªÉ t·∫°o **word embeddings** c·∫ßn hai th√†nh ph·∫ßn ch√≠nh:

- **Corpus** (Kho ng·ªØ li·ªáu) vƒÉn b·∫£n.
- **Embedding method** (Ph∆∞∆°ng ph√°p nh√∫ng).

`Corpus` ph·∫£i li√™n quan ƒë·∫øn ng·ªØ c·∫£nh. V√≠ d·ª•, ƒë·ªÉ t·∫°o `Shakespearean embeddings`, b·∫°n c·∫ßn s·ª≠ d·ª•ng vƒÉn b·∫£n g·ªëc c·ªßa Shakespeare ch·ª© kh√¥ng ph·∫£i ch·ªâ l√† c√°c ghi ch√∫ t√≥m t·∫Øt.

#### T·∫ßm quan tr·ªçng c·ªßa Context

- **Context** (Ng·ªØ c·∫£nh) ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c t·ª´ xung quanh cung c·∫•p √Ω nghƒ©a cho m·ªói **word embedding**.
- M·ªôt `vocabulary list` ƒë∆°n gi·∫£n l√† kh√¥ng ƒë·ªß; c·∫ßn c√≥ m·ªôt `corpus` to√†n di·ªán ƒë·ªÉ n·∫Øm b·∫Øt c√°c s·∫Øc th√°i ng·ªØ nghƒ©a.

#### Ph∆∞∆°ng ph√°p v√† Gi√°m s√°t

- **Embedding method**, th∆∞·ªùng d·ª±a tr√™n c√°c `machine learning models`, t·∫°o ra **word embeddings** t·ª´ `corpus`.
- `Learning task` c√≥ th·ªÉ l√† **self-supervised** (t·ª± gi√°m s√°t), t·∫≠n d·ª•ng d·ªØ li·ªáu kh√¥ng c√≥ nh√£n trong khi `model` t·ª± cung c·∫•p ng·ªØ c·∫£nh c·ªßa ri√™ng n√≥ ƒë·ªÉ gi√°m s√°t.

#### Hyperparameters v√† Bi·ªÉu di·ªÖn To√°n h·ªçc

- **Word embeddings** c√≥ th·ªÉ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh b·∫±ng **hyperparameters** (si√™u tham s·ªë), ch·∫≥ng h·∫°n nh∆∞ **dimension** (chi·ªÅu) c·ªßa c√°c `embedding vectors`, th∆∞·ªùng dao ƒë·ªông t·ª´ h√†ng trƒÉm ƒë·∫øn h√†ng ngh√¨n.
- `Corpus` ph·∫£i ƒë∆∞·ª£c bi·∫øn ƒë·ªïi th√†nh m·ªôt **bi·ªÉu di·ªÖn to√°n h·ªçc** ph√π h·ª£p cho `model`, th∆∞·ªùng s·ª≠ d·ª•ng **integer-based indices** ho·∫∑c **one-hot vectors**.

N·ªôi dung s·∫Øp t·ªõi s·∫Ω gi·ªõi thi·ªáu c√°c `word embedding methods` kh√°c nhau, bao g·ªìm c√°ch ti·∫øp c·∫≠n **continuous bag-of-words** (`CBOW`), c√°i m√† s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai trong b√†i t·∫≠p ti·∫øp theo.

> ƒê·ªÉ t·∫°o **word embeddings**, b·∫°n lu√¥n c·∫ßn m·ªôt **corpus** (kho ng·ªØ li·ªáu) vƒÉn b·∫£n v√† m·ªôt **embedding method** (ph∆∞∆°ng ph√°p nh√∫ng). **Context** (Ng·ªØ c·∫£nh) c·ªßa m·ªôt t·ª´ cho b·∫°n bi·∫øt lo·∫°i t·ª´ n√†o c√≥ xu h∆∞·ªõng x·∫£y ra g·∫ßn t·ª´ c·ª• th·ªÉ ƒë√≥. **Context** l√† quan tr·ªçng v√¨ ƒë√¢y l√† y·∫øu t·ªë s·∫Ω mang l·∫°i √Ω nghƒ©a cho m·ªói `word embedding`.


#### Ph∆∞∆°ng ph√°p Embeddings v√† T·ª± gi√°m s√°t

> C√≥ nhi·ªÅu lo·∫°i ph∆∞∆°ng ph√°p c√≥ th·ªÉ cho ph√©p b·∫°n h·ªçc c√°c **word embeddings**. `Machine learning model` th·ª±c hi·ªán m·ªôt `learning task` (nhi·ªám v·ª• h·ªçc t·∫≠p), v√† s·∫£n ph·∫©m ph·ª• ch√≠nh c·ªßa `task` n√†y l√† c√°c `word embeddings`. `Task` c√≥ th·ªÉ l√† h·ªçc c√°ch d·ª± ƒëo√°n m·ªôt t·ª´ d·ª±a tr√™n c√°c t·ª´ xung quanh trong m·ªôt c√¢u c·ªßa `corpus`, nh∆∞ trong tr∆∞·ªùng h·ª£p c·ªßa **continuous bag-of-words** (`CBOW`).

> `Task` l√† **self-supervised** (t·ª± gi√°m s√°t): n√≥ v·ª´a l√† **unsupervised** (kh√¥ng gi√°m s√°t) ·ªü ch·ªó d·ªØ li·ªáu ƒë·∫ßu v√†o ‚Äî `corpus` ‚Äî l√† **unlabelled** (kh√¥ng c√≥ nh√£n), v√† v·ª´a l√† **supervised** (c√≥ gi√°m s√°t) ·ªü ch·ªó b·∫£n th√¢n d·ªØ li·ªáu cung c·∫•p `context` c·∫ßn thi·∫øt m√† th√¥ng th∆∞·ªùng s·∫Ω t·∫°o th√†nh c√°c `labels` (nh√£n).

> Khi `training word vectors`, c√≥ m·ªôt s·ªë **hyperparameters** (si√™u tham s·ªë) b·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh (v√≠ d·ª•: **dimension** (chi·ªÅu) c·ªßa `word vector`).

![05_How_to_Create_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/05_How_to_Create_Word_Embeddings.png)

---
### **Word Embedding Methods**
---

N·ªôi dung n√†y t·∫≠p trung v√†o c√°c **word embedding methods** kh√°c nhau ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `natural language processing`.

#### M√¥ h√¨nh Word2Vec

* **Word2Vec** s·ª≠ d·ª•ng m·ªôt **shallow neural network** (m·∫°ng n∆°-ron n√¥ng) v·ªõi hai `architectures` (ki·∫øn tr√∫c): **continuous bag-of-words** (**CBOW**) v√† **continuous skip-gram**.
    * **CBOW** d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu d·ª±a tr√™n c√°c t·ª´ xung quanh.
    * **Skip-gram** d·ª± ƒëo√°n c√°c t·ª´ xung quanh t·ª´ m·ªôt t·ª´ `input` cho tr∆∞·ªõc.

#### C√°c K·ªπ thu·∫≠t Embeddings N√¢ng cao

* **GloVe** (`Global Vectors`) ph√¢n t√≠ch ma tr·∫≠n **word co-occurrence matrix** (ƒë·ªìng xu·∫•t hi·ªán t·ª´) ƒë·ªÉ n·∫Øm b·∫Øt √Ω nghƒ©a c·ªßa t·ª´.
* **FastText** c·∫£i ti·∫øn `skip-gram` b·∫±ng c√°ch bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng **character n-grams**, cho ph√©p n√≥ x·ª≠ l√Ω hi·ªáu qu·∫£ c√°c **unseen words** (t·ª´ ch∆∞a t·ª´ng th·∫•y).

### Contextual Word Embeddings

* C√°c `models` ti√™n ti·∫øn nh∆∞ **BERT**, **ELMo**, v√† **GPT-2** t·∫°o ra c√°c `embeddings` kh√°c nhau cho c√°c t·ª´ d·ª±a tr√™n **context** (ng·ªØ c·∫£nh) c·ªßa ch√∫ng, h·ªó tr·ª£ **polysemy** (ƒëa nghƒ©a).
* C√°c `models` n√†y c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y d∆∞·ªõi d·∫°ng **pretrained versions** (phi√™n b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc) tr·ª±c tuy·∫øn v√† c√≥ th·ªÉ ƒë∆∞·ª£c **fine-tuned** (tinh ch·ªânh) v·ªõi c√°c `corpora` c·ª• th·ªÉ ƒë·ªÉ c√≥ `performance` t·ªët h∆°n.

### üìö Ph∆∞∆°ng ph√°p Word Embedding

#### Ph∆∞∆°ng ph√°p C·ªï ƒëi·ªÉn (`Classical Methods`)

* **word2vec** (Google, 2013):
    * **Continuous bag-of-words (CBOW)**: `model` h·ªçc c√°ch **d·ª± ƒëo√°n** t·ª´ trung t√¢m (`center word`) cho tr∆∞·ªõc c√°c `context words` (t·ª´ ng·ªØ c·∫£nh).
    * **Continuous skip-gram / Skip-gram with negative sampling (SGNS)**: `model` h·ªçc c√°ch **d·ª± ƒëo√°n** c√°c t·ª´ xung quanh (`surrounding words`) cho tr∆∞·ªõc m·ªôt t·ª´ `input`.

* **Global Vectors (GloVe)** (Stanford, 2014): Ph√¢n t√≠ch `logarithm` c·ªßa **word co-occurrence matrix** (ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán t·ª´) c·ªßa `corpus`, t∆∞∆°ng t·ª± nh∆∞ `count matrix` b·∫°n ƒë√£ s·ª≠ d·ª•ng tr∆∞·ªõc ƒë√¢y.
* **fastText** (Facebook, 2016): D·ª±a tr√™n `skip-gram model` v√† t√≠nh ƒë·∫øn c·∫•u tr√∫c c·ªßa t·ª´ b·∫±ng c√°ch bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng `n-gram` c·ªßa k√Ω t·ª±. N√≥ h·ªó tr·ª£ c√°c t·ª´ **out-of-vocabulary (OOV)**.

#### Deep Learning, Contextual Embeddings

Trong c√°c `models` ti√™n ti·∫øn h∆°n n√†y, c√°c t·ª´ c√≥ c√°c `embeddings` kh√°c nhau t√πy thu·ªôc v√†o **context** (ng·ªØ c·∫£nh) c·ªßa ch√∫ng. B·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng c√°c `pre-trained embeddings` (embeddings ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc) cho c√°c `models` sau:

* **BERT** (Google, 2018)
* **ELMo** (Allen Institute for AI, 2018)
* **GPT-2** (OpenAI, 2018)

---
### **Continuous Bag-of-Words Model**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác tri·ªÉn khai **continuous bag-of-words model** (**CBOW**) ƒë·ªÉ t·∫°o `word embeddings` trong `natural language processing`.

#### Qu√° tr√¨nh t·ªïng th·ªÉ c·ªßa Word Embeddings

* **Word embeddings** ƒë∆∞·ª£c t·∫°o ra th√¥ng qua m·ªôt `machine learning model` h·ªçc t·ª´ m·ªôt `corpus` (kho ng·ªØ li·ªáu).
* **Continuous bag-of-words model** d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu (**center word**) d·ª±a tr√™n c√°c **context words** (t·ª´ ng·ªØ c·∫£nh) xung quanh n√≥.

#### T·∫°o D·ªØ li·ªáu Hu·∫•n luy·ªán

* **Context words** ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† c√°c t·ª´ bao quanh m·ªôt **center word**, v·ªõi m·ªôt **hyperparameter $C$** x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng `context words` (b√°n k√≠nh c·ª≠a s·ªï ng·ªØ c·∫£nh).
* `Model` s·ª≠ d·ª•ng **sliding windows** (c·ª≠a s·ªï tr∆∞·ª£t) ƒë·ªÉ t·∫°o c√°c `training examples` (v√≠ d·ª• hu·∫•n luy·ªán), trong ƒë√≥ `context words` l√† `inputs` v√† `center word` l√† `target` (m·ª•c ti√™u) ƒë·ªÉ d·ª± ƒëo√°n.

#### Ki·∫øn tr√∫c M√¥ h√¨nh v√† H·ªçc t·∫≠p

* **Model architecture** bao g·ªìm `context words` l√† `inputs` v√† `center words` l√† `outputs`.
* Khi `model` h·ªçc, n√≥ t·∫°o ra `word embeddings` nh∆∞ m·ªôt s·∫£n ph·∫©m ph·ª• c·ªßa `prediction task` (nhi·ªám v·ª• d·ª± ƒëo√°n), n·∫Øm b·∫Øt ƒë∆∞·ª£c **semantic relationships** (m·ªëi quan h·ªá ng·ªØ nghƒ©a) gi·ªØa c√°c t·ª´.

> ƒê·ªÉ t·∫°o **word embeddings**, b·∫°n c·∫ßn m·ªôt `corpus` v√† m·ªôt `learning algorithm` (thu·∫≠t to√°n h·ªçc t·∫≠p). S·∫£n ph·∫©m ph·ª• c·ªßa `task` n√†y s·∫Ω l√† m·ªôt t·∫≠p h·ª£p c√°c `word embeddings`. Trong tr∆∞·ªùng h·ª£p c·ªßa **continuous bag-of-words model** (**CBOW**), `objective` (m·ª•c ti√™u) c·ªßa `task` l√† **d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu** d·ª±a tr√™n c√°c t·ª´ xung quanh n√≥.

![06_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/06_Continuous_Bag-of-Words_Model.png)

> D∆∞·ªõi ƒë√¢y l√† m·ªôt **visualization** (h√¨nh ·∫£nh tr·ª±c quan) cho b·∫°n th·∫•y `model` ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o.

![07_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/07_Continuous_Bag-of-Words_Model.png)

> Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, **window size** (k√≠ch th∆∞·ªõc c·ª≠a s·ªï) trong h√¨nh ·∫£nh ph√≠a tr√™n l√† 5. **Context size** (k√≠ch th∆∞·ªõc ng·ªØ c·∫£nh), $C$, l√† 2. $C$ th∆∞·ªùng cho b·∫°n bi·∫øt c√≥ bao nhi√™u t·ª´ tr∆∞·ªõc ho·∫∑c sau **center word** (t·ª´ trung t√¢m) m√† `model` s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ ƒë∆∞a ra **prediction** (d·ª± ƒëo√°n).

> D∆∞·ªõi ƒë√¢y l√† m·ªôt **visualization** kh√°c cho th·∫•y t·ªïng quan v·ªÅ `model`.

![08_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/08_Continuous_Bag-of-Words_Model.png)

---
### **Cleaning and Tokenization**
---

N·ªôi dung t·∫≠p trung v√†o c√°c quy tr√¨nh **cleaning** (l√†m s·∫°ch) v√† **tokenization** (t·∫°o token) trong `natural language processing` (`NLP`).

#### Cleaning v√† Tokenization

- C√°c t·ª´ n√™n ƒë∆∞·ª£c x·ª≠ l√Ω d∆∞·ªõi d·∫°ng **case insensitive** (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng), nghƒ©a l√† ch√∫ng n√™n ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh m·ªôt ƒë·ªãnh d·∫°ng duy nh·∫•t (ch·ªØ th∆∞·ªùng ho·∫∑c ch·ªØ hoa) ƒë·ªÉ ƒë·ªìng nh·∫•t.
- **Punctuation** (D·∫•u c√¢u) c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω c·∫©n th·∫≠n; d·∫•u c√¢u g√¢y ng·∫Øt qu√£ng c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu th·ªã b·∫±ng m·ªôt `special word` (t·ª´ ƒë·∫∑c bi·ªát) duy nh·∫•t, trong khi d·∫•u c√¢u kh√¥ng g√¢y ng·∫Øt qu√£ng c√≥ th·ªÉ b·ªã b·ªè qua.

#### X·ª≠ l√Ω S·ªë v√† K√Ω t·ª± ƒê·∫∑c bi·ªát

- **Numbers** (S·ªë) c√≥ th·ªÉ b·ªã b·ªè ƒëi n·∫øu ch√∫ng kh√¥ng quan tr·ªçng, nh∆∞ng c√°c s·ªë quan tr·ªçng n√™n ƒë∆∞·ª£c gi·ªØ l·∫°i ho·∫∑c thay th·∫ø b·∫±ng m·ªôt `special token` nh∆∞ **\<NUMBER\>**.
- C√°c **Special characters** (K√Ω t·ª± ƒë·∫∑c bi·ªát), ch·∫≥ng h·∫°n nh∆∞ k√Ω hi·ªáu to√°n h·ªçc v√† `emojis`, n√™n ƒë∆∞·ª£c qu·∫£n l√Ω d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan c·ªßa ch√∫ng v·ªõi `model`.

### V√≠ d·ª• Th·ª±c h√†nh

M·ªôt `Python example` minh h·ªça c√°ch `clean` m·ªôt `corpus` b·∫±ng c√°ch g·ªôp `punctuation` v√† **tokenizing** vƒÉn b·∫£n b·∫±ng c√°ch s·ª≠ d·ª•ng `NLTK library`, t·∫°o ra m·ªôt `array of tokens` s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch th√™m.

ƒêi·ªÅu n√†y t·∫°o ti·ªÅn ƒë·ªÅ cho ch·ªß ƒë·ªÅ ti·∫øp theo v·ªÅ **continuous bag-of-words model**.

> Tr∆∞·ªõc khi tri·ªÉn khai b·∫•t k·ª≥ thu·∫≠t to√°n `natural language processing` (`NLP`) n√†o, b·∫°n c√≥ th·ªÉ mu·ªën `clean` (l√†m s·∫°ch) d·ªØ li·ªáu v√† `tokenize` (t·∫°o token) n√≥. D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i ƒëi·ªÅu c·∫ßn l∆∞u √Ω khi x·ª≠ l√Ω `data` c·ªßa b·∫°n.

![09_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/09_Cleaning_and_Tokenization.png)

> B·∫°n c√≥ th·ªÉ `clean data` b·∫±ng `Python` nh∆∞ sau:

![10_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/10_Cleaning_and_Tokenization.png)

> B·∫°n c√≥ th·ªÉ th√™m bao nhi√™u ƒëi·ªÅu ki·ªán t√πy th√≠ch v√†o c√°c d√≤ng t∆∞∆°ng ·ª©ng v·ªõi h√¨nh ch·ªØ nh·∫≠t m√†u xanh l√° c√¢y ph√≠a tr√™n.

---
### **Sliding Window of Words in Python**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác tr√≠ch xu·∫•t **center words** (t·ª´ trung t√¢m) v√† **context words** (t·ª´ ng·ªØ c·∫£nh) ƒë·ªÉ `training` (hu·∫•n luy·ªán) **continuous bag-of-words model** trong `natural language processing`.

#### Tr√≠ch xu·∫•t Center v√† Context Words

* Qu√° tr√¨nh b·∫Øt ƒë·∫ßu v·ªõi m·ªôt `cleaned and tokenized corpus` (kho ng·ªØ li·ªáu ƒë√£ l√†m s·∫°ch v√† t·∫°o token), ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt `array` (m·∫£ng) c√°c t·ª´.
* M·ªôt `function` (h√†m) g·ªçi l√† `get_windows` ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ƒë·ªÉ tr√≠ch xu·∫•t `center words` v√† `context words` c·ªßa ch√∫ng d·ª±a tr√™n m·ªôt `context size` ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.

#### Tri·ªÉn khai Function

* `Function` nh·∫≠n m·ªôt `array of words` v√† m·ªôt **context size ($C$)**, c√°i m√† x√°c ƒë·ªãnh c√≥ bao nhi√™u t·ª´ s·∫Ω ƒë∆∞·ª£c xem x√©t ·ªü m·ªói b√™n c·ªßa `center word`.
* N√≥ `initialize` (kh·ªüi t·∫°o) m·ªôt v√≤ng l·∫∑p ƒë·ªÉ l·∫∑p qua `array`, tr√≠ch xu·∫•t `center words` v√† c√°c `context words` t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng.

#### S·ª≠ d·ª•ng Function

* `Function` s·ª≠ d·ª•ng **yield keyword** ƒë·ªÉ tr·∫£ v·ªÅ c√°c gi√° tr·ªã m·ªôt c√°ch **iteratively** (l·∫∑p l·∫°i), cho ph√©p **data generation** (t·∫°o d·ªØ li·ªáu) hi·ªáu qu·∫£.
* M·ªôt v√≤ng l·∫∑p ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hi·ªÉn th·ªã `context` v√† `center words`, c√°i m√† c·∫ßn thi·∫øt cho **continuous bag-of-words model**.

T·ªïng quan, b√†i gi·∫£ng n√†y cung c·∫•p m·ªôt c√°ch ti·∫øp c·∫≠n th·ª±c t·∫ø ƒë·ªÉ chu·∫©n b·ªã `data` cho `training word embeddings` b·∫±ng `Python`.

![11_Sliding_Window_of_Words_in_Python](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/11_Sliding_Window_of_Words_in_Python.png)

> `Code` ph√≠a tr√™n cho th·∫•y m·ªôt `function` (h√†m) nh·∫≠n hai `parameters` (tham s·ªë).

* `Words`: m·ªôt `list` (danh s√°ch) c√°c t·ª´.
* $C$: **context size** (k√≠ch th∆∞·ªõc ng·ªØ c·∫£nh).

> Ch√∫ng ta b·∫Øt ƒë·∫ßu b·∫±ng c√°ch ƒë·∫∑t $i$ b·∫±ng $C$. Sau ƒë√≥, ch√∫ng ta t√°ch **center\_word** (t·ª´ trung t√¢m) v√† **context\_words** (c√°c t·ª´ ng·ªØ c·∫£nh). Ch√∫ng ta sau ƒë√≥ **yield** (tr·∫£ v·ªÅ) c√°c gi√° tr·ªã n√†y v√† **increment** (tƒÉng) $i$ l√™n.


---
### **Transforming Words into Vectors**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác chu·∫©n b·ªã `data` (d·ªØ li·ªáu) cho **continuous bag-of-words model** (**CBOW**) trong `natural language processing`.

#### Chu·∫©n b·ªã D·ªØ li·ªáu cho M√¥ h√¨nh CBOW

* **Context v√† Central Words**: Qu√° tr√¨nh b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác x√°c ƒë·ªãnh **context words** v√† **central word** (t·ª´ trung t√¢m) t·ª´ m·ªôt `sliding window` (c·ª≠a s·ªï tr∆∞·ª£t) tr√™n `corpus` (kho ng·ªØ li·ªáu).
* **Vocabulary Creation**: M·ªôt **vocabulary** (t·ª´ v·ª±ng) ƒë∆∞·ª£c h√¨nh th√†nh t·ª´ c√°c t·ª´ ƒë·ªôc nh·∫•t (`unique words`) trong `corpus`, sau ƒë√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o **one-hot vectors** cho c√°c `central words`.

#### Bi·ªÉu di·ªÖn Vector

* **One-Hot Encoding**: M·ªói t·ª´ trong `vocabulary` ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **one-hot vector**, trong ƒë√≥ '1' cho bi·∫øt s·ª± hi·ªán di·ªán c·ªßa m·ªôt t·ª´ v√† '0' cho bi·∫øt s·ª± v·∫Øng m·∫∑t.
* **Averaging Context Vectors**: ƒê·ªëi v·ªõi **context words**, m·ªôt `vector` duy nh·∫•t ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch **averaging** (t√≠nh trung b√¨nh) c√°c `one-hot vectors` c·ªßa m·ªói `context word`, cung c·∫•p m·ªôt bi·ªÉu di·ªÖn cho `context`.

#### Chu·∫©n b·ªã D·ªØ li·ªáu Hu·∫•n luy·ªán

* **Final Vector Representation**: C√°c `final vectors` (v√©c-t∆° cu·ªëi c√πng) cho c·∫£ `central words` v√† `context words` ƒë∆∞·ª£c chu·∫©n b·ªã ƒë·ªÉ `training` (hu·∫•n luy·ªán) **CBOW model**.
* **Transition to Model Learning**: V·ªõi `data` ƒë√£ ƒë∆∞·ª£c bi·ªÉu di·ªÖn ƒë·∫ßy ƒë·ªß, b∆∞·ªõc ti·∫øp theo l√† t√¨m hi·ªÉu v·ªÅ **architecture** (ki·∫øn tr√∫c) c·ªßa **CBOW model** v√† √°p d·ª•ng c√°c k·ªπ nƒÉng v√†o c√°c b√†i t·∫≠p s·∫Øp t·ªõi.

> ƒê·ªÉ bi·∫øn ƒë·ªïi c√°c **context vectors** (v√©c-t∆° ng·ªØ c·∫£nh) th√†nh m·ªôt **single vector** (v√©c-t∆° ƒë∆°n l·∫ª), b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√¥ng th·ª©c/ph∆∞∆°ng ph√°p sau:

![12_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/12_Transforming_Words_into_Vectors.png)

> Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, ch√∫ng ta b·∫Øt ƒë·∫ßu v·ªõi c√°c **one-hot vectors** cho c√°c t·ª´ ng·ªØ c·∫£nh v√† bi·∫øn ƒë·ªïi ch√∫ng th√†nh m·ªôt **single vector** b·∫±ng c√°ch l·∫•y **average** (trung b√¨nh). K·∫øt qu·∫£ l√† b·∫°n nh·∫≠n ƒë∆∞·ª£c c√°c `vectors` sau ƒë√¢y m√† b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng cho vi·ªác **training** (hu·∫•n luy·ªán) c·ªßa m√¨nh.

![13_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/13_Transforming_Words_into_Vectors.png)

---
### **Architecture of the CBOW Model**
---

N·ªôi dung t·∫≠p trung v√†o **architecture** (ki·∫øn tr√∫c) c·ªßa **Continuous Bag of Words** (**CBOW**) `model` ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `natural language processing`.

#### T·ªïng quan Ki·∫øn tr√∫c

* **CBOW model** bao g·ªìm m·ªôt **shallow dense neural network** (m·∫°ng n∆°-ron d√†y ƒë·∫∑c n√¥ng) v·ªõi m·ªôt `input layer`, m·ªôt `hidden layer` (l·ªõp ·∫©n), v√† m·ªôt `output layer`.
* `Input` l√† m·ªôt `vector` c·ªßa **context words** (c√°c t·ª´ ng·ªØ c·∫£nh), trong khi `output` l√† **center word** (t·ª´ trung t√¢m) ƒë∆∞·ª£c d·ª± ƒëo√°n, c·∫£ hai ƒë·ªÅu c√≥ k√≠ch th∆∞·ªõc theo **vocabulary** ($V$).

#### Chi ti·∫øt c√°c L·ªõp

* K√≠ch th∆∞·ªõc c·ªßa `hidden layer` ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi **dimension** (chi·ªÅu) ƒë√£ ch·ªçn c·ªßa **word embeddings** ($N$), th∆∞·ªùng dao ƒë·ªông t·ª´ $100$ ƒë·∫øn $1,000$.
* `Network` l√† **fully connected** (k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß), v·ªõi c√°c **weight matrices** ($W_1$ v√† $W_2$) v√† **bias vectors** ($b_1$ v√† $b_2$) m√† `model` h·ªçc trong qu√° tr√¨nh `training`.

#### H√†m K√≠ch ho·∫°t (Activation Functions)

* `Hidden layer` s·ª≠ d·ª•ng **Rectified Linear Units** (**ReLU**) `activation function` (h√†m k√≠ch ho·∫°t).
* `Output layer` s·ª≠ d·ª•ng **softmax function** ƒë·ªÉ ƒë∆∞a ra **predictions** (d·ª± ƒëo√°n).

B·∫£n t√≥m t·∫Øt n√†y cung c·∫•p s·ª± hi·ªÉu bi·∫øt ng·∫Øn g·ªçn v·ªÅ c·∫•u tr√∫c v√† c√°c th√†nh ph·∫ßn c·ªßa `CBOW model`.

> `Architecture` (Ki·∫øn tr√∫c) cho **CBOW model** c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ sau:

![14_Architecture_of_the_CBOW_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/14_Architecture_of_the_CBOW_Model.png)

> B·∫°n c√≥ m·ªôt `input`, $X$, l√† gi√° tr·ªã trung b√¨nh (`average`) c·ªßa t·∫•t c·∫£ c√°c **context vectors** (v√©c-t∆° ng·ªØ c·∫£nh). Sau ƒë√≥, b·∫°n nh√¢n n√≥ v·ªõi $W_1$ v√† c·ªông th√™m $b_1$. K·∫øt qu·∫£ n√†y ƒëi qua m·ªôt **ReLU function** ƒë·ªÉ t·∫°o ra **hidden layer** (l·ªõp ·∫©n) c·ªßa b·∫°n. L·ªõp ƒë√≥ sau ƒë√≥ ƒë∆∞·ª£c nh√¢n v·ªõi $W_2$ v√† b·∫°n c·ªông th√™m $b_2$. K·∫øt qu·∫£ n√†y ƒëi qua m·ªôt **softmax** ƒë·ªÉ cung c·∫•p cho b·∫°n m·ªôt **distribution** (ph√¢n ph·ªëi) tr√™n $V$ (k√≠ch th∆∞·ªõc `vocabulary`). B·∫°n ch·ªçn `vocabulary word` t∆∞∆°ng ·ª©ng v·ªõi **arg-max** c·ªßa `output`.


---
### **Architecture of the CBOW Model: Dimensions**
---

---
### **Architecture of the CBOW Model: Dimensions 2**
---

---
### **Architecture of the CBOW Model: Activation Functions**
---


---
### **Training a CBOW Model: Cost Function**
---

---
### **Training a CBOW Model: Forward Propagation**
---


---
### **Training a CBOW Model: Backpropagation and Gradient Descent**
---


---
### **Extracting Word Embedding Vectors**
---


---
### **Evaluating Word Embeddings: Intrinsic Evaluation**
---


---
### **Evaluating Word Embeddings: Extrinsic Evaluation**
---



