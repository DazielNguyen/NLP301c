# **Module 02 - Natural Language Processing with Probabilistic Models**
## **Week 4: Word Embeddings with Neural Network**
---
### **Overview**
---

N·ªôi dung tu·∫ßn n√†y t·∫≠p trung v√†o **word vectors**, c√≤n ƒë∆∞·ª£c g·ªçi l√† **word embeddings**, v√† c√°ch `training` (hu·∫•n luy·ªán) ch√∫ng t·ª´ ƒë·∫ßu.

#### Understanding Word Vectors

- **Word vectors** r·∫•t c·∫ßn thi·∫øt cho c√°c ·ª©ng d·ª•ng kh√°c nhau trong `natural language processing` (`NLP`), ch·∫≥ng h·∫°n nh∆∞ `sentiment analysis` (ph√¢n t√≠ch t√¨nh c·∫£m) v√† `machine translation` (d·ªãch m√°y).
- Ch√∫ng cho ph√©p bi·ªÉu di·ªÖn s·ªë h·ªçc c·ªßa c√°c t·ª´, t·∫°o ƒëi·ªÅu ki·ªán cho vi·ªác s·ª≠ d·ª•ng ch√∫ng trong c√°c `mathematical models`.

#### Training Word Vectors

- Kh√≥a h·ªçc s·∫Ω ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c ph∆∞∆°ng ph√°p t·∫°o **word embeddings**, bao g·ªìm **continuous bag-of-words model** (`CBOW`).
- C√°c k·ªπ thu·∫≠t kh√°c nh∆∞ **GloVe** v√† **Word2Vec** c≈©ng s·∫Ω ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p, nh∆∞ng tr·ªçng t√¢m s·∫Ω l√† **continuous bag-of-words model**.

#### Preparing Text for Machine Learning

- Ng∆∞·ªùi h·ªçc s·∫Ω bi·∫øt c√°ch bi·∫øn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n th√†nh m·ªôt `training set` ph√π h·ª£p cho c√°c `machine learning models`.
- L·ªùi khuy√™n th·ª±c t·∫ø s·∫Ω ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ l√†m vi·ªác v·ªõi c√°c `text corpora` ƒëa d·∫°ng, ch·∫≥ng h·∫°n nh∆∞ s√°ch v√† `tweets`.

> **Word embeddings** (nh√∫ng t·ª´) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·∫ßu h·∫øt c√°c `NLP applications`. B·∫•t c·ª© khi n√†o b·∫°n x·ª≠ l√Ω vƒÉn b·∫£n, tr∆∞·ªõc ti√™n b·∫°n ph·∫£i t√¨m c√°ch ƒë·ªÉ `encode` (m√£ h√≥a) c√°c t·ª´ d∆∞·ªõi d·∫°ng s·ªë. `Word embedding` l√† m·ªôt k·ªπ thu·∫≠t r·∫•t ph·ªï bi·∫øn cho ph√©p b·∫°n l√†m ƒëi·ªÅu ƒë√≥.

> D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i `applications` (·ª©ng d·ª•ng) c·ªßa `word embeddings` m√† b·∫°n s·∫Ω c√≥ th·ªÉ tri·ªÉn khai khi ho√†n th√†nh chuy√™n ng√†nh n√†y.

![01_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/01_Overview.png)

![02_Overview](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/02_Overview.png)

#### M·ª•c ti√™u h·ªçc t·∫≠p trong tu·∫ßn

ƒê·∫øn cu·ªëi tu·∫ßn n√†y, b·∫°n s·∫Ω c√≥ th·ªÉ:

- X√°c ƒë·ªãnh c√°c kh√°i ni·ªám ch√≠nh c·ªßa **word representations** (bi·ªÉu di·ªÖn t·ª´).
- T·∫°o ra **word embeddings**.
- Chu·∫©n b·ªã vƒÉn b·∫£n cho **machine learning**.
- Tri·ªÉn khai **continuous bag-of-words model**.

---
### **Basic Word Representations**
---
N·ªôi dung t·∫≠p trung v√†o vi·ªác bi·ªÉu di·ªÖn c√°c t·ª´ trong m·ªôt `vocabulary` (t·ª´ v·ª±ng) b·∫±ng c√°c `numerical vectors` (v√©c-t∆° s·ªë), c·ª• th·ªÉ th√¥ng qua kh√°i ni·ªám **one-hot vectors**.

#### Understanding One-Hot Vectors

- M·ªói t·ª´ trong m·ªôt `vocabulary` ƒë∆∞·ª£c g√°n m·ªôt s·ªë nguy√™n (`integer`) duy nh·∫•t, nh∆∞ng ph∆∞∆°ng ph√°p n√†y thi·∫øu **semantic meaning** (√Ω nghƒ©a ng·ªØ nghƒ©a).
- **One-hot vectors** bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng c√°c `binary vectors` (v√©c-t∆° nh·ªã ph√¢n), trong ƒë√≥ '1' cho bi·∫øt s·ª± hi·ªán di·ªán c·ªßa m·ªôt t·ª´ v√† '0' cho bi·∫øt s·ª± v·∫Øng m·∫∑t.

#### Advantages and Limitations of One-Hot Vectors

- **One-hot vectors** ƒë∆°n gi·∫£n v√† kh√¥ng ng·ª• √Ω b·∫•t k·ª≥ m·ªëi quan h·ªá n√†o gi·ªØa c√°c t·ª´.
- Tuy nhi√™n, ch√∫ng c√≥ th·ªÉ tr·ªü n√™n r·∫•t l·ªõn v√† **kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a** ho·∫∑c s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c t·ª´, d·∫´n ƒë·∫øn nh·ªØng h·∫°n ch·∫ø trong c√°c `natural language processing tasks`.

#### Transition to Word Embeddings

- Cu·ªôc th·∫£o lu·∫≠n t·∫°o ti·ªÅn ƒë·ªÅ cho vi·ªác gi·ªõi thi·ªáu **word embeddings**, c√°i m√† nh·∫±m m·ª•c ƒë√≠ch gi·∫£i quy·∫øt nh·ªØng h·∫°n ch·∫ø c·ªßa `one-hot vectors` b·∫±ng c√°ch n·∫Øm b·∫Øt c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a (`semantic relationships`) gi·ªØa c√°c t·ª´.

> C√°c bi·ªÉu di·ªÖn t·ª´ c∆° b·∫£n c√≥ th·ªÉ ƒë∆∞·ª£c ph√¢n lo·∫°i th√†nh c√°c d·∫°ng sau:

- **Integers** (S·ªë nguy√™n)
- **One-hot vectors**
- **Word embeddings**

![03_Basic_Word_Representations](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/03_Basic_Word_Representations.png)

> ·ªû b√™n tr√°i, b·∫°n c√≥ m·ªôt v√≠ d·ª• trong ƒë√≥ b·∫°n s·ª≠ d·ª•ng s·ªë nguy√™n (`integers`) ƒë·ªÉ bi·ªÉu di·ªÖn m·ªôt t·ª´. V·∫•n ƒë·ªÅ ·ªü ƒë√≥ l√† kh√¥ng c√≥ l√Ω do g√¨ khi·∫øn t·ª´ n√†y t∆∞∆°ng ·ª©ng v·ªõi m·ªôt s·ªë l·ªõn h∆°n t·ª´ kh√°c. ƒê·ªÉ kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ n√†y, ch√∫ng ta gi·ªõi thi·ªáu **one hot vectors** (s∆° ƒë·ªì b√™n ph·∫£i). ƒê·ªÉ tri·ªÉn khai `one hot vectors`, b·∫°n ph·∫£i `initialize` (kh·ªüi t·∫°o) m·ªôt `vector` to√†n s·ªë kh√¥ng (`zeros`) c√≥ **dimension $V$** v√† sau ƒë√≥ ƒë·∫∑t s·ªë **1** v√†o `index` t∆∞∆°ng ·ª©ng v·ªõi t·ª´ b·∫°n ƒëang bi·ªÉu di·ªÖn.

> **∆Øu ƒëi·ªÉm** (`Pros`) c·ªßa `one-hot vectors`:
- ƒê∆°n gi·∫£n.
- Kh√¥ng y√™u c·∫ßu th·ª© t·ª± ng·ª• √Ω (`implied ordering`).

> **Nh∆∞·ª£c ƒëi·ªÉm** (`Cons`) c·ªßa `one-hot vectors`:
- R·∫•t l·ªõn (`huge`).
- Kh√¥ng `encode` (m√£ h√≥a) ƒë∆∞·ª£c √Ω nghƒ©a (`meaning`).

---
### **Word Embeddings**
---

N·ªôi dung n√†y t·∫≠p trung v√†o kh√°i ni·ªám **word embeddings**, m·ªôt ph∆∞∆°ng ph√°p ƒë·ªÉ `encode` (m√£ h√≥a) √Ω nghƒ©a c·ªßa c√°c t·ª´ trong m·ªôt **low-dimensional vector space** (kh√¥ng gian v√©c-t∆° chi·ªÅu th·∫•p).

#### Understanding Word Embeddings

- **Word embeddings** bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng c√°c `vectors` theo c√°ch n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a c·ªßa ch√∫ng, cho ph√©p so s√°nh d·ª±a tr√™n s·ª± g·∫ßn g≈©i trong `vector space`.
- C√°c t·ª´ c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·ªãnh v·ªã d·ªçc theo hai tr·ª•c: m·ªôt cho **sentiment** (t·ª´ t√≠ch c·ª±c ƒë·∫øn ti√™u c·ª±c) v√† m·ªôt cho **concreteness** (t·ª´ c·ª• th·ªÉ ƒë·∫øn tr·ª´u t∆∞·ª£ng).

#### Creating Word Vectors

- M·ªôt `two-dimensional vector` (v√©c-t∆° hai chi·ªÅu) c√≥ th·ªÉ bi·ªÉu di·ªÖn c√°c t·ª´, trong ƒë√≥ c√°c `coordinates` (t·ªça ƒë·ªô) ch·ªâ ra `sentiment` v√† m·ª©c ƒë·ªô tr·ª´u t∆∞·ª£ng c·ªßa ch√∫ng.
- Bi·ªÉu di·ªÖn n√†y cho ph√©p x√°c ƒë·ªãnh s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c t·ª´, ch·∫≥ng h·∫°n nh∆∞ "happy" v√† "excited" g·∫ßn nhau h∆°n "paper."

#### Applications and Importance

- **Word embeddings** t·∫°o ƒëi·ªÅu ki·ªán cho c√°c `natural language processing` (`NLP`) `tasks` kh√°c nhau, bao g·ªìm **analogies** (s·ª± t∆∞∆°ng t·ª±) v√† **sentence meaning encoding** (m√£ h√≥a √Ω nghƒ©a c√¢u).
- B√†i gi·∫£ng nh·∫•n m·∫°nh r·∫±ng vi·ªác t·∫°o `word embeddings` l√† m·ªôt m·ª•c ti√™u ch√≠nh c·ªßa m√¥-ƒëun n√†y, d·∫´n ƒë·∫øn c√°c `NLP applications` ph·ª©c t·∫°p h∆°n nh∆∞ **question answering** v√† **translation**.

> V·∫≠y t·∫°i sao l·∫°i s·ª≠ d·ª•ng **word embeddings**? H√£y c√πng xem.

![04_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/04_Word_Embeddings.png)

> T·ª´ s∆° ƒë·ªì tr√™n, b·∫°n c√≥ th·ªÉ th·∫•y r·∫±ng khi `encode` (m√£ h√≥a) m·ªôt t·ª´ trong kh√¥ng gian **2D**, c√°c t·ª´ t∆∞∆°ng t·ª± c√≥ xu h∆∞·ªõng n·∫±m g·∫ßn nhau. C√≥ l·∫Ω **coordinate** ƒë·∫ßu ti√™n ƒë·∫°i di·ªán cho vi·ªác m·ªôt t·ª´ l√† t√≠ch c·ª±c hay ti√™u c·ª±c. **Coordinate** th·ª© hai cho b·∫°n bi·∫øt t·ª´ ƒë√≥ l√† **abstract** (tr·ª´u t∆∞·ª£ng) hay **concrete** (c·ª• th·ªÉ). ƒê√¢y ch·ªâ l√† m·ªôt v√≠ d·ª•, trong th·∫ø gi·ªõi th·ª±c, b·∫°n s·∫Ω t√¨m th·∫•y c√°c `embeddings` v·ªõi h√†ng trƒÉm **dimensions** (chi·ªÅu). B·∫°n c√≥ th·ªÉ coi m·ªói **coordinate** l√† m·ªôt con s·ªë cho b·∫°n bi·∫øt ƒëi·ªÅu g√¨ ƒë√≥ v·ªÅ t·ª´ ƒë√≥.

> ∆Øu ƒëi·ªÉm c·ªßa Word Embeddings

- **Low dimensions** (Chi·ªÅu th·∫•p) (√≠t h∆°n $V$, k√≠ch th∆∞·ªõc `vocabulary`).
- Cho ph√©p b·∫°n `encode` (m√£ h√≥a) √Ω nghƒ©a (`meaning`).

---
### **How to Create Word Embeddings**
---

N·ªôi dung n√†y t·∫≠p trung v√†o qu√° tr√¨nh t·∫°o **word embeddings** trong `natural language processing` (`NLP`).

#### C√°c Th√†nh ph·∫ßn Thi·∫øt y·∫øu

ƒê·ªÉ t·∫°o **word embeddings** c·∫ßn hai th√†nh ph·∫ßn ch√≠nh:

- **Corpus** (Kho ng·ªØ li·ªáu) vƒÉn b·∫£n.
- **Embedding method** (Ph∆∞∆°ng ph√°p nh√∫ng).

`Corpus` ph·∫£i li√™n quan ƒë·∫øn ng·ªØ c·∫£nh. V√≠ d·ª•, ƒë·ªÉ t·∫°o `Shakespearean embeddings`, b·∫°n c·∫ßn s·ª≠ d·ª•ng vƒÉn b·∫£n g·ªëc c·ªßa Shakespeare ch·ª© kh√¥ng ph·∫£i ch·ªâ l√† c√°c ghi ch√∫ t√≥m t·∫Øt.

#### T·∫ßm quan tr·ªçng c·ªßa Context

- **Context** (Ng·ªØ c·∫£nh) ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c t·ª´ xung quanh cung c·∫•p √Ω nghƒ©a cho m·ªói **word embedding**.
- M·ªôt `vocabulary list` ƒë∆°n gi·∫£n l√† kh√¥ng ƒë·ªß; c·∫ßn c√≥ m·ªôt `corpus` to√†n di·ªán ƒë·ªÉ n·∫Øm b·∫Øt c√°c s·∫Øc th√°i ng·ªØ nghƒ©a.

#### Ph∆∞∆°ng ph√°p v√† Gi√°m s√°t

- **Embedding method**, th∆∞·ªùng d·ª±a tr√™n c√°c `machine learning models`, t·∫°o ra **word embeddings** t·ª´ `corpus`.
- `Learning task` c√≥ th·ªÉ l√† **self-supervised** (t·ª± gi√°m s√°t), t·∫≠n d·ª•ng d·ªØ li·ªáu kh√¥ng c√≥ nh√£n trong khi `model` t·ª± cung c·∫•p ng·ªØ c·∫£nh c·ªßa ri√™ng n√≥ ƒë·ªÉ gi√°m s√°t.

#### Hyperparameters v√† Bi·ªÉu di·ªÖn To√°n h·ªçc

- **Word embeddings** c√≥ th·ªÉ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh b·∫±ng **hyperparameters** (si√™u tham s·ªë), ch·∫≥ng h·∫°n nh∆∞ **dimension** (chi·ªÅu) c·ªßa c√°c `embedding vectors`, th∆∞·ªùng dao ƒë·ªông t·ª´ h√†ng trƒÉm ƒë·∫øn h√†ng ngh√¨n.
- `Corpus` ph·∫£i ƒë∆∞·ª£c bi·∫øn ƒë·ªïi th√†nh m·ªôt **bi·ªÉu di·ªÖn to√°n h·ªçc** ph√π h·ª£p cho `model`, th∆∞·ªùng s·ª≠ d·ª•ng **integer-based indices** ho·∫∑c **one-hot vectors**.

N·ªôi dung s·∫Øp t·ªõi s·∫Ω gi·ªõi thi·ªáu c√°c `word embedding methods` kh√°c nhau, bao g·ªìm c√°ch ti·∫øp c·∫≠n **continuous bag-of-words** (`CBOW`), c√°i m√† s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai trong b√†i t·∫≠p ti·∫øp theo.

> ƒê·ªÉ t·∫°o **word embeddings**, b·∫°n lu√¥n c·∫ßn m·ªôt **corpus** (kho ng·ªØ li·ªáu) vƒÉn b·∫£n v√† m·ªôt **embedding method** (ph∆∞∆°ng ph√°p nh√∫ng). **Context** (Ng·ªØ c·∫£nh) c·ªßa m·ªôt t·ª´ cho b·∫°n bi·∫øt lo·∫°i t·ª´ n√†o c√≥ xu h∆∞·ªõng x·∫£y ra g·∫ßn t·ª´ c·ª• th·ªÉ ƒë√≥. **Context** l√† quan tr·ªçng v√¨ ƒë√¢y l√† y·∫øu t·ªë s·∫Ω mang l·∫°i √Ω nghƒ©a cho m·ªói `word embedding`.


#### Ph∆∞∆°ng ph√°p Embeddings v√† T·ª± gi√°m s√°t

> C√≥ nhi·ªÅu lo·∫°i ph∆∞∆°ng ph√°p c√≥ th·ªÉ cho ph√©p b·∫°n h·ªçc c√°c **word embeddings**. `Machine learning model` th·ª±c hi·ªán m·ªôt `learning task` (nhi·ªám v·ª• h·ªçc t·∫≠p), v√† s·∫£n ph·∫©m ph·ª• ch√≠nh c·ªßa `task` n√†y l√† c√°c `word embeddings`. `Task` c√≥ th·ªÉ l√† h·ªçc c√°ch d·ª± ƒëo√°n m·ªôt t·ª´ d·ª±a tr√™n c√°c t·ª´ xung quanh trong m·ªôt c√¢u c·ªßa `corpus`, nh∆∞ trong tr∆∞·ªùng h·ª£p c·ªßa **continuous bag-of-words** (`CBOW`).

> `Task` l√† **self-supervised** (t·ª± gi√°m s√°t): n√≥ v·ª´a l√† **unsupervised** (kh√¥ng gi√°m s√°t) ·ªü ch·ªó d·ªØ li·ªáu ƒë·∫ßu v√†o ‚Äî `corpus` ‚Äî l√† **unlabelled** (kh√¥ng c√≥ nh√£n), v√† v·ª´a l√† **supervised** (c√≥ gi√°m s√°t) ·ªü ch·ªó b·∫£n th√¢n d·ªØ li·ªáu cung c·∫•p `context` c·∫ßn thi·∫øt m√† th√¥ng th∆∞·ªùng s·∫Ω t·∫°o th√†nh c√°c `labels` (nh√£n).

> Khi `training word vectors`, c√≥ m·ªôt s·ªë **hyperparameters** (si√™u tham s·ªë) b·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh (v√≠ d·ª•: **dimension** (chi·ªÅu) c·ªßa `word vector`).

![05_How_to_Create_Word_Embeddings](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/05_How_to_Create_Word_Embeddings.png)

---
### **Word Embedding Methods**
---

N·ªôi dung n√†y t·∫≠p trung v√†o c√°c **word embedding methods** kh√°c nhau ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `natural language processing`.

#### M√¥ h√¨nh Word2Vec

* **Word2Vec** s·ª≠ d·ª•ng m·ªôt **shallow neural network** (m·∫°ng n∆°-ron n√¥ng) v·ªõi hai `architectures` (ki·∫øn tr√∫c): **continuous bag-of-words** (**CBOW**) v√† **continuous skip-gram**.
    * **CBOW** d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu d·ª±a tr√™n c√°c t·ª´ xung quanh.
    * **Skip-gram** d·ª± ƒëo√°n c√°c t·ª´ xung quanh t·ª´ m·ªôt t·ª´ `input` cho tr∆∞·ªõc.

#### C√°c K·ªπ thu·∫≠t Embeddings N√¢ng cao

* **GloVe** (`Global Vectors`) ph√¢n t√≠ch ma tr·∫≠n **word co-occurrence matrix** (ƒë·ªìng xu·∫•t hi·ªán t·ª´) ƒë·ªÉ n·∫Øm b·∫Øt √Ω nghƒ©a c·ªßa t·ª´.
* **FastText** c·∫£i ti·∫øn `skip-gram` b·∫±ng c√°ch bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng **character n-grams**, cho ph√©p n√≥ x·ª≠ l√Ω hi·ªáu qu·∫£ c√°c **unseen words** (t·ª´ ch∆∞a t·ª´ng th·∫•y).

### Contextual Word Embeddings

* C√°c `models` ti√™n ti·∫øn nh∆∞ **BERT**, **ELMo**, v√† **GPT-2** t·∫°o ra c√°c `embeddings` kh√°c nhau cho c√°c t·ª´ d·ª±a tr√™n **context** (ng·ªØ c·∫£nh) c·ªßa ch√∫ng, h·ªó tr·ª£ **polysemy** (ƒëa nghƒ©a).
* C√°c `models` n√†y c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y d∆∞·ªõi d·∫°ng **pretrained versions** (phi√™n b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc) tr·ª±c tuy·∫øn v√† c√≥ th·ªÉ ƒë∆∞·ª£c **fine-tuned** (tinh ch·ªânh) v·ªõi c√°c `corpora` c·ª• th·ªÉ ƒë·ªÉ c√≥ `performance` t·ªët h∆°n.

### üìö Ph∆∞∆°ng ph√°p Word Embedding

#### Ph∆∞∆°ng ph√°p C·ªï ƒëi·ªÉn (`Classical Methods`)

* **word2vec** (Google, 2013):
    * **Continuous bag-of-words (CBOW)**: `model` h·ªçc c√°ch **d·ª± ƒëo√°n** t·ª´ trung t√¢m (`center word`) cho tr∆∞·ªõc c√°c `context words` (t·ª´ ng·ªØ c·∫£nh).
    * **Continuous skip-gram / Skip-gram with negative sampling (SGNS)**: `model` h·ªçc c√°ch **d·ª± ƒëo√°n** c√°c t·ª´ xung quanh (`surrounding words`) cho tr∆∞·ªõc m·ªôt t·ª´ `input`.

* **Global Vectors (GloVe)** (Stanford, 2014): Ph√¢n t√≠ch `logarithm` c·ªßa **word co-occurrence matrix** (ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán t·ª´) c·ªßa `corpus`, t∆∞∆°ng t·ª± nh∆∞ `count matrix` b·∫°n ƒë√£ s·ª≠ d·ª•ng tr∆∞·ªõc ƒë√¢y.
* **fastText** (Facebook, 2016): D·ª±a tr√™n `skip-gram model` v√† t√≠nh ƒë·∫øn c·∫•u tr√∫c c·ªßa t·ª´ b·∫±ng c√°ch bi·ªÉu di·ªÖn c√°c t·ª´ d∆∞·ªõi d·∫°ng `n-gram` c·ªßa k√Ω t·ª±. N√≥ h·ªó tr·ª£ c√°c t·ª´ **out-of-vocabulary (OOV)**.

#### Deep Learning, Contextual Embeddings

Trong c√°c `models` ti√™n ti·∫øn h∆°n n√†y, c√°c t·ª´ c√≥ c√°c `embeddings` kh√°c nhau t√πy thu·ªôc v√†o **context** (ng·ªØ c·∫£nh) c·ªßa ch√∫ng. B·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng c√°c `pre-trained embeddings` (embeddings ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc) cho c√°c `models` sau:

* **BERT** (Google, 2018)
* **ELMo** (Allen Institute for AI, 2018)
* **GPT-2** (OpenAI, 2018)

---
### **Continuous Bag-of-Words Model**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác tri·ªÉn khai **continuous bag-of-words model** (**CBOW**) ƒë·ªÉ t·∫°o `word embeddings` trong `natural language processing`.

#### Qu√° tr√¨nh t·ªïng th·ªÉ c·ªßa Word Embeddings

* **Word embeddings** ƒë∆∞·ª£c t·∫°o ra th√¥ng qua m·ªôt `machine learning model` h·ªçc t·ª´ m·ªôt `corpus` (kho ng·ªØ li·ªáu).
* **Continuous bag-of-words model** d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu (**center word**) d·ª±a tr√™n c√°c **context words** (t·ª´ ng·ªØ c·∫£nh) xung quanh n√≥.

#### T·∫°o D·ªØ li·ªáu Hu·∫•n luy·ªán

* **Context words** ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† c√°c t·ª´ bao quanh m·ªôt **center word**, v·ªõi m·ªôt **hyperparameter $C$** x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng `context words` (b√°n k√≠nh c·ª≠a s·ªï ng·ªØ c·∫£nh).
* `Model` s·ª≠ d·ª•ng **sliding windows** (c·ª≠a s·ªï tr∆∞·ª£t) ƒë·ªÉ t·∫°o c√°c `training examples` (v√≠ d·ª• hu·∫•n luy·ªán), trong ƒë√≥ `context words` l√† `inputs` v√† `center word` l√† `target` (m·ª•c ti√™u) ƒë·ªÉ d·ª± ƒëo√°n.

#### Ki·∫øn tr√∫c M√¥ h√¨nh v√† H·ªçc t·∫≠p

* **Model architecture** bao g·ªìm `context words` l√† `inputs` v√† `center words` l√† `outputs`.
* Khi `model` h·ªçc, n√≥ t·∫°o ra `word embeddings` nh∆∞ m·ªôt s·∫£n ph·∫©m ph·ª• c·ªßa `prediction task` (nhi·ªám v·ª• d·ª± ƒëo√°n), n·∫Øm b·∫Øt ƒë∆∞·ª£c **semantic relationships** (m·ªëi quan h·ªá ng·ªØ nghƒ©a) gi·ªØa c√°c t·ª´.

> ƒê·ªÉ t·∫°o **word embeddings**, b·∫°n c·∫ßn m·ªôt `corpus` v√† m·ªôt `learning algorithm` (thu·∫≠t to√°n h·ªçc t·∫≠p). S·∫£n ph·∫©m ph·ª• c·ªßa `task` n√†y s·∫Ω l√† m·ªôt t·∫≠p h·ª£p c√°c `word embeddings`. Trong tr∆∞·ªùng h·ª£p c·ªßa **continuous bag-of-words model** (**CBOW**), `objective` (m·ª•c ti√™u) c·ªßa `task` l√† **d·ª± ƒëo√°n m·ªôt t·ª´ b·ªã thi·∫øu** d·ª±a tr√™n c√°c t·ª´ xung quanh n√≥.

![06_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/06_Continuous_Bag-of-Words_Model.png)

> D∆∞·ªõi ƒë√¢y l√† m·ªôt **visualization** (h√¨nh ·∫£nh tr·ª±c quan) cho b·∫°n th·∫•y `model` ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o.

![07_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/07_Continuous_Bag-of-Words_Model.png)

> Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, **window size** (k√≠ch th∆∞·ªõc c·ª≠a s·ªï) trong h√¨nh ·∫£nh ph√≠a tr√™n l√† 5. **Context size** (k√≠ch th∆∞·ªõc ng·ªØ c·∫£nh), $C$, l√† 2. $C$ th∆∞·ªùng cho b·∫°n bi·∫øt c√≥ bao nhi√™u t·ª´ tr∆∞·ªõc ho·∫∑c sau **center word** (t·ª´ trung t√¢m) m√† `model` s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ ƒë∆∞a ra **prediction** (d·ª± ƒëo√°n).

> D∆∞·ªõi ƒë√¢y l√† m·ªôt **visualization** kh√°c cho th·∫•y t·ªïng quan v·ªÅ `model`.

![08_Continuous_Bag-of-Words_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/08_Continuous_Bag-of-Words_Model.png)

---
### **Cleaning and Tokenization**
---

N·ªôi dung t·∫≠p trung v√†o c√°c quy tr√¨nh **cleaning** (l√†m s·∫°ch) v√† **tokenization** (t·∫°o token) trong `natural language processing` (`NLP`).

#### Cleaning v√† Tokenization

- C√°c t·ª´ n√™n ƒë∆∞·ª£c x·ª≠ l√Ω d∆∞·ªõi d·∫°ng **case insensitive** (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng), nghƒ©a l√† ch√∫ng n√™n ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh m·ªôt ƒë·ªãnh d·∫°ng duy nh·∫•t (ch·ªØ th∆∞·ªùng ho·∫∑c ch·ªØ hoa) ƒë·ªÉ ƒë·ªìng nh·∫•t.
- **Punctuation** (D·∫•u c√¢u) c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω c·∫©n th·∫≠n; d·∫•u c√¢u g√¢y ng·∫Øt qu√£ng c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu th·ªã b·∫±ng m·ªôt `special word` (t·ª´ ƒë·∫∑c bi·ªát) duy nh·∫•t, trong khi d·∫•u c√¢u kh√¥ng g√¢y ng·∫Øt qu√£ng c√≥ th·ªÉ b·ªã b·ªè qua.

#### X·ª≠ l√Ω S·ªë v√† K√Ω t·ª± ƒê·∫∑c bi·ªát

- **Numbers** (S·ªë) c√≥ th·ªÉ b·ªã b·ªè ƒëi n·∫øu ch√∫ng kh√¥ng quan tr·ªçng, nh∆∞ng c√°c s·ªë quan tr·ªçng n√™n ƒë∆∞·ª£c gi·ªØ l·∫°i ho·∫∑c thay th·∫ø b·∫±ng m·ªôt `special token` nh∆∞ **\<NUMBER\>**.
- C√°c **Special characters** (K√Ω t·ª± ƒë·∫∑c bi·ªát), ch·∫≥ng h·∫°n nh∆∞ k√Ω hi·ªáu to√°n h·ªçc v√† `emojis`, n√™n ƒë∆∞·ª£c qu·∫£n l√Ω d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan c·ªßa ch√∫ng v·ªõi `model`.

### V√≠ d·ª• Th·ª±c h√†nh

M·ªôt `Python example` minh h·ªça c√°ch `clean` m·ªôt `corpus` b·∫±ng c√°ch g·ªôp `punctuation` v√† **tokenizing** vƒÉn b·∫£n b·∫±ng c√°ch s·ª≠ d·ª•ng `NLTK library`, t·∫°o ra m·ªôt `array of tokens` s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch th√™m.

ƒêi·ªÅu n√†y t·∫°o ti·ªÅn ƒë·ªÅ cho ch·ªß ƒë·ªÅ ti·∫øp theo v·ªÅ **continuous bag-of-words model**.

> Tr∆∞·ªõc khi tri·ªÉn khai b·∫•t k·ª≥ thu·∫≠t to√°n `natural language processing` (`NLP`) n√†o, b·∫°n c√≥ th·ªÉ mu·ªën `clean` (l√†m s·∫°ch) d·ªØ li·ªáu v√† `tokenize` (t·∫°o token) n√≥. D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i ƒëi·ªÅu c·∫ßn l∆∞u √Ω khi x·ª≠ l√Ω `data` c·ªßa b·∫°n.

![09_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/09_Cleaning_and_Tokenization.png)

> B·∫°n c√≥ th·ªÉ `clean data` b·∫±ng `Python` nh∆∞ sau:

![10_Cleaning_and_Tokenization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/10_Cleaning_and_Tokenization.png)

> B·∫°n c√≥ th·ªÉ th√™m bao nhi√™u ƒëi·ªÅu ki·ªán t√πy th√≠ch v√†o c√°c d√≤ng t∆∞∆°ng ·ª©ng v·ªõi h√¨nh ch·ªØ nh·∫≠t m√†u xanh l√° c√¢y ph√≠a tr√™n.

---
### **Sliding Window of Words in Python**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác tr√≠ch xu·∫•t **center words** (t·ª´ trung t√¢m) v√† **context words** (t·ª´ ng·ªØ c·∫£nh) ƒë·ªÉ `training` (hu·∫•n luy·ªán) **continuous bag-of-words model** trong `natural language processing`.

#### Tr√≠ch xu·∫•t Center v√† Context Words

* Qu√° tr√¨nh b·∫Øt ƒë·∫ßu v·ªõi m·ªôt `cleaned and tokenized corpus` (kho ng·ªØ li·ªáu ƒë√£ l√†m s·∫°ch v√† t·∫°o token), ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt `array` (m·∫£ng) c√°c t·ª´.
* M·ªôt `function` (h√†m) g·ªçi l√† `get_windows` ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ƒë·ªÉ tr√≠ch xu·∫•t `center words` v√† `context words` c·ªßa ch√∫ng d·ª±a tr√™n m·ªôt `context size` ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.

#### Tri·ªÉn khai Function

* `Function` nh·∫≠n m·ªôt `array of words` v√† m·ªôt **context size ($C$)**, c√°i m√† x√°c ƒë·ªãnh c√≥ bao nhi√™u t·ª´ s·∫Ω ƒë∆∞·ª£c xem x√©t ·ªü m·ªói b√™n c·ªßa `center word`.
* N√≥ `initialize` (kh·ªüi t·∫°o) m·ªôt v√≤ng l·∫∑p ƒë·ªÉ l·∫∑p qua `array`, tr√≠ch xu·∫•t `center words` v√† c√°c `context words` t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng.

#### S·ª≠ d·ª•ng Function

* `Function` s·ª≠ d·ª•ng **yield keyword** ƒë·ªÉ tr·∫£ v·ªÅ c√°c gi√° tr·ªã m·ªôt c√°ch **iteratively** (l·∫∑p l·∫°i), cho ph√©p **data generation** (t·∫°o d·ªØ li·ªáu) hi·ªáu qu·∫£.
* M·ªôt v√≤ng l·∫∑p ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hi·ªÉn th·ªã `context` v√† `center words`, c√°i m√† c·∫ßn thi·∫øt cho **continuous bag-of-words model**.

T·ªïng quan, b√†i gi·∫£ng n√†y cung c·∫•p m·ªôt c√°ch ti·∫øp c·∫≠n th·ª±c t·∫ø ƒë·ªÉ chu·∫©n b·ªã `data` cho `training word embeddings` b·∫±ng `Python`.

![11_Sliding_Window_of_Words_in_Python](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/11_Sliding_Window_of_Words_in_Python.png)

> `Code` ph√≠a tr√™n cho th·∫•y m·ªôt `function` (h√†m) nh·∫≠n hai `parameters` (tham s·ªë).

* `Words`: m·ªôt `list` (danh s√°ch) c√°c t·ª´.
* $C$: **context size** (k√≠ch th∆∞·ªõc ng·ªØ c·∫£nh).

> Ch√∫ng ta b·∫Øt ƒë·∫ßu b·∫±ng c√°ch ƒë·∫∑t $i$ b·∫±ng $C$. Sau ƒë√≥, ch√∫ng ta t√°ch **center\_word** (t·ª´ trung t√¢m) v√† **context\_words** (c√°c t·ª´ ng·ªØ c·∫£nh). Ch√∫ng ta sau ƒë√≥ **yield** (tr·∫£ v·ªÅ) c√°c gi√° tr·ªã n√†y v√† **increment** (tƒÉng) $i$ l√™n.


---
### **Transforming Words into Vectors**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác chu·∫©n b·ªã `data` (d·ªØ li·ªáu) cho **continuous bag-of-words model** (**CBOW**) trong `natural language processing`.

#### Chu·∫©n b·ªã D·ªØ li·ªáu cho M√¥ h√¨nh CBOW

* **Context v√† Central Words**: Qu√° tr√¨nh b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác x√°c ƒë·ªãnh **context words** v√† **central word** (t·ª´ trung t√¢m) t·ª´ m·ªôt `sliding window` (c·ª≠a s·ªï tr∆∞·ª£t) tr√™n `corpus` (kho ng·ªØ li·ªáu).
* **Vocabulary Creation**: M·ªôt **vocabulary** (t·ª´ v·ª±ng) ƒë∆∞·ª£c h√¨nh th√†nh t·ª´ c√°c t·ª´ ƒë·ªôc nh·∫•t (`unique words`) trong `corpus`, sau ƒë√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o **one-hot vectors** cho c√°c `central words`.

#### Bi·ªÉu di·ªÖn Vector

* **One-Hot Encoding**: M·ªói t·ª´ trong `vocabulary` ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **one-hot vector**, trong ƒë√≥ '1' cho bi·∫øt s·ª± hi·ªán di·ªán c·ªßa m·ªôt t·ª´ v√† '0' cho bi·∫øt s·ª± v·∫Øng m·∫∑t.
* **Averaging Context Vectors**: ƒê·ªëi v·ªõi **context words**, m·ªôt `vector` duy nh·∫•t ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch **averaging** (t√≠nh trung b√¨nh) c√°c `one-hot vectors` c·ªßa m·ªói `context word`, cung c·∫•p m·ªôt bi·ªÉu di·ªÖn cho `context`.

#### Chu·∫©n b·ªã D·ªØ li·ªáu Hu·∫•n luy·ªán

* **Final Vector Representation**: C√°c `final vectors` (v√©c-t∆° cu·ªëi c√πng) cho c·∫£ `central words` v√† `context words` ƒë∆∞·ª£c chu·∫©n b·ªã ƒë·ªÉ `training` (hu·∫•n luy·ªán) **CBOW model**.
* **Transition to Model Learning**: V·ªõi `data` ƒë√£ ƒë∆∞·ª£c bi·ªÉu di·ªÖn ƒë·∫ßy ƒë·ªß, b∆∞·ªõc ti·∫øp theo l√† t√¨m hi·ªÉu v·ªÅ **architecture** (ki·∫øn tr√∫c) c·ªßa **CBOW model** v√† √°p d·ª•ng c√°c k·ªπ nƒÉng v√†o c√°c b√†i t·∫≠p s·∫Øp t·ªõi.

> ƒê·ªÉ bi·∫øn ƒë·ªïi c√°c **context vectors** (v√©c-t∆° ng·ªØ c·∫£nh) th√†nh m·ªôt **single vector** (v√©c-t∆° ƒë∆°n l·∫ª), b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√¥ng th·ª©c/ph∆∞∆°ng ph√°p sau:

![12_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/12_Transforming_Words_into_Vectors.png)

> Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, ch√∫ng ta b·∫Øt ƒë·∫ßu v·ªõi c√°c **one-hot vectors** cho c√°c t·ª´ ng·ªØ c·∫£nh v√† bi·∫øn ƒë·ªïi ch√∫ng th√†nh m·ªôt **single vector** b·∫±ng c√°ch l·∫•y **average** (trung b√¨nh). K·∫øt qu·∫£ l√† b·∫°n nh·∫≠n ƒë∆∞·ª£c c√°c `vectors` sau ƒë√¢y m√† b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng cho vi·ªác **training** (hu·∫•n luy·ªán) c·ªßa m√¨nh.

![13_Transforming_Words_into_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/13_Transforming_Words_into_Vectors.png)

---
### **Architecture of the CBOW Model**
---

N·ªôi dung t·∫≠p trung v√†o **architecture** (ki·∫øn tr√∫c) c·ªßa **Continuous Bag of Words** (**CBOW**) `model` ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `natural language processing`.

#### T·ªïng quan Ki·∫øn tr√∫c

* **CBOW model** bao g·ªìm m·ªôt **shallow dense neural network** (m·∫°ng n∆°-ron d√†y ƒë·∫∑c n√¥ng) v·ªõi m·ªôt `input layer`, m·ªôt `hidden layer` (l·ªõp ·∫©n), v√† m·ªôt `output layer`.
* `Input` l√† m·ªôt `vector` c·ªßa **context words** (c√°c t·ª´ ng·ªØ c·∫£nh), trong khi `output` l√† **center word** (t·ª´ trung t√¢m) ƒë∆∞·ª£c d·ª± ƒëo√°n, c·∫£ hai ƒë·ªÅu c√≥ k√≠ch th∆∞·ªõc theo **vocabulary** ($V$).

#### Chi ti·∫øt c√°c L·ªõp

* K√≠ch th∆∞·ªõc c·ªßa `hidden layer` ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi **dimension** (chi·ªÅu) ƒë√£ ch·ªçn c·ªßa **word embeddings** ($N$), th∆∞·ªùng dao ƒë·ªông t·ª´ $100$ ƒë·∫øn $1,000$.
* `Network` l√† **fully connected** (k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß), v·ªõi c√°c **weight matrices** ($W_1$ v√† $W_2$) v√† **bias vectors** ($b_1$ v√† $b_2$) m√† `model` h·ªçc trong qu√° tr√¨nh `training`.

#### H√†m K√≠ch ho·∫°t (Activation Functions)

* `Hidden layer` s·ª≠ d·ª•ng **Rectified Linear Units** (**ReLU**) `activation function` (h√†m k√≠ch ho·∫°t).
* `Output layer` s·ª≠ d·ª•ng **softmax function** ƒë·ªÉ ƒë∆∞a ra **predictions** (d·ª± ƒëo√°n).

B·∫£n t√≥m t·∫Øt n√†y cung c·∫•p s·ª± hi·ªÉu bi·∫øt ng·∫Øn g·ªçn v·ªÅ c·∫•u tr√∫c v√† c√°c th√†nh ph·∫ßn c·ªßa `CBOW model`.

> `Architecture` (Ki·∫øn tr√∫c) cho **CBOW model** c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ sau:

![14_Architecture_of_the_CBOW_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/14_Architecture_of_the_CBOW_Model.png)

> B·∫°n c√≥ m·ªôt `input`, $X$, l√† gi√° tr·ªã trung b√¨nh (`average`) c·ªßa t·∫•t c·∫£ c√°c **context vectors** (v√©c-t∆° ng·ªØ c·∫£nh). Sau ƒë√≥, b·∫°n nh√¢n n√≥ v·ªõi $W_1$ v√† c·ªông th√™m $b_1$. K·∫øt qu·∫£ n√†y ƒëi qua m·ªôt **ReLU function** ƒë·ªÉ t·∫°o ra **hidden layer** (l·ªõp ·∫©n) c·ªßa b·∫°n. L·ªõp ƒë√≥ sau ƒë√≥ ƒë∆∞·ª£c nh√¢n v·ªõi $W_2$ v√† b·∫°n c·ªông th√™m $b_2$. K·∫øt qu·∫£ n√†y ƒëi qua m·ªôt **softmax** ƒë·ªÉ cung c·∫•p cho b·∫°n m·ªôt **distribution** (ph√¢n ph·ªëi) tr√™n $V$ (k√≠ch th∆∞·ªõc `vocabulary`). B·∫°n ch·ªçn `vocabulary word` t∆∞∆°ng ·ª©ng v·ªõi **arg-max** c·ªßa `output`.

---
### **Architecture of the CBOW Model: Dimensions**
---

N·ªôi dung t·∫≠p trung v√†o vi·ªác hi·ªÉu **dimensions** (chi·ªÅu) c·ªßa c√°c l·ªõp trong m·ªôt `neural network model`, c·ª• th·ªÉ l√† **continuous bag of words** (**CBOW**) `model`.

#### Ki·∫øn tr√∫c Neural Network

* `Input layer` ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng m·ªôt **column vector** ($x$) v·ªõi c√°c s·ªë kh√¥ng (`zeros`), trong ƒë√≥ $V$ l√† **vocabulary size** (k√≠ch th∆∞·ªõc t·ª´ v·ª±ng). ($x$ c√≥ dimension $V \times 1$).
* `Hidden layer` ($h$) ƒë∆∞·ª£c t√≠nh b·∫±ng **weighted sum** ($W_1 x + b_1$), trong ƒë√≥ $W_1$ l√† **weight matrix** (ma tr·∫≠n tr·ªçng s·ªë) (dimension $N \times V$) v√† $b_1$ l√† **bias vector** (v√©c-t∆° ƒë·ªô l·ªách) (dimension $N \times 1$). ($N$ l√† embedding dimension).

#### T√≠nh to√°n Output

* C√°c gi√° tr·ªã **output layer** ƒë∆∞·ª£c suy ra t·ª´ `hidden layer` ($h$) b·∫±ng c√°ch s·ª≠ d·ª•ng ($W_2 h + b_2$), trong ƒë√≥ $W_2$ l√† **weight matrix** cho `output layer` (dimension $V \times N$) v√† $b_2$ l√† **bias vector** t∆∞∆°ng ·ª©ng (dimension $V \times 1$).
* `Output` cu·ªëi c√πng ($\hat{y}$) ƒë∆∞·ª£c thu ƒë∆∞·ª£c b·∫±ng c√°ch √°p d·ª•ng **softmax activation function** (h√†m k√≠ch ho·∫°t softmax) cho c√°c gi√° tr·ªã `output layer`. ($\hat{y}$ c√≥ dimension $V \times 1$).

#### X·ª≠ l√Ω c√°c Lo·∫°i Vector

* N·∫øu s·ª≠ d·ª•ng **row vectors** (v√©c-t∆° h√†ng) thay v√¨ **column vectors** (v√©c-t∆° c·ªôt), c√°c ph√©p t√≠nh `matrix` ph·∫£i ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh t∆∞∆°ng ·ª©ng, ch·∫≥ng h·∫°n nh∆∞ **transposing matrices** (chuy·ªÉn v·ªã ma tr·∫≠n) trong qu√° tr√¨nh nh√¢n.
* Hi·ªÉu r√µ c√°c `dimensions` n√†y l√† r·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh c√°c l·ªói **dimension mismatch errors** (l·ªói kh√¥ng kh·ªõp chi·ªÅu) trong c√°c `programming assignments`.

> C√°c ph∆∞∆°ng tr√¨nh cho `model` tr∆∞·ªõc l√†:

$$z_1 = W_1 x + b_1$$

$$h = \text{ReLU}(z_1)$$

$$z_2 = W_2 h + b_2$$

$$\hat{y} = \text{softmax}(z_2)$$

> ·ªû ƒë√¢y, b·∫°n c√≥ th·ªÉ th·∫•y c√°c **dimensions** (chi·ªÅu):

![15_Architecture_of_the_CBOW_Model_Dimensions](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/15_Architecture_of_the_CBOW_Model_Dimensions.png)

> H√£y ƒë·∫£m b·∫£o r·∫±ng b·∫°n xem k·ªπ c√°c ph√©p **matrix multiplications** (nh√¢n ma tr·∫≠n) v√† hi·ªÉu t·∫°i sao c√°c **dimensions** (chi·ªÅu) l·∫°i h·ª£p l√Ω.

---
### **Architecture of the CBOW Model: Dimensions 2**
---

N·ªôi dung t·∫≠p trung v√†o kh√°i ni·ªám **batch processing** (x·ª≠ l√Ω theo l√¥) trong `Continuous Bag of Words` (**CBOW**) `model` ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `neural networks`.

#### Batch Processing trong CBOW

* Thay v√¨ cung c·∫•p c√°c `individual examples` (v√≠ d·ª• ri√™ng l·∫ª), nhi·ªÅu `input examples` c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªìng th·ªùi, ƒëi·ªÅu n√†y gi√∫p tƒÉng t·ªëc qu√° tr√¨nh h·ªçc t·∫≠p.
* **Batch size** ($M$) l√† m·ªôt **hyperparameter** (si√™u tham s·ªë) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong qu√° tr√¨nh `training`, cho ph√©p h√¨nh th√†nh m·ªôt **matrix** ($X$) t·ª´ c√°c `input vectors` n√†y.

#### C√°c Ph√©p to√°n Matrix

* C√°c gi√° tr·ªã **hidden layer** ($H$) ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch √°p d·ª•ng **ReLU activation function** cho `weighted input matrix` ($Z_1$), c√°i m√† bao g·ªìm m·ªôt **bias matrix** ($B_1$).
* **Output matrix** ($\hat{Y}$) ƒë∆∞·ª£c suy ra t·ª´ `hidden layer` v√† bao g·ªìm m·ªôt **replicated bias matrix** ($B_2$), bi·∫øn ƒë·ªïi c√°c `input vectors` th√†nh c√°c `output vectors` t∆∞∆°ng ·ª©ng.

#### H√†m K√≠ch ho·∫°t

* B√†i gi·∫£ng g·ª£i √Ω v·ªÅ vi·ªác gi·ªõi thi·ªáu c√°c **activation functions** (h√†m k√≠ch ho·∫°t) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `CBOW model`, cho th·∫•y ng∆∞·ªùi h·ªçc ƒëang ti·∫øn t·ªõi vi·ªác x√¢y d·ª±ng m·ªôt `model` ch·ª©c nƒÉng.

> Khi x·ª≠ l√Ω **batch input** (ƒë·∫ßu v√†o theo l√¥), b·∫°n c√≥ th·ªÉ **stack** (x·∫øp ch·ªìng) c√°c v√≠ d·ª• th√†nh c√°c **columns** (c·ªôt). Sau ƒë√≥, b·∫°n c√≥ th·ªÉ ti·∫øn h√†nh nh√¢n c√°c **matrices** (ma tr·∫≠n) nh∆∞ sau:

![16_Architecture_of_the_CBOW_Model_Dimensions_2](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/16_Architecture_of_the_CBOW_Model_Dimensions_2.png)

> Trong s∆° ƒë·ªì ph√≠a tr√™n, b·∫°n c√≥ th·ªÉ th·∫•y c√°c **dimensions** (chi·ªÅu) c·ªßa m·ªói **matrix**. L∆∞u √Ω r·∫±ng $\hat{Y}$ c·ªßa b·∫°n c√≥ **dimension** $V$ nh√¢n $M$. M·ªói **column** l√† **prediction** (d·ª± ƒëo√°n) c·ªßa `column` t∆∞∆°ng ·ª©ng v·ªõi c√°c **context words**. V√¨ v·∫≠y, `column` ƒë·∫ßu ti√™n trong $\hat{Y}$ l√† **prediction** t∆∞∆°ng ·ª©ng v·ªõi `column` ƒë·∫ßu ti√™n c·ªßa $X$.

---
### **Architecture of the CBOW Model: Activation Functions**
---

N·ªôi dung n√†y t·∫≠p trung v√†o hai **activation functions** (h√†m k√≠ch ho·∫°t) quan tr·ªçng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong tr√≠ tu·ªá nh√¢n t·∫°o: **Rectified Linear Unit** (`ReLU`) v√† **Softmax function**.

#### ‚öôÔ∏è ReLU Function

* **ReLU** l√† m·ªôt **activation function** ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i, n√≥ ch·ªâ k√≠ch ho·∫°t m·ªôt **neuron** khi `weighted input` l√† d∆∞∆°ng, thi·∫øt l·∫≠p t·∫•t c·∫£ c√°c `inputs` √¢m v·ªÅ 0.
* C√¥ng th·ª©c c·ªßa `ReLU` l√†:

$$f(z) = \max(0, z)$$

* V√≠ d·ª•, n·∫øu `input vector` ch·ª©a c√°c gi√° tr·ªã √¢m, nh·ªØng gi√° tr·ªã ƒë√≥ s·∫Ω tr·ªü th√†nh s·ªë 0 trong `output`, trong khi c√°c gi√° tr·ªã d∆∞∆°ng v·∫´n gi·ªØ nguy√™n.

#### üìä Softmax Function

* **Softmax function** nh·∫≠n m·ªôt `vector of real numbers` (v√©c-t∆° c√°c s·ªë th·ª±c) l√†m `input` v√† `output` ra m·ªôt **probability distribution** (ph√¢n ph·ªëi x√°c su·∫•t), trong ƒë√≥ t·ªïng c√°c gi√° tr·ªã b·∫±ng m·ªôt.

* C√¥ng th·ª©c cho `Softmax` (ƒë·ªëi v·ªõi ph·∫ßn t·ª≠ th·ª© $i$ trong vector $z$) l√†:

$$\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

* N√≥ ƒë·∫∑c bi·ªát h·ªØu √≠ch trong c√°c v·∫•n ƒë·ªÅ **multi-class classification** (ph√¢n lo·∫°i ƒëa l·ªõp), v√¨ n√≥ cung c·∫•p `probabilities` c·ªßa m·ªói `class`, cho ph√©p gi·∫£i th√≠ch `model's predictions`.

T√≥m l·∫°i, `ReLU` gi√∫p qu·∫£n l√Ω k√≠ch ho·∫°t **neuron** ·ªü `hidden layer`, trong khi `Softmax` l√† c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o **probabilities** ·ªü `output layer` trong c√°c `classification tasks`.

> ReLU function

**ReLU function** (`Rectified Linear Unit`), l√† m·ªôt trong nh·ªØng `activation functions` ph·ªï bi·∫øn nh·∫•t. Khi b·∫°n ƒë∆∞a m·ªôt `vector`, c·ª• th·ªÉ l√† $x$, v√†o m·ªôt `ReLU function`. B·∫°n k·∫øt th√∫c v·ªõi ph√©p t√≠nh:

$$x = \max(0, x)$$

ƒê√¢y l√† h√¨nh v·∫Ω minh h·ªça `ReLU`.

![17_Architecture_of_the_CBOW_Model_Dimensions_AF](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/17_Architecture_of_the_CBOW_Model_Dimensions_AF.png)

> Softmax function

**Softmax function** nh·∫≠n m·ªôt `vector` v√† bi·∫øn ƒë·ªïi n√≥ th√†nh m·ªôt **probability distribution** (ph√¢n ph·ªëi x√°c su·∫•t). V√≠ d·ª•, cho tr∆∞·ªõc `vector` $z$ sau, b·∫°n c√≥ th·ªÉ bi·∫øn ƒë·ªïi n√≥ th√†nh m·ªôt **probability distribution** nh∆∞ sau.

![18_Architecture_of_the_CBOW_Model_Dimensions_AF](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/18_Architecture_of_the_CBOW_Model_Dimensions_AF.png)

Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, b·∫°n c√≥ th·ªÉ t√≠nh `probability` ($\hat{y}_i$) c·ªßa ph·∫ßn t·ª≠ $i$ nh∆∞ sau:

$$\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^{V} e^{z_j}}$$

Trong ƒë√≥ $V$ l√† k√≠ch th∆∞·ªõc c·ªßa `vector` $z$ (t·ª©c l√† k√≠ch th∆∞·ªõc `vocabulary`).

---
### **Training a CBOW Model: Cost Function**
---

N·ªôi dung n√†y t·∫≠p trung v√†o **cost function** (h√†m chi ph√≠) cho **Softmax** trong `machine learning`, ƒë·∫∑c bi·ªát trong b·ªëi c·∫£nh d·ª± ƒëo√°n t·ª´ b·∫±ng **continuous bag of words model** (**CBOW**).

#### T·ªïng quan Cost Function

* **Cost function** r·∫•t c·∫ßn thi·∫øt ƒë·ªÉ d·ª± ƒëo√°n m·ªôt trong nh·ªØng t·ª´ c√≥ th·ªÉ c√≥ b·∫±ng c√°ch t·ªëi thi·ªÉu h√≥a m·ªôt chi ph√≠ c·ª• th·ªÉ.
* M·ªôt **training example** (v√≠ d·ª• hu·∫•n luy·ªán) ƒë∆°n l·∫ª bao g·ªìm m·ªôt `input`, m·ªôt **true target** (m·ª•c ti√™u th·ª±c t·∫ø), v√† gi√° tr·ªã d·ª± ƒëo√°n c·ªßa `model`.

#### Loss Function v√† Tham s·ªë

* **Loss function** (h√†m m·∫•t m√°t) ƒëo l∆∞·ªùng sai s·ªë gi·ªØa `prediction` v√† **true value** cho m·ªôt `training example`.
* Trong **CBOW model**, c√°c **parameters** (tham s·ªë) ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh bao g·ªìm **weight matrices** ($W_1, W_2$) v√† **bias factors** ($b_1, b_2$).

#### Cross Entropy Loss

* **Cross entropy loss** (m·∫•t m√°t entropy ch√©o) th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng v·ªõi c√°c **classification models** v√† c√≥ li√™n quan ƒë·∫øn l·ªõp `output Softmax`.
* C√¥ng th·ª©c cho **cross entropy loss** ($J$) li√™n quan ƒë·∫øn t·ªïng √¢m c·ªßa t√≠ch gi·ªØa **true value** ($y_i$) v√† `log` c·ªßa **predicted value** ($\hat{y}_i$):
$$J = - \sum_{i=1}^{V} y_i \log(\hat{y}_i)$$

#### V√≠ d·ª• D·ª± ƒëo√°n

* **Loss function** th∆∞·ªüng cho c√°c `predictions` ƒë√∫ng v√† ph·∫°t c√°c `predictions` kh√¥ng ch√≠nh x√°c (cho th·∫•y `loss` tƒÉng l√™n v·ªõi c√°c `predictions` kh√¥ng ch√≠nh x√°c), v·ªõi √Ω nghƒ©a ƒë·ªëi v·ªõi **model performance**.

![19_Training_a_CBOW_Model_Cost_Function](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/19_Training_a_CBOW_Model_Cost_Function.png)

> M·ª©c **chi ph√≠ (cost)** l√† $4.61$ trong v√≠ d·ª• tr√™n l√† do **m√¥ h√¨nh ƒë√£ d·ª± ƒëo√°n m·ªôt x√°c su·∫•t r·∫•t th·∫•p cho t·ª´ ƒë√∫ng (true target)**.

> Gi√° tr·ªã $4.61$ kh√¥ng ph·∫£i l√† ng·∫´u nhi√™n; n√≥ th·ªÉ hi·ªán m·ªëi quan h·ªá ngh·ªãch ƒë·∫£o gi·ªØa chi ph√≠ v√† x√°c su·∫•t d·ª± ƒëo√°n th√¥ng qua h√†m $\log$ t·ª± nhi√™n ($\ln$).

D∆∞·ªõi ƒë√¢y l√† l√Ω do chi ti·∫øt:

#### 1. C√¥ng th·ª©c ƒê∆°n gi·∫£n h√≥a

C√¥ng th·ª©c **Cross-Entropy Loss** cho m·ªôt v√≠ d·ª• hu·∫•n luy·ªán ƒë∆°n l·∫ª, n∆°i **true target ($y$)** l√† m·ªôt **one-hot vector** (ch·ªâ c√≥ 1 ·ªü v·ªã tr√≠ t·ª´ ƒë√∫ng $k^*$) ƒë∆∞·ª£c ƒë∆°n gi·∫£n h√≥a th√†nh:

$$J = - \sum_{k=1}^{V} y_k \log(\hat{y}_k) = - \log(\hat{y}_{k^*})$$

Trong ƒë√≥:
* $V$ l√† k√≠ch th∆∞·ªõc t·ª´ v·ª±ng.
* $y_{k^*}$ l√† $1$ (x√°c su·∫•t $100\%$ r·∫±ng t·ª´ $k^*$ l√† ƒë√∫ng).
* $\hat{y}_{k^*}$ l√† x√°c su·∫•t m√† m√¥ h√¨nh d·ª± ƒëo√°n cho t·ª´ ƒë√∫ng $k^*$.

#### 2. T√≠nh to√°n X√°c su·∫•t D·ª± ƒëo√°n

N·∫øu chi ph√≠ ƒë∆∞·ª£c t√≠nh l√† $J = 4.61$ (s·ª≠ d·ª•ng $\log$ t·ª± nhi√™n, $\ln$, l√† ti√™u chu·∫©n):

$$4.61 = - \ln(\hat{y}_{k^*})$$

Ch√∫ng ta c√≥ th·ªÉ gi·∫£i ph∆∞∆°ng tr√¨nh n√†y ƒë·ªÉ t√¨m x√°c su·∫•t d·ª± ƒëo√°n $\hat{y}_{k^*}$:

$$\ln(\hat{y}_{k^*}) = -4.61$$
$$\hat{y}_{k^*} = e^{-4.61} \approx 0.010$$

#### K·∫øt lu·∫≠n

Gi√° tr·ªã $4.61$ cho th·∫•y **m√¥ h√¨nh ch·ªâ d·ª± ƒëo√°n x√°c su·∫•t kho·∫£ng $0.01$ (t·ª©c $1\%$) cho t·ª´ l·∫Ω ra ph·∫£i l√† ƒë√°p √°n ƒë√∫ng** trong v√≠ d·ª• n√†y. Chi ph√≠ cao (nh∆∞ $4.61$) l√† c√°ch **loss function** tr·ª´ng ph·∫°t m√¥ h√¨nh v√¨ ƒë√£ ƒë∆∞a ra m·ªôt d·ª± ƒëo√°n sai l·ªách cao (x√°c su·∫•t th·∫•p) cho k·∫øt qu·∫£ ƒë√∫ng.

---
### **Training a CBOW Model: Forward Propagation**
---

N·ªôi dung t·∫≠p trung v√†o qu√° tr√¨nh **forward propagation** (lan truy·ªÅn ti·∫øn) trong `Continuous Bag-of-Words` (**CBOW**) `model` ƒë∆∞·ª£c s·ª≠ d·ª•ng trong `neural networks`.

#### T·ªïng quan Forward Propagation

* **Forward propagation** bao g·ªìm vi·ªác truy·ªÅn c√°c gi√° tr·ªã `input` qua `neural network` t·ª´ `input` ƒë·∫øn `output`, t√≠nh to√°n c√°c gi√° tr·ªã ·ªü m·ªói l·ªõp.
* M·ªôt **batch of examples** (l√¥ v√≠ d·ª•) ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt `matrix`, v√† **output matrix** ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch lan truy·ªÅn `input` n√†y qua `network`.

#### T√≠nh to√°n Cost

* **Cost function** (h√†m chi ph√≠) l√† m·ªôt ph·∫ßn m·ªü r·ªông c·ªßa **loss function** (h√†m m·∫•t m√°t), ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëo l∆∞·ªùng sai s·ªë cho m·ªôt `batch of training examples`.
* **Cross-entropy cost** cho m·ªôt `batch` l√† **mean** (gi√° tr·ªã trung b√¨nh) c·ªßa c√°c **cross-entropy losses** ri√™ng l·∫ª cho m·ªói v√≠ d·ª•, cho ph√©p h√¨nh dung `cost` nh∆∞ l√† m·ªôt gi√° tr·ªã trung b√¨nh c·ªßa c√°c `losses`.

#### Qu√° tr√¨nh Optimization

* Sau khi t√≠nh to√°n `cost`, **back propagation** (lan truy·ªÅn ng∆∞·ª£c) v√† **gradient descent** (gi·∫£m ƒë·ªô d·ªëc) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°c `parameters` (tham s·ªë) c·ªßa `network` nh·∫±m c·∫£i thi·ªán `predictions`.
* C√°c b∆∞·ªõc ti·∫øp theo bao g·ªìm `training word vectors` b·∫±ng c√°ch s·ª≠ d·ª•ng **cost function** ƒë·ªÉ n√¢ng cao `model's performance`.

> Forward Propagation (Lan truy·ªÅn ti·∫øn)

> **Forward Propagation** ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:

$$Z_1 = W_1 X + B_1$$

$$H = \text{ReLU}(Z_1)$$

$$Z_2 = W_2 H + B_2$$

$$\hat{Y} = \text{softmax}(Z_2)$$

> Trong ƒë√≥ $X$ l√† ma tr·∫≠n `input` (ƒë·∫ßu v√†o theo l√¥), $W_1, W_2$ l√† ma tr·∫≠n `weights` (tr·ªçng s·ªë), $B_1, B_2$ l√† ma tr·∫≠n `bias` (ƒë·ªô l·ªách), $H$ l√† **hidden layer** (l·ªõp ·∫©n), v√† $\hat{Y}$ l√† ma tr·∫≠n d·ª± ƒëo√°n `output` (`predicted output matrix`).

> Trong h√¨nh ·∫£nh d∆∞·ªõi ƒë√¢y, b·∫°n b·∫Øt ƒë·∫ßu t·ª´ b√™n tr√°i v√† **forward propagate** (lan truy·ªÅn ti·∫øn) su·ªët t·ªõi b√™n ph·∫£i.

![20_Training_a_CBOW_Model_FP](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/20_Training_a_CBOW_Model_FP.png)

> Batch Cost Function (H√†m chi ph√≠ theo l√¥)

> ƒê·ªÉ t√≠nh **loss** (t·ªïn th·∫•t) c·ªßa m·ªôt **batch** (l√¥), b·∫°n ph·∫£i t√≠nh c√¥ng th·ª©c sau. C√¥ng th·ª©c n√†y l√† gi√° tr·ªã trung b√¨nh (`mean`) c·ªßa c√°c **Cross-Entropy losses** tr√™n $M$ v√≠ d·ª• trong `batch`:

$$J_{\text{batch}} = -\frac{1}{M}\sum_{i=1}^{M}\sum_{j=1}^{V}y_{j}^{(i)}\log\hat{y}_{j}^{(i)}$$

> Trong ƒë√≥:

* $M$: K√≠ch th∆∞·ªõc `batch`.
* $V$: K√≠ch th∆∞·ªõc `vocabulary`.
* $y_{j}^{(i)}$: Gi√° tr·ªã th·ª±c t·∫ø (`actual`) c·ªßa t·ª´ th·ª© $j$ trong v√≠ d·ª• th·ª© $i$.
* $\hat{y}_{j}^{(i)}$: Gi√° tr·ªã d·ª± ƒëo√°n (`predicted`) c·ªßa t·ª´ th·ª© $j$ trong v√≠ d·ª• th·ª© $i$.

> Cho ma tr·∫≠n **predicted center word** c·ªßa b·∫°n ($\hat{Y}$) v√† ma tr·∫≠n **actual center word** ($Y_{\text{true}}$), b·∫°n c√≥ th·ªÉ t√≠nh `loss`.

![21_Training_a_CBOW_Model_FP](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/21_Training_a_CBOW_Model_FP.png)

---
### **Training a CBOW Model: Backpropagation and Gradient Descent**
---

N·ªôi dung n√†y t·∫≠p trung v√†o c√°c k·ªπ thu·∫≠t ƒë·ªÉ t·ªëi thi·ªÉu h√≥a **cost** (chi ph√≠) trong `neural networks`, c·ª• th·ªÉ th√¥ng qua **backpropagation** v√† **gradient descent**.

#### Backpropagation (Lan truy·ªÅn ng∆∞·ª£c)

* **Backpropagation** l√† m·ªôt `algorithm` (thu·∫≠t to√°n) t√≠nh to√°n c√°c **partial derivatives** (ƒë·∫°o h√†m ri√™ng) c·ªßa `cost` ƒë·ªëi v·ªõi c√°c `weights` (tr·ªçng s·ªë) v√† `biases` (ƒë·ªô l·ªách) c·ªßa `neural network`.
* N√≥ s·ª≠ d·ª•ng **chain rule** (quy t·∫Øc chu·ªói) cho c√°c ƒë·∫°o h√†m, b·∫Øt ƒë·∫ßu t·ª´ `output layer` v√† t√≠nh to√°n ng∆∞·ª£c tr·ªü l·∫°i qua c√°c l·ªõp.

#### Gradient Descent (Gi·∫£m ƒë·ªô d·ªëc)

* **Gradient Descent** l√† m·ªôt ph∆∞∆°ng ph√°p ƒëi·ªÅu ch·ªânh c√°c `weights` v√† `biases` b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c **gradients** (ƒë·ªô d·ªëc) ƒë√£ t√≠nh to√°n ƒë·ªÉ t·ªëi thi·ªÉu h√≥a `cost`.
* **Learning rate** ($\alpha$) l√† m·ªôt **hyperparameter** (si√™u tham s·ªë) ki·ªÉm so√°t k√≠ch th∆∞·ªõc c·ªßa c√°c `updates` (c·∫≠p nh·∫≠t) ƒë·ªëi v·ªõi c√°c `weights` v√† `biases`.

#### C√¥ng th·ª©c c·∫≠p nh·∫≠t Tr·ªçng s·ªë v√† ƒê·ªô l·ªách

C√°c c√¥ng th·ª©c sau ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu ch·ªânh `weights` ($W$) v√† `biases` ($b$) trong m·ªói b∆∞·ªõc l·∫∑p:

* **C·∫≠p nh·∫≠t Tr·ªçng s·ªë:**

$$W := W - \alpha \frac{\partial J}{\partial W}$$

* **C·∫≠p nh·∫≠t ƒê·ªô l·ªách:**

$$b := b - \alpha \frac{\partial J}{\partial b}$$

`Learning rates` **nh·ªè h∆°n** cho ph√©p `updates` d·∫ßn d·∫ßn v√† ch√≠nh x√°c, trong khi `rates` **l·ªõn h∆°n** cho ph√©p `updates` nhanh h∆°n, nh∆∞ng c√≥ nguy c∆° b·ªè l·ª° ƒëi·ªÉm t·ªëi thi·ªÉu.

B·∫£n t√≥m t·∫Øt n√†y g√≥i g·ªçn c√°c kh√°i ni·ªám ch√≠nh li√™n quan ƒë·∫øn `training` m·ªôt **continuous bag of words model** (`CBOW`) trong b·ªëi c·∫£nh `neural networks`.

Qu√° tr√¨nh **Backpropagation** (lan truy·ªÅn ng∆∞·ª£c) v√† **Gradient Descent** (gi·∫£m ƒë·ªô d·ªëc) ƒë∆∞·ª£c t√≥m t·∫Øt nh∆∞ sau:

> Backpropagation (Lan truy·ªÅn ng∆∞·ª£c)

> **Backpropagation** l√† qu√° tr√¨nh t√≠nh to√°n **partial derivatives** (ƒë·∫°o h√†m ri√™ng) c·ªßa h√†m chi ph√≠ (`cost function`) $J_{\text{batch}}$ ƒë·ªëi v·ªõi t·∫•t c·∫£ c√°c tham s·ªë (`parameters`) c·ªßa m√¥ h√¨nh: $\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2$.

> Khi th·ª±c hi·ªán **back-prop** trong m√¥ h√¨nh **CBOW** n√†y, b·∫°n c·∫ßn t√≠nh to√°n c√°c ƒë·∫°o h√†m sau:

$$\frac{\partial J_{\text{batch}}}{\partial \mathbf{W}_1}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{W}_2}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{b}_1}, \quad \frac{\partial J_{\text{batch}}}{\partial \mathbf{b}_2}$$

> Gradient Descent (Gi·∫£m ƒë·ªô d·ªëc)

> **Gradient Descent** s·ª≠ d·ª•ng c√°c ƒë·∫°o h√†m ƒë√£ t√≠nh ·ªü tr√™n ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tham s·ªë, nh·∫±m t·ªëi thi·ªÉu h√≥a chi ph√≠ $J_{\text{batch}}$. C√°c c√¥ng th·ª©c c·∫≠p nh·∫≠t ƒë∆∞·ª£c l·∫∑p l·∫°i (`iterate`) nh∆∞ sau:

$$\mathbf{W}_{1} := \mathbf{W}_{1} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{W}_{1}}$$

$$\mathbf{W}_{2} := \mathbf{W}_{2} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{W}_{2}}$$

$$\mathbf{b}_{1} := \mathbf{b}_{1} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{b}_{1}}$$

$$\mathbf{b}_{2} := \mathbf{b}_{2} - \alpha \frac{\partial J_{\text {batch }}}{\partial \mathbf{b}_{2}}$$

> **Learning rate** ($\alpha$) l√† m·ªôt **hyperparameter** (si√™u tham s·ªë) quan tr·ªçng ki·ªÉm so√°t t·ªëc ƒë·ªô h·ªçc:

* **$\alpha$ nh·ªè h∆°n** cho ph√©p c√°c c·∫≠p nh·∫≠t **gradual** (d·∫ßn d·∫ßn) ƒë·ªëi v·ªõi c√°c `weights` v√† `biases`.
* **$\alpha$ l·ªõn h∆°n** cho ph√©p c·∫≠p nh·∫≠t **faster** (nhanh h∆°n).

> **L∆∞u √Ω:** N·∫øu $\alpha$ qu√° l·ªõn, b·∫°n c√≥ th·ªÉ **v∆∞·ª£t qu√°** ƒëi·ªÉm t·ªëi thi·ªÉu v√† kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨; n·∫øu n√≥ qu√° nh·ªè, `model` c·ªßa b·∫°n s·∫Ω m·∫•t r·∫•t nhi·ªÅu th·ªùi gian ƒë·ªÉ `training`.

---
### **Extracting Word Embedding Vectors**
---

N·ªôi dung n√†y t·∫≠p trung v√†o vi·ªác tr√≠ch xu·∫•t **word embeddings** t·ª´ m·ªôt `trained neural network model` (m√¥ h√¨nh m·∫°ng n∆°-ron ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán), c·ª• th·ªÉ trong b·ªëi c·∫£nh **continuous bag-of-words model** (`CBOW`).

#### Ph∆∞∆°ng ph√°p Tr√≠ch xu·∫•t

* **Column Vectors t·ª´ $W_1$**: M·ªói **column vector** (v√©c-t∆° c·ªôt) c·ªßa **matrix $W_1$** bi·ªÉu di·ªÖn `word embedding` cho m·ªôt t·ª´ t∆∞∆°ng ·ª©ng trong `vocabulary`, duy tr√¨ c√πng th·ª© t·ª± v·ªõi `input data`.
* **Row Vectors t·ª´ $W_2$**: M·ªói **row vector** (v√©c-t∆° h√†ng) c·ªßa **matrix $W_2$** ƒë√≥ng vai tr√≤ l√† `word embedding` cho t·ª´ t∆∞∆°ng ·ª©ng, c≈©ng tu√¢n theo th·ª© t·ª± `input`.

#### Bi·ªÉu di·ªÖn Trung b√¨nh

* **Average c·ªßa $W_1$ v√† $W_2$**: Ph∆∞∆°ng ph√°p th·ª© ba li√™n quan ƒë·∫øn vi·ªác t√≠nh **average** (trung b√¨nh) c√°c `column vectors` t·ª´ $W_1$ v√† c√°c `row vectors` t·ª´ $W_2$ ƒë·ªÉ t·∫°o ra m·ªôt **matrix $W_3$** m·ªõi, t·ª´ ƒë√≥ `word embeddings` c√≥ th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t.

#### T·ªïng quan B√†i t·∫≠p (Assignment Overview)

* Trong **assignment** (b√†i t·∫≠p) s·∫Øp t·ªõi, b·∫°n s·∫Ω t√≠nh **average** c·ªßa **transpose** (chuy·ªÉn v·ªã) c·ªßa $W_1$ v√† $W_2$ ƒë·ªÉ tr√≠ch xu·∫•t `word embeddings` d∆∞·ªõi d·∫°ng **row vectors**, chu·∫©n b·ªã cho c√°c **evaluation metrics** (s·ªë li·ªáu ƒë√°nh gi√°) trong t∆∞∆°ng lai.

> C√≥ hai t√πy ch·ªçn ƒë·ªÉ tr√≠ch xu·∫•t **word embeddings** sau khi hu·∫•n luy·ªán **continuous bag of words model** (`CBOW`).

>1. S·ª≠ d·ª•ng Ma tr·∫≠n $W_1$

B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng ma tr·∫≠n tr·ªçng s·ªë $\mathbf{W}_1$ (ma tr·∫≠n t·ª´ l·ªõp `input` ƒë·∫øn `hidden layer`) nh∆∞ sau:

* N·∫øu b·∫°n s·ª≠ d·ª•ng $\mathbf{W}_1$, m·ªói **column** (c·ªôt) c·ªßa ma tr·∫≠n s·∫Ω t∆∞∆°ng ·ª©ng v·ªõi `embedding` c·ªßa m·ªôt t·ª´ c·ª• th·ªÉ trong `vocabulary`.

![22_Extracting_Word_Embedding_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/22_Extracting_Word_Embedding_Vectors.png)


> 2. S·ª≠ d·ª•ng Ma tr·∫≠n $W_2$

B·∫°n c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng ma tr·∫≠n tr·ªçng s·ªë $\mathbf{W}_2$ (ma tr·∫≠n t·ª´ `hidden layer` ƒë·∫øn `output layer`) nh∆∞ sau:

* Trong tr∆∞·ªùng h·ª£p n√†y, m·ªói **row** (h√†ng) c·ªßa ma tr·∫≠n $\mathbf{W}_2$ s·∫Ω t∆∞∆°ng ·ª©ng v·ªõi `embedding` c·ªßa m·ªôt t·ª´.

![23_Extracting_Word_Embedding_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/23_Extracting_Word_Embedding_Vectors.png)

> 3. L·∫•y Gi√° tr·ªã Trung b√¨nh (Averaging)

T√πy ch·ªçn cu·ªëi c√πng l√† l·∫•y gi√° tr·ªã **average** (trung b√¨nh) c·ªßa c·∫£ hai ma tr·∫≠n nh∆∞ sau:

* Ph∆∞∆°ng ph√°p n√†y th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng v√¨ c·∫£ $\mathbf{W}_1$ v√† **chuy·ªÉn v·ªã** c·ªßa $\mathbf{W}_2$ ($\mathbf{W}_2^T$) ƒë·ªÅu ƒë√≥ng vai tr√≤ l√† `embedding matrices`. Vi·ªác l·∫•y gi√° tr·ªã trung b√¨nh gi·ªØa hai bi·ªÉu di·ªÖn n√†y th∆∞·ªùng cho k·∫øt qu·∫£ **embeddings** ·ªïn ƒë·ªãnh v√† m·∫°nh m·∫Ω h∆°n.

![24_Extracting_Word_Embedding_Vectors](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W4/24_Extracting_Word_Embedding_Vectors.png)

---
### **Evaluating Word Embeddings: Intrinsic Evaluation**
---


---
### **Evaluating Word Embeddings: Extrinsic Evaluation**
---



