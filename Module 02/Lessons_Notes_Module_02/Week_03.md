# **Module 02 - Natural Language Processing with Probabilistic Models**
## **Week 3: Autocomplete and Language Models**
---
### **N-Grams: Overview**
---
#### Understanding N-grams

- **N-grams** lÃ  cÃ¡c chuá»—i tá»« (`sequences of words`) Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c `NLP tasks` khÃ¡c nhau nhÆ° nháº­n dáº¡ng giá»ng nÃ³i (`speech recognition`) vÃ  sá»­a lá»—i chÃ­nh táº£ (`spelling correction`).
- Má»™t `language model` tÃ­nh toÃ¡n **xÃ¡c suáº¥t** (`probabilities`) cá»§a cÃ¡c cÃ¢u vÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³.

#### Building an N-gram Language Model

- Báº¡n sáº½ táº¡o má»™t **n-gram language model** tá»« má»™t **text corpus** (má»™t táº­p há»£p lá»›n cÃ¡c tÃ i liá»‡u vÄƒn báº£n).
- `Model` sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ **auto-completing** (tá»± Ä‘á»™ng hoÃ n thÃ nh) cÃ¡c cÃ¢u báº±ng cÃ¡ch gá»£i Ã½ cÃ¡c tá»« cÃ³ kháº£ nÄƒng xáº£y ra dá»±a trÃªn `input` cá»§a ngÆ°á»i dÃ¹ng.

#### Techniques and Applications

- KhÃ³a há»c sáº½ Ä‘á» cáº­p Ä‘áº¿n viá»‡c xá»­ lÃ½ cÃ¡c tá»« **out-of-vocabulary** (ngoÃ i tá»« vá»±ng) vÃ  sá»­ dá»¥ng cÃ¡c **smoothing techniques** (ká»¹ thuáº­t lÃ m má»‹n) Ä‘á»ƒ Æ°á»›c tÃ­nh `probabilities` cho cÃ¡c tá»« chÆ°a tá»«ng tháº¥y (`unseen words`).
- `Language models` cÅ©ng Ä‘Æ°á»£c táº­n dá»¥ng trong cÃ¡c há»‡ thá»‘ng **augmentative communication** (giao tiáº¿p tÄƒng cÆ°á»ng), giÃºp ngÆ°á»i dÃ¹ng bá»‹ suy giáº£m kháº£ nÄƒng nÃ³i hÃ¬nh thÃ nh cÃ¢u.

Äáº¿n cuá»‘i tuáº§n, báº¡n sáº½ triá»ƒn khai má»™t `model auto-completion` cÃ¢u sá»­ dá»¥ng cÃ¡c ká»¹ nÄƒng Ä‘Ã£ há»c.

> `N-grams` lÃ  ná»n táº£ng vÃ  cung cáº¥p cho báº¡n cÆ¡ sá»Ÿ Ä‘á»ƒ hiá»ƒu cÃ¡c `models` phá»©c táº¡p hÆ¡n trong chuyÃªn ngÃ nh nÃ y. CÃ¡c `models` nÃ y cho phÃ©p báº¡n tÃ­nh toÃ¡n `probabilities` (xÃ¡c suáº¥t) cá»§a cÃ¡c tá»« nháº¥t Ä‘á»‹nh xáº£y ra trong má»™t `sequence` cá»¥ thá»ƒ. Sá»­ dá»¥ng Ä‘iá»u Ä‘Ã³, báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t cÃ´ng cá»¥ `auto-correct` (tá»± Ä‘á»™ng sá»­a lá»—i) hoáº·c tháº­m chÃ­ lÃ  má»™t cÃ´ng cá»¥ gá»£i Ã½ tÃ¬m kiáº¿m (`search suggestion tool`).

> CÃ¡c á»©ng dá»¥ng khÃ¡c cá»§a `N-gram language modeling` bao gá»“m:

![01_N-Grams](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/01_N-Grams.png)

> Tuáº§n nÃ y báº¡n sáº½ há»c cÃ¡ch:

- `Process` má»™t `text corpus` (kho ngá»¯ liá»‡u vÄƒn báº£n) thÃ nh `N-gram language model`.
- Xá»­ lÃ½ cÃ¡c tá»« **out of vocabulary** (ngoÃ i tá»« vá»±ng).
- Triá»ƒn khai `smoothing` (lÃ m má»‹n) cho cÃ¡c `N-grams` chÆ°a tá»«ng tháº¥y trÆ°á»›c Ä‘Ã¢y.
- `Evaluation` (Ä‘Ã¡nh giÃ¡) `language model`.

---
### **N-grams and Probabilities**
---

Ná»™i dung nÃ y táº­p trung vÃ o `N-gram language models`, cÃ¡i mÃ  cáº§n thiáº¿t Ä‘á»ƒ táº¡o vÄƒn báº£n tá»± Ä‘á»™ng.

#### Understanding N-grams

- Má»™t **N-gram** lÃ  má»™t chuá»—i tá»« (`sequence of words`) trong Ä‘Ã³ thá»© tá»± quan trá»ng; nÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘á» cáº­p Ä‘áº¿n kÃ½ tá»± hoáº·c cÃ¡c yáº¿u tá»‘ khÃ¡c.
- **Unigrams** bao gá»“m cÃ¡c tá»« Ä‘Æ¡n duy nháº¥t, **bigrams** lÃ  cÃ¡c cáº·p tá»« liá»n ká», vÃ  **trigrams** lÃ  cÃ¡c bá»™ ba tá»«.

#### Calculating Probabilities

- `Probability` cá»§a má»™t `unigram` Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch chia sá»‘ láº§n Ä‘áº¿m cá»§a tá»« Ä‘Ã³ cho tá»•ng sá»‘ tá»« trong `corpus`.
- Äá»‘i vá»›i **bigrams**, `probability` cá»§a má»™t tá»« ($w_i$) Ä‘i sau má»™t tá»« khÃ¡c ($w_{i-1}$) Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng sá»‘ láº§n Ä‘áº¿m cá»§a `bigram` chia cho sá»‘ láº§n Ä‘áº¿m cá»§a `unigram` Ä‘á»©ng trÆ°á»›c:
$$P(w_i | w_{i-1}) = \frac{Count(w_{i-1} w_i)}{Count(w_{i-1})}$$

#### Generalizing to N-grams

- `Probability` cá»§a má»™t tá»« Ä‘i sau má»™t chuá»—i tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c tá»•ng quÃ¡t hÃ³a cho báº¥t ká»³ `N-gram` nÃ o, sá»­ dá»¥ng sá»‘ láº§n Ä‘áº¿m cá»§a `N-gram` Ä‘Ã³ vÃ  `prefix` ($N-1$ grams) cá»§a nÃ³:
$$P(w_i | w_{i-(N-1)} \dots w_{i-1}) = \frac{Count(w_{i-(N-1)} \dots w_i)}{Count(w_{i-(N-1)} \dots w_{i-1})}$$
- `Framework` nÃ y cho phÃ©p tÃ­nh toÃ¡n `probabilities` cho cÃ¡c chuá»—i cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau, nÃ¢ng cao kháº£ nÄƒng táº¡o vÄƒn báº£n.

> TrÆ°á»›c khi chÃºng ta báº¯t Ä‘áº§u tÃ­nh toÃ¡n `probabilities` cá»§a cÃ¡c `sequences` nháº¥t Ä‘á»‹nh, Ä‘áº§u tiÃªn chÃºng ta cáº§n Ä‘á»‹nh nghÄ©a `N-gram language model` lÃ  gÃ¬:

![02_N-grams_and_Probabilities](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/02_N-grams_and_Probabilities.png)

> BÃ¢y giá» vá»›i nhá»¯ng Ä‘á»‹nh nghÄ©a Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ gÃ¡n nhÃ£n cho má»™t cÃ¢u nhÆ° sau:

![03_N-grams_and_Probabilities](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/03_N-grams_and_Probabilities.png)

> Báº±ng kÃ½ hiá»‡u khÃ¡c, báº¡n cÃ³ thá»ƒ viáº¿t:

$$w_{1}^{m}=w_{1}w_{2}w_{3}\dots w_{m}$$

$$w_{1}^{3}=w_{1}w_{2}w_{3}$$

$$w_{m-2}^{m}=w_{m-2}w_{m-1}w_{m}$$

> Cho `corpus` sau: "I am happy because I am learning." KÃ­ch thÆ°á»›c `corpus` $m = 7$.

$$P(\text{I}) = \frac{2}{7}$$

$$P(\text{happy}) = \frac{1}{7}$$

> Äá»ƒ tá»•ng quÃ¡t hÃ³a, `probability` cá»§a má»™t `unigram` lÃ :

$$P(w) = \frac{C(w)}{m}$$

#### Bigram Probability

![04_N-grams_and_Probabilities](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/04_N-grams_and_Probabilities.png)

> `Bigram Probability` Ä‘Æ°á»£c tÃ­nh nhÆ° sau (Ã¡p dá»¥ng cÃ´ng thá»©c tá»•ng quÃ¡t cho $N=2$):

$$P(w_2 \mid w_1) = \frac{C(w_1 w_2)}{C(w_1)}$$

#### Trigram Probability

> Äá»ƒ tÃ­nh `probability` cá»§a má»™t `trigram`:

$$P(w_3 \mid w_{1}^{2}) = \frac{C(w_{1}^{2}w_{3})}{C(w_{1}^{2})}$$

> Trong Ä‘Ã³ `count` lÃ :

$$C(w_{1}^{2}w_{3})=C(w_{1}w_{2}w_{3})=C(w_{1}^{3})$$

#### N-gram Probability

> `N-gram Probability` Ä‘Æ°á»£c tá»•ng quÃ¡t hÃ³a nhÆ° sau:

$$P(w_{N} \mid w_{1}^{N-1}) = \frac{C(w_{1}^{N-1}w_{N})}{C(w_{1}^{N-1})}$$

> Trong Ä‘Ã³ `count` lÃ :

$$C(w_{1}^{N-1}w_{N})=C(w_{1}^{N})$$

---
### **Sequence Probabilities**
---

Ná»™i dung nÃ y táº­p trung vÃ o viá»‡c mÃ´ hÃ¬nh hÃ³a toÃ n bá»™ cÃ¢u báº±ng cÃ¡ch sá»­ dá»¥ng `n-gram probabilities`, Ä‘iá»u nÃ y cáº§n thiáº¿t Ä‘á»ƒ táº¡o vÄƒn báº£n.

#### Understanding Conditional Probability

- `Conditional probability` (xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n) cá»§a má»™t tá»« phá»¥ thuá»™c vÃ o (cÃ¡c) tá»« Ä‘á»©ng trÆ°á»›c, Ä‘Æ°á»£c thá»ƒ hiá»‡n thÃ´ng qua **chain rule** (quy táº¯c chuá»—i).
- CÃ´ng thá»©c cho `conditional probability` cÃ³ thá»ƒ Ä‘Æ°á»£c sáº¯p xáº¿p láº¡i Ä‘á»ƒ tÃ­nh **joint probability** (xÃ¡c suáº¥t Ä‘á»“ng thá»i) cá»§a cÃ¡c chuá»—i tá»«.

#### Applying the Chain Rule

- `Probability` cá»§a má»™t cÃ¢u ($w_1, \dots, w_m$) Ä‘Æ°á»£c tÃ­nh báº±ng tÃ­ch cá»§a `probabilities` cá»§a má»—i tá»« vá»›i Ä‘iá»u kiá»‡n lÃ  cÃ¡c tá»« Ä‘á»©ng trÆ°á»›c nÃ³:

$$P(w_1, \dots, w_m) = P(w_1) \prod_{i=2}^{m} P(w_i | w_1, \dots, w_{i-1})$$

- Khi cÃ¡c cÃ¢u dÃ i hÆ¡n, kháº£ nÄƒng tÃ¬m tháº¥y cÃ¡c `sequences` chÃ­nh xÃ¡c trong `training corpus` (kho ngá»¯ liá»‡u huáº¥n luyá»‡n) giáº£m xuá»‘ng.

#### Using the Markov Assumption

- **Markov assumption** (giáº£ Ä‘á»‹nh Markov) Ä‘Æ¡n giáº£n hÃ³a cÃ¡c phÃ©p tÃ­nh báº±ng cÃ¡ch chá»‰ xem xÃ©t má»™t lá»‹ch sá»­ giá»›i háº¡n cá»§a cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ ($n-1$).
- DÆ°á»›i giáº£ Ä‘á»‹nh Markov, `conditional probability` cá»§a $w_i$ Ä‘Æ°á»£c tÃ­nh:

$$P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i | w_{i-(N-1)}, \dots, w_{i-1})$$

- Äá»‘i vá»›i `bigrams` (N=2), cÃ´ng thá»©c táº­p trung vÃ o tá»« Ä‘á»©ng ngay trÆ°á»›c nÃ³ ($w_{i-1}$), cho phÃ©p Æ°á»›c tÃ­nh `sentence probabilities` dá»… dÃ ng hÆ¡n.

Báº£n tÃ³m táº¯t nÃ y gÃ³i gá»n cÃ¡c khÃ¡i niá»‡m chÃ­nh vá» `n-gram modeling` vÃ  á»©ng dá»¥ng cá»§a nÃ³ trong `natural language processing`.

> Báº¡n vá»«a tháº¥y cÃ¡ch tÃ­nh toÃ¡n `sequence probabilities`, nhá»¯ng thiáº¿u sÃ³t cá»§a chÃºng, vÃ  cuá»‘i cÃ¹ng lÃ  cÃ¡ch xáº¥p xá»‰ `N-gram probabilities`. Khi lÃ m nhÆ° váº­y, báº¡n cá»‘ gáº¯ng xáº¥p xá»‰ `probability` (xÃ¡c suáº¥t) cá»§a má»™t cÃ¢u. VÃ­ dá»¥, `probability` cá»§a cÃ¢u sau lÃ  gÃ¬: "The teacher drinks tea."

> Äá»ƒ tÃ­nh toÃ¡n nÃ³, báº¡n sáº½ sá»­ dá»¥ng nhá»¯ng Ä‘iá»u sau:

- $$P(B \mid A)=\frac{P(A,B)}{P(A)}\Longrightarrow P(A,B)=P(A)P(B\mid A)$$

- $$P(A,B,C,D)=P(A)P(B\mid A)P(C\mid A,B)P(D\mid A,B,C)$$

> Äá»ƒ tÃ­nh `probability` cá»§a má»™t `sequence`, báº¡n cÃ³ thá»ƒ tÃ­nh nhÆ° sau:

$$\begin{array}{r}P(\text { the teacher drinks tea })= \begin{array}{r}P(\text {the}) P(\text { teacher } \mid \text {the}) P(\text { drinks } \mid \text {the teacher}) P(\text {tea} \mid \text {the teacher drinks })\end{array}\end{array}$$

> Má»™t trong nhá»¯ng váº¥n Ä‘á» chÃ­nh khi tÃ­nh `probabilities` á»Ÿ trÃªn lÃ  `corpus` hiáº¿m khi chá»©a chÃ­nh xÃ¡c cÃ¡c cá»¥m tá»« giá»‘ng nhÆ° nhá»¯ng cá»¥m tá»« báº¡n Ä‘Ã£ tÃ­nh `probabilities`. Do Ä‘Ã³, báº¡n cÃ³ thá»ƒ dá»… dÃ ng nháº­n Ä‘Æ°á»£c `probability` báº±ng 0. **Markov assumption** (giáº£ Ä‘á»‹nh Markov) chá»‰ ra ráº±ng chá»‰ tá»« cuá»‘i cÃ¹ng má»›i quan trá»ng. Do Ä‘Ã³:

- $$\text{Bigram } P(w_n \mid w_{1}^{n-1})\approx P(w_n \mid w_{n-1})$$

- $$\text{N-gram } P(w_n \mid w_{1}^{n-1})\approx P(w_n \mid w_{n-N+1}^{n-1})$$

> Báº¡n cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a toÃ n bá»™ `sentence` nhÆ° sau:

- $$P(w_{1}^{n})\approx\prod_{i=1}^{n}P(w_i\mid w_{i-1})$$

- $$P(w_{1}^{n})\approx P(w_1)P(w_2\mid w_1)\dots P(w_n\mid w_{n-1})$$

---
### **Starting and Ending Sentences**
---

Ná»™i dung nÃ y táº­p trung vÃ o viá»‡c xá»­ lÃ½ Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a cÃ¢u khi triá»ƒn khai `N-gram language models`.

#### Understanding Sentence Boundaries

- Giá»›i thiá»‡u cÃ¡c kÃ½ hiá»‡u Ä‘áº·c biá»‡t Ä‘á»ƒ biá»ƒu thá»‹ `start` (báº¯t Ä‘áº§u) vÃ  `end` (káº¿t thÃºc) cá»§a má»™t `sentence`, nhá»¯ng kÃ½ hiá»‡u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ tÃ­nh toÃ¡n `probabilities` trong `N-gram models`.
- Giáº£i thÃ­ch cÃ¡ch sá»­a Ä‘á»•i cÃ¡c cÃ¢u báº±ng cÃ¡ch thÃªm `start tokens` (vÃ­ dá»¥: "S") vÃ  `end tokens` (vÃ­ dá»¥: "/S") Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n thuáº­n lá»£i cho cÃ¡c phÃ©p tÃ­nh `bigram` vÃ  `N-gram`.

#### Calculating Probabilities

- Tháº£o luáº­n vá» nhá»¯ng thÃ¡ch thá»©c trong viá»‡c tÃ­nh toÃ¡n `probabilities` cho tá»« Ä‘áº§u tiÃªn trong má»™t `sentence` do thiáº¿u `context`, vÃ  cÃ¡ch viá»‡c thÃªm `start tokens` giáº£i quyáº¿t váº¥n Ä‘á» nÃ y.
- MÃ´ táº£ cÃ¡ch xá»­ lÃ½ tá»« cuá»‘i cÃ¹ng cá»§a má»™t `sentence`, nháº¥n máº¡nh sá»± cáº§n thiáº¿t cá»§a má»™t `end-of-sentence token` Ä‘á»ƒ duy trÃ¬ cÃ¡c phÃ©p tÃ­nh `probability` chÃ­nh xÃ¡c.

#### Generalizing to N-grams

- Giáº£i thÃ­ch ráº±ng Ä‘á»‘i vá»›i **N-grams**, viá»‡c thÃªm $n-1$ `start tokens` á»Ÿ Ä‘áº§u vÃ  má»™t `end token` á»Ÿ cuá»‘i má»—i `sentence` cho phÃ©p Æ°á»›c tÃ­nh `probability` chÃ­nh xÃ¡c.
- Minh há»a báº±ng cÃ¡c vÃ­ dá»¥ vá» cÃ¡ch tÃ­nh `probabilities` cho cÃ¡c Ä‘á»™ dÃ i `sentence` khÃ¡c nhau, Ä‘áº£m báº£o tá»•ng `probability` cá»™ng láº¡i báº±ng 1 trÃªn táº¥t cáº£ cÃ¡c `sentences` cÃ³ thá»ƒ cÃ³.

> ChÃºng ta thÆ°á»ng báº¯t Ä‘áº§u vÃ  káº¿t thÃºc má»™t cÃ¢u vá»›i cÃ¡c `tokens` sau tÆ°Æ¡ng á»©ng: `<s>` vÃ  `</s>`.

> Khi tÃ­nh toÃ¡n `probabilities` báº±ng cÃ¡ch sá»­ dá»¥ng má»™t `unigram`, báº¡n cÃ³ thá»ƒ thÃªm má»™t `<s>` vÃ o Ä‘áº§u cÃ¢u. Äá»ƒ tá»•ng quÃ¡t hÃ³a cho má»™t **N-gram language model**, báº¡n cÃ³ thá»ƒ thÃªm $N-1$ `start tokens` `<s>`.

> Äá»‘i vá»›i `end of sentence token` `</s>`, báº¡n chá»‰ cáº§n má»™t cÃ¡i ngay cáº£ khi Ä‘Ã³ lÃ  má»™t `N-gram`.

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥:

![05_Starting_and_Ending_Sentences](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/05_Starting_and_Ending_Sentences.png)

> HÃ£y Ä‘áº£m báº£o ráº±ng báº¡n biáº¿t cÃ¡ch tÃ­nh toÃ¡n `probabilities` á»Ÿ trÃªn!

---
### **The N-gram Language Model**
---

Ná»™i dung nÃ y táº­p trung vÃ o viá»‡c táº¡o vÃ  sá»­ dá»¥ng má»™t **count matrix** (ma tráº­n Ä‘áº¿m) cho `n-grams` trong `natural language processing`.

#### Count Matrix Creation

- **Count matrix** ghi láº¡i sá»± xuáº¥t hiá»‡n cá»§a `n-grams`, vá»›i cÃ¡c `rows` (hÃ ng) Ä‘áº¡i diá»‡n cho cÃ¡c `n-1 grams` Ä‘á»™c nháº¥t (`unique`) vÃ  cÃ¡c `columns` (cá»™t) Ä‘áº¡i diá»‡n cho cÃ¡c tá»« Ä‘á»™c nháº¥t.
- Äá»‘i vá»›i `bigrams`, phÆ°Æ¡ng phÃ¡p **sliding window** (cá»­a sá»• trÆ°á»£t) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘áº¿m sá»± xuáº¥t hiá»‡n khi báº¡n `process` `corpus`.

#### Probability Matrix Transformation

- **Count matrix** Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh **probability matrix** (ma tráº­n xÃ¡c suáº¥t) báº±ng cÃ¡ch `normalizing` (chuáº©n hÃ³a) má»—i Ã´ dá»±a trÃªn tá»•ng cá»§a `row` tÆ°Æ¡ng á»©ng.
- `Matrix` nÃ y cung cáº¥p **conditional probabilities** (xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n) cá»§a `n-grams`, Ä‘iá»u cáº§n thiáº¿t cho `language modeling`.

#### Language Model Implementation

- `Language model` sá»­ dá»¥ng `probability matrix` Ä‘á»ƒ Æ°á»›c tÃ­nh `sentence probabilities` vÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t `sequence`.
- NÃ³ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» **numerical underflow** (trÃ n sá»‘ Ã¢m) phÃ¡t sinh tá»« viá»‡c nhÃ¢n cÃ¡c `probabilities` nhá», thÆ°á»ng sá»­ dá»¥ng cÃ¡c **logarithmic transformations** Ä‘á»ƒ Ä‘áº£m báº£o `stability` (á»•n Ä‘á»‹nh).

> Báº¡n Ä‘Ã£ xem qua ráº¥t nhiá»u khÃ¡i niá»‡m trong video trÆ°á»›c. Báº¡n Ä‘Ã£ tháº¥y:

- **Count matrix** (Ma tráº­n Ä‘áº¿m)
- **Probability matrix** (Ma tráº­n xÃ¡c suáº¥t)
- **Language model** (MÃ´ hÃ¬nh ngÃ´n ngá»¯)
- **Log probability** (XÃ¡c suáº¥t log) Ä‘á»ƒ trÃ¡nh trÃ n sá»‘ Ã¢m (`underflow`)
- **Generative language model** (MÃ´ hÃ¬nh ngÃ´n ngá»¯ sinh)

#### Count Matrix vÃ  Chuyá»ƒn Ä‘á»•i sang Probability Matrix

> Trong **Count matrix**:

- CÃ¡c **Rows** (HÃ ng) tÆ°Æ¡ng á»©ng vá»›i cÃ¡c `N-1 grams` Ä‘á»™c nháº¥t (`unique`) cá»§a `corpus`.
- CÃ¡c **Columns** (Cá»™t) tÆ°Æ¡ng á»©ng vá»›i cÃ¡c tá»« Ä‘á»™c nháº¥t cá»§a `corpus`.

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ vá» **count matrix** cá»§a má»™t `bigram` (Ngá»¥ Ã½ trong hÃ¬nh trÃªn).

![06_The_N-gram_Language_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/06_The_N-gram_Language_Model.png)


> Äá»ƒ chuyá»ƒn Ä‘á»•i nÃ³ thÃ nh **probability matrix**, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ´ng thá»©c sau:

- $$P(w_{n} \mid w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n-1}, w_{n})}{C(w_{n-N+1}^{n-1})}$$

> Trong Ä‘Ã³, tá»•ng cá»§a má»—i hÃ ng chÃ­nh lÃ  sá»‘ láº§n Ä‘áº¿m cá»§a `prefix` ($N-1$ gram), Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuáº©n hÃ³a (`normalize`):

- $$sum(row)=\sum_{w \in V} C(w_{n-N+1}^{n-1}, w) = C(w_{n-N+1}^{n-1})$$

#### MÃ´ hÃ¬nh hÃ³a vÃ  Khá»­ Underflow

> BÃ¢y giá» vá»›i **probability matrix**, báº¡n cÃ³ thá»ƒ táº¡o ra `language model`. Báº¡n cÃ³ thá»ƒ tÃ­nh toÃ¡n `sentence probability` (xÃ¡c suáº¥t cÃ¢u) vÃ  `next word prediction` (dá»± Ä‘oÃ¡n tá»« tiáº¿p theo).

> Äá»ƒ tÃ­nh `probability` cá»§a má»™t `sequence` (chuá»—i) ($w_1, \dots, w_n$), báº¡n cáº§n tÃ­nh:

- $$P(w_{1}^{n}) \approx \prod_{i=1}^{n} P(w_i \mid w_{i-1})$$

> Äá»ƒ trÃ¡nh **underflow** (trÃ n sá»‘ Ã¢m), báº¡n cÃ³ thá»ƒ nhÃ¢n báº±ng `log` (logarit):

- $$\log(P(w_{1}^{n})) \approx \sum_{i=1}^{n} \log(P(w_i \mid w_{i-1}))$$

> Cuá»‘i cÃ¹ng, Ä‘Ã¢y lÃ  báº£n tÃ³m táº¯t Ä‘á»ƒ táº¡o ra **generative model**:

![07_The_N-gram_Language_Model](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/07_The_N-gram_Language_Model.png)

---
### **Language Model Evaluation**
---

Ná»™i dung nÃ y táº­p trung vÃ o viá»‡c Ä‘Ã¡nh giÃ¡ `language models` báº±ng cÃ¡ch sá»­ dá»¥ng `perplexity metric`.

#### Understanding Data Splits

- `Text corpus` Ä‘Æ°á»£c chia thÃ nh cÃ¡c táº­p **training**, **validation**, vÃ  **test sets** Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ `model` hiá»‡u quáº£.
- Má»™t tá»· lá»‡ chia phá»• biáº¿n cho cÃ¡c `datasets` nhá» hÆ¡n lÃ  **80%** cho `training`, **10%** cho `validation`, vÃ  **10%** cho `testing`.

#### Perplexity as a Metric

- **Perplexity** Ä‘o lÆ°á»ng Ä‘á»™ phá»©c táº¡p cá»§a má»™t vÄƒn báº£n vÃ  cho biáº¿t má»©c Ä‘á»™ `language model` dá»± Ä‘oÃ¡n má»™t táº­p há»£p cÃ¡c cÃ¢u tá»‘t nhÆ° tháº¿ nÃ o.
- **Perplexity scores** tháº¥p hÆ¡n cho tháº¥y vÄƒn báº£n xuáº¥t hiá»‡n tá»± nhiÃªn vÃ  giá»‘ng con ngÆ°á»i hÆ¡n, trong khi `scores` cao hÆ¡n cho tháº¥y sá»± ngáº«u nhiÃªn.

#### Calculating Perplexity

- **Perplexity** ($PP$) Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh `probability` cá»§a cÃ¡c cÃ¢u trong `test set` vÃ  `normalizing` (chuáº©n hÃ³a) nÃ³ theo sá»‘ lÆ°á»£ng tá»« ($N$).
- CÃ´ng thá»©c tá»•ng quÃ¡t cho Perplexity cá»§a má»™t chuá»—i tá»« $W = w_1, \dots, w_N$ lÃ :
$$PP(W) = P(w_1, w_2, \dots, w_N)^{-\frac{1}{N}}$$
- CÃ´ng thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hÃ³a cho cÃ¡c `bigram models`, vÃ  **log perplexity** thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n dá»… dÃ ng hÆ¡n.

#### Examples of Language Models

- CÃ¡c `language models` khÃ¡c nhau mang láº¡i cÃ¡c `perplexity scores` khÃ¡c nhau, vá»›i cÃ¡c `models` tá»‘t thÆ°á»ng Ä‘áº¡t tá»« **20 Ä‘áº¿n 60**.
- Ná»™i dung minh há»a cÃ¡ch cÃ¡c **unigram**, **bigram**, vÃ  **trigram models** hoáº¡t Ä‘á»™ng vá» máº·t `perplexity`, cho tháº¥y sá»± cáº£i thiá»‡n vá» tÃ­nh **coherence** (máº¡ch láº¡c) cá»§a vÄƒn báº£n vá»›i cÃ¡c `models` phá»©c táº¡p hÆ¡n.

> BÃ¢y giá» chÃºng ta sáº½ tháº£o luáº­n vá» cÃ¡c `train/val/test splits` (chia táº­p huáº¥n luyá»‡n/xÃ¡c thá»±c/kiá»ƒm thá»­) vÃ  `perplexity`.

#### Train/Val/Test splits

- **Corpora (Kho ngá»¯ liá»‡u) nhá» hÆ¡n:**
    + 80% train
    + 10% val
    + 10% test
- **Corpora lá»›n hÆ¡n:**
    + 98% train
    + 1% val
    + 1% test
> CÃ³ 2 phÆ°Æ¡ng phÃ¡p cho chia táº­p dá»¯ liá»‡u

![08_Language_Model_Evaluation](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/08_Language_Model_Evaluation.png)

#### Perplexity

> `Perplexity` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cho chÃºng ta biáº¿t liá»‡u má»™t táº­p há»£p cÃ¡c cÃ¢u cÃ³ váº» Ä‘Æ°á»£c viáº¿t bá»Ÿi con ngÆ°á»i hay khÃ´ng, thay vÃ¬ Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t chÆ°Æ¡ng trÃ¬nh Ä‘Æ¡n giáº£n chá»n tá»« ngáº«u nhiÃªn. Má»™t vÄƒn báº£n Ä‘Æ°á»£c viáº¿t bá»Ÿi con ngÆ°á»i cÃ³ nhiá»u kháº£ nÄƒng cÃ³ `perplexity` tháº¥p hÆ¡n, trong khi má»™t vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra bá»Ÿi viá»‡c chá»n tá»« ngáº«u nhiÃªn sáº½ cÃ³ `perplexity` cao hÆ¡n.

> Cá»¥ thá»ƒ, Ä‘Ã¢y lÃ  cÃ¡c cÃ´ng thá»©c Ä‘á»ƒ tÃ­nh `perplexity`.

$$PP(W)=P(s_1,s_2,\dots,s_m)^{-\frac{1}{m}}$$

$$PP(W) = \sqrt[m]{ \prod_{i=1}^{m} \prod_{j=1}^{|s_i|} \frac{1}{P\left(w_j^{(i)} \mid w_{j-1}^{(i)}\right)} }$$

> $w_j^{(i)} \to j$ tÆ°Æ¡ng á»©ng vá»›i tá»« thá»© $j$ trong cÃ¢u thá»© $i$. Náº¿u báº¡n ná»‘i táº¥t cáº£ cÃ¡c cÃ¢u láº¡i, thÃ¬ $w_i$ lÃ  tá»« thá»© $i$ trong `test set`.

$$PP(W) = \sqrt[m]{ \prod_{i=1}^{m} \frac{1}{P(w_i \mid w_{i-1})} }$$

> Äá»ƒ tÃ­nh **log perplexity**, báº¡n chuyá»ƒn tá»« cÃ´ng thá»©c trÃªn thÃ nh:

$$\log PP(W)=-\frac{1}{m}\sum_{i=1}^{m}\log_{2}(P(w_i\mid w_{i-1}))$$

---
### **Out of Vocabulary Words**
---

Ná»™i dung nÃ y táº­p trung vÃ o viá»‡c xá»­ lÃ½ cÃ¡c tá»« **out-of-vocabulary** (`OOV`) trong `language models`.

#### Understanding Out-of-Vocabulary Words

- **OOV words** lÃ  nhá»¯ng tá»« khÃ´ng cÃ³ máº·t trong `training vocabulary` cá»§a `model`, thÆ°á»ng gáº·p trong cÃ¡c `tasks` nhÆ° `speech recognition`.
- Má»™t **closed vocabulary** (tá»« vá»±ng Ä‘Ã³ng) giá»›i háº¡n `model` trong má»™t táº­p há»£p tá»« cá»‘ Ä‘á»‹nh, trong khi **open vocabulary** (tá»« vá»±ng má»Ÿ) cho phÃ©p cÃ¡c tá»« má»›i, chÆ°a tá»«ng tháº¥y.

#### Using the UNK Token

- Äá»ƒ quáº£n lÃ½ `OOV words`, chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c thay tháº¿ báº±ng má»™t `special token` (**UNK**), trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n `probability`.
- `Vocabulary` Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a dá»±a trÃªn `word frequency` (táº§n suáº¥t tá»«), trong Ä‘Ã³ cÃ¡c tá»« xuáº¥t hiá»‡n Ã­t hÆ¡n má»™t sá»‘ láº§n Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh sáº½ Ä‘Æ°á»£c thay tháº¿ báº±ng **UNK**.

#### Building Vocabulary

- `Vocabulary` cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o báº±ng cÃ¡ch Ä‘áº·t má»™t **minimum frequency threshold** (ngÆ°á»¡ng táº§n suáº¥t tá»‘i thiá»ƒu) hoáº·c má»™t giá»›i háº¡n kÃ­ch thÆ°á»›c tá»‘i Ä‘a.
- Sá»± hiá»‡n diá»‡n cá»§a `UNK tokens` cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n `perplexity` cá»§a `model`, thÆ°á»ng lÃ m nÃ³ cÃ³ váº» hiá»‡u quáº£ hÆ¡n, nhÆ°ng quÃ¡ nhiá»u `UNKs` cÃ³ thá»ƒ dáº«n Ä‘áº¿n `outputs` vÃ´ nghÄ©a.

TÃ³m láº¡i, bÃ i giáº£ng nháº¥n máº¡nh táº§m quan trá»ng cá»§a viá»‡c quáº£n lÃ½ hiá»‡u quáº£ `OOV words` Ä‘á»ƒ cáº£i thiá»‡n `language model performance`.

> Nhiá»u khi, báº¡n sáº½ pháº£i xá»­ lÃ½ cÃ¡c tá»« khÃ´ng xÃ¡c Ä‘á»‹nh trong `corpus`. Váº­y lÃ m tháº¿ nÃ o Ä‘á»ƒ báº¡n chá»n **vocabulary** (tá»« vá»±ng) cá»§a mÃ¬nh?

#### Äá»‹nh nghÄ©a vÃ  Loáº¡i Vocabulary

> **Vocabulary** lÃ  má»™t táº­p há»£p cÃ¡c tá»« Ä‘á»™c nháº¥t Ä‘Æ°á»£c há»— trá»£ bá»Ÿi `language model` cá»§a báº¡n.

- Trong má»™t sá»‘ `tasks` nhÆ° `speech recognition` hoáº·c `question answering`, báº¡n sáº½ gáº·p vÃ  táº¡o ra cÃ¡c tá»« chá»‰ tá»« má»™t táº­p há»£p tá»« cá»‘ Ä‘á»‹nh. Do Ä‘Ã³, Ä‘Ã¢y lÃ  **closed vocabulary** (tá»« vá»±ng Ä‘Ã³ng).
- **Open vocabulary** (tá»« vá»±ng má»Ÿ) cÃ³ nghÄ©a lÃ  báº¡n cÃ³ thá»ƒ gáº·p cÃ¡c tá»« bÃªn ngoÃ i `vocabulary`, nhÆ° tÃªn cá»§a má»™t thÃ nh phá»‘ má»›i trong `training set`.

#### "CÃ´ng thá»©c" xá»­ lÃ½ tá»« khÃ´ng xÃ¡c Ä‘á»‹nh

> DÆ°á»›i Ä‘Ã¢y lÃ  má»™t "cÃ´ng thá»©c" cho phÃ©p báº¡n xá»­ lÃ½ cÃ¡c tá»« khÃ´ng xÃ¡c Ä‘á»‹nh:

1.  Táº¡o `vocabulary` $V$.
2.  Thay tháº¿ báº¥t ká»³ tá»« nÃ o trong `corpus` vÃ  khÃ´ng cÃ³ trong $V$ báº±ng **`<UNK>`** (Unknown token).
3.  Äáº¿m `probabilities` vá»›i `<UNK>` nhÆ° vá»›i báº¥t ká»³ tá»« nÃ o khÃ¡c.

![09_Out_of_Vocabulary_Words](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/09_Out_of_Vocabulary_Words.png)

> VÃ­ dá»¥ trÃªn cho tháº¥y cÃ¡ch báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng **min\_frequency** (táº§n suáº¥t tá»‘i thiá»ƒu) vÃ  thay tháº¿ táº¥t cáº£ cÃ¡c tá»« xuáº¥t hiá»‡n Ã­t hÆ¡n `min_frequency` báº±ng **UNK**. Sau Ä‘Ã³ báº¡n cÃ³ thá»ƒ coi **UNK** nhÆ° má»™t tá»« thÃ´ng thÆ°á»ng.

#### TiÃªu chÃ­ Ä‘á»ƒ táº¡o Vocabulary

- **Min word frequency $f$**: Chá»n má»™t táº§n suáº¥t tá»« tá»‘i thiá»ƒu.
- **Max $|V|$**: Äáº·t giá»›i háº¡n kÃ­ch thÆ°á»›c tá»‘i Ä‘a cá»§a `vocabulary`, bao gá»“m cÃ¡c tá»« theo táº§n suáº¥t.
- Sá»­ dá»¥ng `<UNK>` má»™t cÃ¡ch **tiáº¿t kiá»‡m** (VÃ¬ viá»‡c sá»­ dá»¥ng quÃ¡ nhiá»u `<UNK>` cÃ³ thá»ƒ lÃ m giáº£m Ã½ nghÄ©a cá»§a `output`).
- **Perplexity**: Chá»‰ so sÃ¡nh `LM` (Language Models) cÃ³ cÃ¹ng `vocabulary` ($V$) Ä‘á»ƒ cÃ³ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ cÃ´ng báº±ng.

---
### **Smoothing**
---

Ná»™i dung nÃ y táº­p trung vÃ o khÃ¡i niá»‡m **smoothing** (lÃ m má»‹n) trong `N-gram language models`, cÃ¡i mÃ  cáº§n thiáº¿t Ä‘á»ƒ cáº£i thiá»‡n viá»‡c Æ°á»›c tÃ­nh `probabilities` trong `natural language processing`.

#### Ká»¹ thuáº­t Smoothing

- **Smoothing** giáº£i quyáº¿t váº¥n Ä‘á» **zero probabilities** (xÃ¡c suáº¥t báº±ng 0) Ä‘á»‘i vá»›i cÃ¡c `N-grams` khÃ´ng cÃ³ máº·t trong má»™t `corpus` giá»›i háº¡n.
- **Add-one smoothing** (hay **Laplacian smoothing**) Ä‘iá»u chá»‰nh cÃ´ng thá»©c `N-gram probability` báº±ng cÃ¡ch thÃªm 1 vÃ o cáº£ tá»­ sá»‘ (`numerator`) vÃ  máº«u sá»‘ (`denominator`), Ä‘áº£m báº£o khÃ´ng cÃ³ `N-grams` nÃ o cÃ³ `zero probability`.
- CÃ´ng thá»©c cho `Add-one smoothing` (cho `bigram`) lÃ :

$$P_{\text{Add-1}}(w_i \mid w_{i-1}) = \frac{Count(w_{i-1} w_i) + 1}{Count(w_{i-1}) + |V|}$$

(Trong Ä‘Ã³ $|V|$ lÃ  kÃ­ch thÆ°á»›c `vocabulary`).

#### Backoff vÃ  Interpolation

- **Backoff** sá»­ dá»¥ng cÃ¡c `N-grams` cáº¥p tháº¥p hÆ¡n khi cÃ¡c `N-grams` cáº¥p cao hÆ¡n bá»‹ thiáº¿u, Ã¡p dá»¥ng **discounting** (chiáº¿t kháº¥u) Ä‘á»ƒ Ä‘iá»u chá»‰nh `probabilities`.
- **Linear interpolation** (ná»™i suy tuyáº¿n tÃ­nh) káº¿t há»£p `weighted probabilities` (xÃ¡c suáº¥t cÃ³ trá»ng sá»‘) tá»« cÃ¡c cáº¥p Ä‘á»™ `N-gram` khÃ¡c nhau, tá»‘i Æ°u hÃ³a `model` báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c háº±ng sá»‘ cá»™ng láº¡i báº±ng 1.

#### CÃ¡c PhÆ°Æ¡ng phÃ¡p NÃ¢ng cao

- CÃ¡c ká»¹ thuáº­t `smoothing` phá»©c táº¡p hÆ¡n bao gá»“m **add-k smoothing**, **Kneser-Ney**, vÃ  **Good-Turing methods**, nhá»¯ng ká»¹ thuáº­t nÃ y tinh chá»‰nh thÃªm cÃ¡c Æ°á»›c tÃ­nh `probability`.
- CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y nÃ¢ng cao `performance` cá»§a `N-gram models`, Ä‘áº·c biá»‡t trong cÃ¡c `corpora` lá»›n hÆ¡n, báº±ng cÃ¡ch cung cáº¥p kháº£ nÄƒng xá»­ lÃ½ tá»‘t hÆ¡n dá»¯ liá»‡u bá»‹ thiáº¿u.

Ba khÃ¡i niá»‡m chÃ­nh Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ Ä‘Ã¢y lÃ  xá»­ lÃ½ cÃ¡c `n-grams` bá»‹ thiáº¿u, **smoothing** (lÃ m má»‹n), vÃ  **Backoff** cÃ¹ng **interpolation** (ná»™i suy).

---

### ğŸ›‘ Váº¥n Ä‘á» Zero Probability

CÃ´ng thá»©c `probability N-gram` (Maximum Likelihood Estimation - MLE) cÃ³ thá»ƒ báº±ng 0 khi `n-gram` cá»¥ thá»ƒ khÃ´ng xuáº¥t hiá»‡n trong `corpus`:
$$P(w_{n} \mid w_{n-N+1}^{n-1})=\frac{C(w_{n-N+1}^{n-1}, w_{n})}{C(w_{n-N+1}^{n-1})} \text{ cÃ³ thá»ƒ báº±ng } 0$$

### Smoothing: Add-1 vÃ  Add-k

Äá»ƒ kháº¯c phá»¥c váº¥n Ä‘á» **zero probability**, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng **smoothing** báº±ng cÃ¡ch thÃªm má»™t lÆ°á»£ng nhá» vÃ o cÃ¡c `counts` (sá»‘ láº§n Ä‘áº¿m).

* **Add-1 smoothing** (cho `Bigram`):

$$P(w_{n} \mid w_{n-1}) = \frac{C(w_{n-1}, w_{n}) + 1}{\sum_{w \in V}(C(w_{n-1}, w) + 1)} = \frac{C(w_{n-1}, w_{n}) + 1}{C(w_{n-1}) + |V|}$$

* **Add-k smoothing** (tá»•ng quÃ¡t hÆ¡n):
$$P(w_{n} \mid w_{n-1}) = \frac{C(w_{n-1}, w_{n}) + k}{\sum_{w \in V}(C(w_{n-1}, w) + k)} = \frac{C(w_{n-1}, w_{n}) + k}{C(w_{n-1}) + k \cdot |V|}$$

---

### ğŸ”„ Backoff vÃ  Interpolation

### Backoff Strategies

Khi sá»­ dá»¥ng **back-off**, náº¿u `N-gram` cáº¥p cao hÆ¡n bá»‹ thiáº¿u, `model` sáº½ dá»± phÃ²ng sá»­ dá»¥ng `(N-1)-gram`, v.v. Äiá»u nÃ y yÃªu cáº§u **probability discounting** (chiáº¿t kháº¥u xÃ¡c suáº¥t) Ä‘á»ƒ Ä‘iá»u chá»‰nh `probability distribution` tá»•ng thá»ƒ.

- **Katz backoff**: LÃ  má»™t vÃ­ dá»¥ sá»­ dá»¥ng **discounting** Ä‘á»ƒ láº¥y má»™t pháº§n `probability` tá»« cÃ¡c `N-grams` Ä‘Ã£ tháº¥y Ä‘á»ƒ phÃ¢n bá»• cho cÃ¡c `N-grams` chÆ°a tá»«ng tháº¥y.
- **â€œStupidâ€ backoff**: Náº¿u `probability N-gram` cáº¥p cao hÆ¡n bá»‹ thiáº¿u, `probability N-gram` cáº¥p tháº¥p hÆ¡n sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng, chá»‰ cáº§n nhÃ¢n vá»›i má»™t **constant** (háº±ng sá»‘), thÆ°á»ng lÃ  khoáº£ng $0.4$.

> Visualization

![10_Smoothing](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Image_Module_02/M2_W3/10_Smoothing.png)

### Interpolation

**Interpolation** (Ná»™i suy) káº¿t há»£p cÃ¡c `probabilities` tá»« nhiá»u cáº¥p Ä‘á»™ `N-gram` khÃ¡c nhau báº±ng cÃ¡ch sá»­ dá»¥ng trá»ng sá»‘ ($\lambda_i$):

$$\hat{P}(w_{n} \mid w_{n-2} w_{n-1})=\lambda_{1} \times P(w_{n} \mid w_{n-2} w_{n-1}) +\lambda_{2} \times P(w_{n} \mid w_{n-1})+\lambda_{3} \times P(w_{n})$$

Trong Ä‘Ã³ tá»•ng cÃ¡c trá»ng sá»‘ pháº£i báº±ng 1:

$$\sum_{i} \lambda_{i}=1$$


