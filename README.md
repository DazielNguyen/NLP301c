# **NLP301c - X·ª≠ L√Ω Ng√¥n Ng·ªØ T·ª± Nhi√™n - K·ª≥ 6 - FALL 2025**

Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi repository h·ªçc t·∫≠p m√¥n Natural Language Processing! üëã

## **Gi·ªõi thi·ªáu**

Xin ch√†o! M√¨nh l√† **Nguy·ªÖn VƒÉn Anh Duy**, sinh vi√™n ng√†nh Tr√≠ Tu·ªá Nh√¢n T·∫°o t·∫°i ƒê·∫°i h·ªçc FPT TP.HCM. Repository n√†y l∆∞u tr·ªØ to√†n b·ªô t√†i li·ªáu h·ªçc t·∫≠p, b√†i t·∫≠p v√† ghi ch√∫ c·ªßa m√¨nh trong m√¥n NLP301c.

## **C·∫•u tr√∫c Repository**

### **Module 01 - Natural Language Processing with Classification and Vector Spaces**

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi ph√¢n lo·∫°i v√† kh√¥ng gian vector_

#### **Week 1: Sentiment Analysis with Logistic Regression**

- Ph√¢n t√≠ch t√¨nh c·∫£m (Sentiment Analysis) s·ª≠ d·ª•ng Logistic Regression
- Bi·ªÉu di·ªÖn vƒÉn b·∫£n d∆∞·ªõi d·∫°ng vector
- X√¢y d·ª±ng b·ªô ph√¢n lo·∫°i t√≠ch c·ª±c/ti√™u c·ª±c cho tweets
- Supervised Machine Learning v√† h√†m chi ph√≠
- **B√†i t·∫≠p**: X√¢y d·ª±ng model ph√¢n lo·∫°i sentiment cho Twitter data

#### **Week 2: Sentiment Analysis with Naive Bayes**

- Probability v√† Bayes' Rule (Quy t·∫Øc Bayes)
- Conditional Probability (X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán)
- Naive Bayes Classifier cho ph√¢n lo·∫°i vƒÉn b·∫£n
- Laplacian Smoothing ƒë·ªÉ x·ª≠ l√Ω t·ª´ ch∆∞a g·∫∑p
- **B√†i t·∫≠p**: Tri·ªÉn khai Naive Bayes cho sentiment analysis

#### **Week 3: Vector Space Models**

- Word by Word v√† Word by Document design
- Co-occurrence matrices (Ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán)
- Euclidean Distance v√† Cosine Similarity
- PCA (Principal Component Analysis) ƒë·ªÉ gi·∫£m chi·ªÅu
- **B√†i t·∫≠p**: X√¢y d·ª±ng vector space model ƒë·ªÉ t√¨m t·ª´ t∆∞∆°ng t·ª±

#### **Week 4: Machine Translation and Document Search**

- Transforming word vectors gi·ªØa c√°c ng√¥n ng·ªØ
- Locality Sensitive Hashing (LSH)
- K-Nearest Neighbors search
- D·ªãch m√°y c∆° b·∫£n s·ª≠ d·ª•ng word embeddings
- **B√†i t·∫≠p**: X√¢y d·ª±ng h·ªá th·ªëng d·ªãch thu·∫≠t Anh-Ph√°p ƒë∆°n gi·∫£n

---

### **Module 02 - Natural Language Processing with Probabilistic Models**

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh x√°c su·∫•t_

#### **Week 1: Autocorrect and Minimum Edit Distance**

- X√¢y d·ª±ng model Autocorrect (t·ª± ƒë·ªông s·ª≠a l·ªói ch√≠nh t·∫£)
- Minimum Edit Distance (Kho·∫£ng c√°ch ch·ªânh s·ª≠a t·ªëi thi·ªÉu)
- Dynamic Programming cho edit distance
- Insert, Delete, Switch, Replace operations
- **B√†i t·∫≠p**: Tri·ªÉn khai autocorrect system

#### **Week 2: Part of Speech Tagging and Hidden Markov Models**

- Part of Speech (POS) Tagging
- Markov Chains (Chu·ªói Markov)
- Hidden Markov Models (HMMs)
- Viterbi Algorithm
- Named Entity Recognition
- **B√†i t·∫≠p**: X√¢y d·ª±ng POS tagger s·ª≠ d·ª•ng HMM

#### **Week 3: Autocomplete and Language Models**

- N-grams (Unigrams, Bigrams, Trigrams)
- Language Models v√† t√≠nh x√°c su·∫•t c√¢u
- Perplexity ƒë·ªÉ ƒë√°nh gi√° model
- Smoothing techniques cho unseen n-grams
- Out-of-vocabulary words handling
- **B√†i t·∫≠p**: X√¢y d·ª±ng autocomplete system s·ª≠ d·ª•ng N-gram model

#### **Week 4: Word Embeddings with Neural Networks**

- Basic Word Representations (One-hot vectors)
- Word2Vec v√† Continuous Bag of Words (CBOW)
- Training word embeddings
- Word analogies v√† semantic relationships
- GloVe embeddings
- **B√†i t·∫≠p**: Training word embeddings t·ª´ text corpus

---

### **Module 03 - Natural Language Processing with Sequence Models**

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh chu·ªói_

#### **Week 1: Recurrent Neural Networks for Language Modeling**

- Neural Networks cho Sentiment Analysis
- Recurrent Neural Networks (RNNs)
- Forward Propagation trong RNNs
- Dense Layers v√† ReLU activation
- Backpropagation Through Time (BPTT)
- **B√†i t·∫≠p**: X√¢y d·ª±ng RNN cho sentiment classification

#### **Week 2: LSTMs and Named Entity Recognition**

- Long Short-Term Memory (LSTM) networks
- Vanishing v√† Exploding Gradients problem
- LSTM Architecture (gates: forget, input, output)
- Named Entity Recognition (NER)
- Gated Recurrent Units (GRUs)
- **B√†i t·∫≠p**: Tri·ªÉn khai LSTM cho NER task

#### **Week 3: Siamese Networks**

- Siamese Network Architecture
- Similarity v√† distance metrics
- One-shot learning
- Duplicate question detection
- Triplet Loss function
- **B√†i t·∫≠p**: X√¢y d·ª±ng Siamese network cho question similarity

---

### **Module 04 - Natural Language Processing with Attention Models**

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh attention_

#### **Week 1: Neural Machine Translation**

- Sequence-to-Sequence (Seq2Seq) models
- Encoder-Decoder architecture
- Attention Mechanism
- Teacher Forcing
- BLEU Score ƒë·ªÉ ƒë√°nh gi√° d·ªãch m√°y
- **B√†i t·∫≠p**: X√¢y d·ª±ng neural machine translation system

#### **Week 2: Text Summarization**

- Transformers Architecture
- Multi-Head Attention
- Positional Encoding
- Self-Attention mechanism
- Text Summarization (extractive v√† abstractive)
- **B√†i t·∫≠p**: Tri·ªÉn khai text summarization v·ªõi Transformers

#### **Week 3: Question Answering**

- Transfer Learning trong NLP
- BERT (Bidirectional Encoder Representations from Transformers)
- T5 (Text-to-Text Transfer Transformer)
- Fine-tuning pre-trained models
- Context-based Question Answering
- **B√†i t·∫≠p**: Fine-tune BERT cho question answering task

---

## **T√†i li·ªáu b·ªï sung**

- **`utils.py`**: C√°c h√†m ti·ªán √≠ch d√πng chung cho c√°c b√†i t·∫≠p
- **`requirements.txt`**: Danh s√°ch th∆∞ vi·ªán Python c·∫ßn thi·∫øt
- **`npl-env/`**: Virtual environment cho project
- **`√în t·∫≠p PE/`**: T√†i li·ªáu √¥n t·∫≠p cho c√°c k·ª≥ thi Practice Exam

## **C√¥ng ngh·ªá s·ª≠ d·ª•ng**

- **Python 3.11**: Ng√¥n ng·ªØ l·∫≠p tr√¨nh ch√≠nh
- **NumPy**: T√≠nh to√°n s·ªë h·ªçc v√† ma tr·∫≠n
- **NLTK**: Natural Language Toolkit
- **TensorFlow/Keras**: Deep Learning frameworks
- **Jupyter Notebook**: M√¥i tr∆∞·ªùng ph√°t tri·ªÉn t∆∞∆°ng t√°c

## **K·∫øt n·ªëi v·ªõi m√¨nh**

N·∫øu b·∫°n mu·ªën trao ƒë·ªïi ho·∫∑c c√≥ th·∫Øc m·∫Øc g√¨, ƒë·ª´ng ng·∫°i li√™n h·ªá nh√©:

- **Email**: duynguyenvananh@gmail.com
- **Phone**: 0387883041
- **GitHub**: [@DazielNguyen](https://github.com/DazielNguyen)
- **Linkedin**: [VƒÉn Anh Duy](https://www.linkedin.com/in/dazielvad/)

## License

Copyright ¬© 2025 Nguyen Van Anh Duy - AWS FCJ Internship Report

---

**C·∫£m ∆°n b·∫°n ƒë√£ gh√© thƒÉm! Have a good day!**
