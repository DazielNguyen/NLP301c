# **NLP301c - X·ª≠ L√Ω Ng√¥n Ng·ªØ T·ª± Nhi√™n - K·ª≥ 6 - FALL 2025**

![Coursera_NLP](https://github.com/DazielNguyen/NLP301c/blob/main/NLP_Coursera.png)

## **Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi repository h·ªçc t·∫≠p m√¥n Natural Language Processing!** üëã

## **Gi·ªõi thi·ªáu**

Xin ch√†o! M√¨nh l√† **Nguy·ªÖn VƒÉn Anh Duy**, sinh vi√™n ng√†nh Tr√≠ Tu·ªá Nh√¢n T·∫°o t·∫°i ƒê·∫°i h·ªçc FPT TP.HCM. Repository n√†y l∆∞u tr·ªØ to√†n b·ªô t√†i li·ªáu h·ªçc t·∫≠p, b√†i t·∫≠p v√† ghi ch√∫ c·ªßa m√¨nh trong m√¥n NLP301c.

## **V·ªÅ kh√≥a h·ªçc**

Kh√≥a h·ªçc **Natural Language Processing Specialization** ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi:

- **[Younes Bensouda Mourri](https://www.linkedin.com/in/younes-bensouda-mourri/)** - Instructor, Stanford University
- **[≈Åukasz Kaiser](https://www.linkedin.com/in/lukasz-kaiser/)** - Staff Research Scientist, Google Brain
- **[Eddy Shyu](https://www.linkedin.com/in/eddy-shyu/)** - Curriculum Product Manager, deeplearning.ai
- **[Andrew Ng](https://www.linkedin.com/in/andrewyng/)** - Founder, DeepLearning.AI & Co-founder, Coursera

Kh√≥a h·ªçc n√†y l√† m·ªôt ph·∫ßn c·ªßa **DeepLearning.AI** v√† ƒë∆∞·ª£c cung c·∫•p tr√™n n·ªÅn t·∫£ng **Coursera**.

## **C·∫•u tr√∫c Repository**

### [Module 01 - Natural Language Processing with Classification and Vector Spaces](https://github.com/DazielNguyen/NLP301c/tree/main/Module%2001)

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi ph√¢n lo·∫°i v√† kh√¥ng gian vector_

#### [Week 1: Sentiment Analysis with Logistic Regression](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Lessons_Notes_Module_01/Week_01.md)

- Ph√¢n t√≠ch t√¨nh c·∫£m (Sentiment Analysis) s·ª≠ d·ª•ng Logistic Regression
- Bi·ªÉu di·ªÖn vƒÉn b·∫£n d∆∞·ªõi d·∫°ng vector
- X√¢y d·ª±ng b·ªô ph√¢n lo·∫°i t√≠ch c·ª±c/ti√™u c·ª±c cho tweets
- Supervised Machine Learning v√† h√†m chi ph√≠
- **B√†i t·∫≠p**: X√¢y d·ª±ng model ph√¢n lo·∫°i sentiment cho Twitter data

#### [Week 2: Sentiment Analysis with Naive Bayes](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Lessons_Notes_Module_01/Week_02.md)

- Probability v√† Bayes' Rule (Quy t·∫Øc Bayes)
- Conditional Probability (X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán)
- Naive Bayes Classifier cho ph√¢n lo·∫°i vƒÉn b·∫£n
- Laplacian Smoothing ƒë·ªÉ x·ª≠ l√Ω t·ª´ ch∆∞a g·∫∑p
- **B√†i t·∫≠p**: Tri·ªÉn khai Naive Bayes cho sentiment analysis

#### [Week 3: Vector Space Models](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Lessons_Notes_Module_01/Week_03.md)

- Word by Word v√† Word by Document design
- Co-occurrence matrices (Ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán)
- Euclidean Distance v√† Cosine Similarity
- PCA (Principal Component Analysis) ƒë·ªÉ gi·∫£m chi·ªÅu
- **B√†i t·∫≠p**: X√¢y d·ª±ng vector space model ƒë·ªÉ t√¨m t·ª´ t∆∞∆°ng t·ª±

#### [Week 4: Machine Translation and Document Search](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2001/Lessons_Notes_Module_01/Week_04.md)

- Transforming word vectors gi·ªØa c√°c ng√¥n ng·ªØ
- Locality Sensitive Hashing (LSH)
- K-Nearest Neighbors search
- D·ªãch m√°y c∆° b·∫£n s·ª≠ d·ª•ng word embeddings
- **B√†i t·∫≠p**: X√¢y d·ª±ng h·ªá th·ªëng d·ªãch thu·∫≠t Anh-Ph√°p ƒë∆°n gi·∫£n

---

### [Module 02 - Natural Language Processing with Probabilistic Models](https://github.com/DazielNguyen/NLP301c/tree/main/Module%2002)

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh x√°c su·∫•t_

#### [Week 1: Autocorrect and Minimum Edit Distance](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Lessons_Notes_Module_02/Week_01.md)

- X√¢y d·ª±ng model Autocorrect (t·ª± ƒë·ªông s·ª≠a l·ªói ch√≠nh t·∫£)
- Minimum Edit Distance (Kho·∫£ng c√°ch ch·ªânh s·ª≠a t·ªëi thi·ªÉu)
- Dynamic Programming cho edit distance
- Insert, Delete, Switch, Replace operations
- **B√†i t·∫≠p**: Tri·ªÉn khai autocorrect system

#### [Week 2: Part of Speech Tagging and Hidden Markov Models](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Lessons_Notes_Module_02/Week_02.md)

- Part of Speech (POS) Tagging
- Markov Chains (Chu·ªói Markov)
- Hidden Markov Models (HMMs)
- Viterbi Algorithm
- Named Entity Recognition
- **B√†i t·∫≠p**: X√¢y d·ª±ng POS tagger s·ª≠ d·ª•ng HMM

#### [Week 3: Autocomplete and Language Models](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Lessons_Notes_Module_02/Week_03.md)

- N-grams (Unigrams, Bigrams, Trigrams)
- Language Models v√† t√≠nh x√°c su·∫•t c√¢u
- Perplexity ƒë·ªÉ ƒë√°nh gi√° model
- Smoothing techniques cho unseen n-grams
- Out-of-vocabulary words handling
- **B√†i t·∫≠p**: X√¢y d·ª±ng autocomplete system s·ª≠ d·ª•ng N-gram model

#### [Week 4: Word Embeddings with Neural Networks](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2002/Lessons_Notes_Module_02/Week_04.md)

- Basic Word Representations (One-hot vectors)
- Word2Vec v√† Continuous Bag of Words (CBOW)
- Training word embeddings
- Word analogies v√† semantic relationships
- GloVe embeddings
- **B√†i t·∫≠p**: Training word embeddings t·ª´ text corpus

---

### [Module 03 - Natural Language Processing with Sequence Models](https://github.com/DazielNguyen/NLP301c/tree/main/Module%2003)

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh chu·ªói_

#### [Week 1: Recurrent Neural Networks for Language Modeling](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2003/Lessons_Notes_Module_03/Week_01.md)

- Neural Networks cho Sentiment Analysis
- Recurrent Neural Networks (RNNs)
- Forward Propagation trong RNNs
- Dense Layers v√† ReLU activation
- Backpropagation Through Time (BPTT)
- **B√†i t·∫≠p**: X√¢y d·ª±ng RNN cho sentiment classification

#### [Week 2: LSTMs and Named Entity Recognition](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2003/Lessons_Notes_Module_03/Week_02.md)

- Long Short-Term Memory (LSTM) networks
- Vanishing v√† Exploding Gradients problem
- LSTM Architecture (gates: forget, input, output)
- Named Entity Recognition (NER)
- Gated Recurrent Units (GRUs)
- **B√†i t·∫≠p**: Tri·ªÉn khai LSTM cho NER task

#### [Week 3: Siamese Networks](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2003/Lessons_Notes_Module_03/Week_03.md)

- Siamese Network Architecture
- Similarity v√† distance metrics
- One-shot learning
- Duplicate question detection
- Triplet Loss function
- **B√†i t·∫≠p**: X√¢y d·ª±ng Siamese network cho question similarity

---

### [Module 04 - Natural Language Processing with Attention Models](https://github.com/DazielNguyen/NLP301c/tree/main/Module%2004)

_X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v·ªõi c√°c m√¥ h√¨nh attention_

#### [Week 1: Neural Machine Translation](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2004/Lessons_Notes_Module_04/Week_01.md)

- Sequence-to-Sequence (Seq2Seq) models
- Encoder-Decoder architecture
- Attention Mechanism
- Teacher Forcing
- BLEU Score ƒë·ªÉ ƒë√°nh gi√° d·ªãch m√°y
- **B√†i t·∫≠p**: X√¢y d·ª±ng neural machine translation system

#### [Week 2: Text Summarization](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2004/Lessons_Notes_Module_04/Week_02.md)

- Transformers Architecture
- Multi-Head Attention
- Positional Encoding
- Self-Attention mechanism
- Text Summarization (extractive v√† abstractive)
- **B√†i t·∫≠p**: Tri·ªÉn khai text summarization v·ªõi Transformers

#### [Week 3: Question Answering](https://github.com/DazielNguyen/NLP301c/blob/main/Module%2004/Lessons_Notes_Module_04/Week_03.md)

- Transfer Learning trong NLP
- BERT (Bidirectional Encoder Representations from Transformers)
- T5 (Text-to-Text Transfer Transformer)
- Fine-tuning pre-trained models
- Context-based Question Answering
- **B√†i t·∫≠p**: Fine-tune BERT cho question answering task

---

## **√în t·∫≠p PE (Practice Exam)**

Repository n√†y bao g·ªìm t√†i li·ªáu √¥n t·∫≠p v√† ƒë·ªÅ thi t·ª´ c√°c k·ª≥ tr∆∞·ªõc, gi√∫p b·∫°n chu·∫©n b·ªã t·ªët cho k·ª≥ thi:

### [Knowledge Base - T·ªïng h·ª£p ki·∫øn th·ª©c NLP](https://github.com/DazielNguyen/NLP301c/blob/main/%C3%94n%20t%E1%BA%ADp%20PE/Knowledge.md)

T√†i li·ªáu t·ªïng h·ª£p to√†n di·ªán c√°c k·ªπ thu·∫≠t x·ª≠ l√Ω chu·ªói v√† c√¢u trong NLP:
- **T√°ch t·ª´ v√† Tokenization**: word_tokenize, sent_tokenize, NLTK
- **X·ª≠ l√Ω v√† L√†m s·∫°ch Chu·ªói**: lowercase, remove punctuation, strip whitespace
- **ƒê·∫øm v√† Th·ªëng k√™ T·ª´**: frequency distribution, sorting
- **L·ªçc v√† Tr√≠ch xu·∫•t T·ª´**: filtering, pattern matching
- **Ki·ªÉm tra v√† X√°c th·ª±c Chu·ªói**: palindrome, isogram, validation
- **Chuy·ªÉn ƒë·ªïi Format**: snake_case, camelCase, title case
- **S·ª≠a l·ªói Ch√≠nh t·∫£**: TextBlob spell correction
- **Thao t√°c v·ªõi List v√† Dictionary**: slicing, sorting, lambda functions
- **N-grams: Bigrams, Trigrams v√† Anagrams**: Language models, text generation, spell checking
- **NLTK Library Cheatsheet**: Tokenization, stemming, lemmatization, POS tagging, NER, sentiment analysis

### Extra Practice Questions (PQ1-PQ20)

[B·ªô c√¢u h·ªèi th·ª±c h√†nh b·ªï sung](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/Extra%20-%20NLP301c) v·ªõi 20 c√¢u h·ªèi t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao:
- **PQ1**: Tokenize paragraph th√†nh sentences v√† words
- **PQ2**: Normalize text (lowercase, remove whitespace)
- **PQ3**: Remove punctuation kh√¥ng d√πng th∆∞ vi·ªán string
- **PQ4**: Count word frequency, filter v√† sort
- **PQ5**: Top 3 most frequent words
- **PQ6**: Average word length (exclude short words)
- **PQ7**: Extract words starting with vowel
- **PQ8**: Extract words with vowel v√† digit
- **PQ9**: Extract unique words, exclude stop words
- **PQ10**: Extract email addresses
- **PQ11**: Find palindrome words
- **PQ12**: Check isogram
- **PQ13**: Validate Python variable names
- **PQ14**: Convert snake_case to camelCase
- **PQ15**: Smart title case preserving acronyms
- **PQ16**: Reverse word order
- **PQ17**: Extract every nth word
- **PQ18**: Group words by length
- **PQ19**: Multi-criteria sorting
- **PQ20**: Longest common prefix

### ƒê·ªÅ thi c√°c k·ª≥ tr∆∞·ªõc

#### **Fall 2024 (FA24)**
- **[FA24 - Practice Exam](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/FA24%20-%20NLP301c%20-%20PE)**
  - M√£ ƒë·ªÅ: NLP301c-FA24-PE
  - N·ªôi dung: Tokenization, sentiment analysis, text processing
  
- **[FA24 - Retake Exam](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/FA24%20-%20NLP301c%20-%20RE)**
  - M√£ ƒë·ªÅ: NLP301c-FA24-RE
  - N·ªôi dung: String manipulation, word frequency, filtering

#### **Fall 2025 (FA25)**
- **[FA25 - Practice Exam](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/FA25%20-%20NLP301c%20-%20PE)**
  - M√£ ƒë·ªÅ: NLP301c-FA25-PE
  - N·ªôi dung: Advanced text processing, pattern matching
  
- **[FA25 - Practice Exam 2](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/FA25%20-%20NLP301c%20-%20PE2)**
  - M√£ ƒë·ªÅ: NLP301c-FA25-PE2
  - N·ªôi dung: NLTK applications, word embeddings

#### **Spring 2024 (SP24)**
- **[SP24 - Practice Exam 1](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/SP24%20-%20NLP301c%20-%20PE1)**
  - M√£ ƒë·ªÅ: NLP301c-SP24-PE1
  - N·ªôi dung: Basic tokenization, frequency analysis
  
- **[SP24 - Practice Exam 2](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/SP24%20-%20NLP301c%20-%20PE2)**
  - M√£ ƒë·ªÅ: NLP301c-SP24-PE2
  - N·ªôi dung: Text cleaning, validation, formatting

#### **Summer 2024 (SU24)**
- **[SU24 - Practice Exam 1](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/SU24%20-%20NLP301c%20-%20PE1)**
  - M√£ ƒë·ªÅ: NLP301c-SU24-PE1
  - N·ªôi dung: String operations, text transformation

#### **Summer 2025 (SU25)**
- **[SU25 - Practice Exam](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/SU25%20-%20NLP301c%20-%20PE)**
  - M√£ ƒë·ªÅ: NLP301c-SU25-PE
  - N·ªôi dung: Comprehensive NLP techniques
  
- **[SU25 - Retake Exam](https://github.com/DazielNguyen/NLP301c/tree/main/%C3%94n%20t%E1%BA%ADp%20PE/SU25%20-%20NLP301c%20-%20RE)**
  - M√£ ƒë·ªÅ: NLP301c-SU25-RE
  - N·ªôi dung: Problem solving with NLTK

### Tips cho Practice Exam

1. **N·∫Øm v·ªØng c∆° b·∫£n**: Tokenization, lowercase, split(), join()
2. **Th√†nh th·∫°o NLTK**: word_tokenize(), sent_tokenize(), stopwords
3. **X·ª≠ l√Ω dictionary**: ƒê·∫øm t·∫ßn su·∫•t, sorting v·ªõi lambda
4. **List comprehension**: Vi·∫øt code ng·∫Øn g·ªçn v√† hi·ªáu qu·∫£
5. **String methods**: strip(), replace(), startswith(), endswith()
6. **Regex patterns**: Cho pattern matching ph·ª©c t·∫°p
7. **Practice coding**: L√†m h·∫øt c√°c ƒë·ªÅ PQ1-PQ20 v√† ƒë·ªÅ c√°c k·ª≥ tr∆∞·ªõc

---

## **√în t·∫≠p FE (Final Exam)**

T·ªïng h·ª£p c√°c ƒë·ªÅ thi Final Exam t·ª´ 2023-2025 tr√™n Quizlet ƒë·ªÉ √¥n t·∫≠p l√Ω thuy·∫øt:

### ƒê·ªÅ thi Final Exam c√°c k·ª≥

- **[NLP301c - SU23 - FE](https://quizlet.com/vn/1124363658/nlp301c-su23-fe-flash-cards/)** - Summer 2023
  - L√Ω thuy·∫øt c∆° b·∫£n v·ªÅ NLP, tokenization, word embeddings
  
- **[NLP301c - SP24 - FE](https://quizlet.com/vn/1124364086/nlp301c-sp24-fe-flash-cards/)** - Spring 2024
  - Vector spaces, probabilistic models, sentiment analysis
  
- **[NLP301c - SP24 - FE Retake](https://quizlet.com/vn/1124361416/nlp301c-sp24-fe-retake-flash-cards/)** - Spring 2024 Retake
  - Autocorrect, edit distance, language models
  
- **[NLP301c - FA24 - FE 1](https://quizlet.com/vn/1124365513/nlp301c-fa24-fe-1-flash-cards/)** - Fall 2024 Exam 1
  - RNNs, LSTMs, sequence models
  
- **[NLP301c - FA24 - FE 2](https://quizlet.com/vn/1124366281/nlp301c-fa24-fe-2-flash-cards/)** - Fall 2024 Exam 2
  - Attention mechanisms, transformers
  
- **[NLP301c - SU25 - FE](https://quizlet.com/vn/1124366683/nlp301c-su25-fe-flash-cards/)** - Summer 2025
  - Comprehensive review, all modules
  
- **[NLP301c - FA25 - FE](https://quizlet.com/vn/1124381752/nlp301c-fe-fa25-flash-cards/)** - Fall 2025
  - Latest exam, BERT, T5, question answering

### Tips cho Final Exam

1. **√în l√Ω thuy·∫øt**: N·∫Øm v·ªØng concepts t·ª´ c·∫£ 4 modules
2. **Flashcards**: S·ª≠ d·ª•ng Quizlet ƒë·ªÉ h·ªçc thu·ªôc ƒë·ªãnh nghƒ©a v√† c√¥ng th·ª©c
3. **So s√°nh models**: Hi·ªÉu r√µ ∆∞u/nh∆∞·ª£c ƒëi·ªÉm c·ªßa t·ª´ng model (Naive Bayes vs Logistic Regression, RNN vs LSTM, etc.)
4. **Math formulas**: Ghi nh·ªõ c√°c c√¥ng th·ª©c x√°c su·∫•t, cosine similarity, perplexity
5. **Architectures**: V·∫Ω v√† gi·∫£i th√≠ch ƒë∆∞·ª£c ki·∫øn tr√∫c c·ªßa RNN, LSTM, Transformer
6. **Applications**: Bi·∫øt ·ª©ng d·ª•ng c·ªßa t·ª´ng technique trong th·ª±c t·∫ø
7. **Practice**: L√†m h·∫øt c√°c ƒë·ªÅ t·ª´ 2023-2025 tr√™n Quizlet

---

## **T√†i li·ªáu b·ªï sung**

- **`utils.py`**: C√°c h√†m ti·ªán √≠ch d√πng chung cho c√°c b√†i t·∫≠p
- **`requirements.txt`**: Danh s√°ch th∆∞ vi·ªán Python c·∫ßn thi·∫øt
- **`npl-env/`**: Virtual environment cho project

## **C√¥ng ngh·ªá s·ª≠ d·ª•ng**

- **Python 3.11**: Ng√¥n ng·ªØ l·∫≠p tr√¨nh ch√≠nh
- **NumPy**: T√≠nh to√°n s·ªë h·ªçc v√† ma tr·∫≠n
- **NLTK**: Natural Language Toolkit
- **TensorFlow/Keras**: Deep Learning frameworks
- **Jupyter Notebook**: M√¥i tr∆∞·ªùng ph√°t tri·ªÉn t∆∞∆°ng t√°c

## **K·∫øt n·ªëi v·ªõi m√¨nh**

N·∫øu b·∫°n mu·ªën trao ƒë·ªïi ho·∫∑c c√≥ th·∫Øc m·∫Øc g√¨, ƒë·ª´ng ng·∫°i li√™n h·ªá nh√©:

- **Email**: duynguyenvananh@gmail.com
- **Phone**: 0387883041
- **GitHub**: [@DazielNguyen](https://github.com/DazielNguyen)
- **Linkedin**: [VƒÉn Anh Duy](https://www.linkedin.com/in/dazielvad/)

## License

Copyright ¬© 2025 Nguyen Van Anh Duy - NLP301c Documents

---

**C·∫£m ∆°n b·∫°n ƒë√£ gh√© thƒÉm! Have a good day!**
